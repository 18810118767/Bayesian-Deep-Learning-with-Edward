<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Inference networks</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="css/normalize.css" rel="stylesheet">
<link href="css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="getting-started">Getting Started</a>
<a class="button u-full-width" href="delving-in">Delving In</a>
<a class="button u-full-width" href="tutorials">Tutorials</a>
<a class="button u-full-width" href="api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="design-philosophy">Design Philosophy</a>
<a class="button2 u-full-width" href="contributing">Contributing</a>
<a class="button2 u-full-width" href="troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="inference-networks">Inference networks</h2>
<p>An inference network is a flexible construction for parameterizing approximating distributions during inference. They are used in Helmholtz machines (Dayan et al., 1995), deep Boltzmann machines (Salakhutdinov and Larochelle, 2010), and variational auto-encoders (Kingma &amp; Welling, 2014; Rezende et al., 2014).</p>
<p>Recall that probabilistic models often have <em>local</em> latent variables, that is, latent variables associated with a data point; for example, the latent cluster assignment of a data point or the hidden representation of a text or image. Variational inference on models with local latent variables requires optimizing over each variational factor, <span class="math display">\[q(\mathbf{z}; \mathbf{\lambda}) = \prod_{n=1}^N q(z_n; \lambda_n),\]</span> where <span class="math inline">\(z_n\)</span> are the latent variables associated to a data point <span class="math inline">\(x_n\)</span>. The set of parameters <span class="math inline">\(\mathbf{\lambda}=\{\lambda_n\}\)</span> grows with the size of data. This makes computation difficult when the data does not fit in memory. Further, a new set of parameters <span class="math inline">\(\mathbf{\lambda}'=\{\lambda_n'\}\)</span> must be optimized at test time, where there may be a new set of data points <span class="math inline">\(\{x_n'\}\)</span>.</p>
<p>An inference network replaces local variational parameters with global parameters. It is a neural network which takes <span class="math inline">\(x_n\)</span> as input and which outputs its local variational parameters <span class="math inline">\(\lambda_n\)</span>, <span class="math display">\[q(\mathbf{z}\mid \mathbf{x}; \theta)
= \prod_{n=1}^N q(z_n \mid x_n; \lambda_n)
= \prod_{n=1}^N q(z_n; \lambda_n = \mathrm{NN}(x_n; \theta)).\]</span> This amortizes inference by only defining a set of <em>global</em> parameters, namely, the parameters of the neural network. These parameters have a fixed size, making the memory complexity of inference a constant. Further, the same set of parameters can be used at test time: regardless of seeing new or old data points <span class="math inline">\(x_n'\)</span>, we simply pass it through the neural network (a forward pass) to obtain its corresponding variational factor <span class="math inline">\(q(z_n'; \lambda_n' =
\mathrm{NN}(x_n'; \theta))\)</span>.</p>
<p>Note that the inference network is an approximation: it is a common misunderstanding that the inference network produces a more expressive variational model. The inference network restricts the size of parameters, meaning it can only do as well as an approximation as the original variational distribution without the inference network.</p>
<p>There are often good reasons beyond computational reasons for wanting to use an inference network, depending on the problem setting. Originating from Helmholtz machines (Dayan et al., 1995), they are motivated by the hypothesis that “the human perceptual system is a statistical inference engine whose function is to infer the probable causes of sensory input.” The inference network is this function. In cognition, “humans and robots are in the setting of amortized inference: they have to solve many similar inference problems, and can thus offload part of the computational work to shared precomputation and adaptation over time” (Stuhlmuller et al., 2013).</p>
<h3 id="implementation">Implementation</h3>
<p>Inference networks are easy to build in Edward. In the example below, a data point <code>x</code> is a 28 by 28 pixel image (from MNIST).</p>
<p>We define a TensorFlow placeholder <code>x_ph</code> for the neural network’s input, which is a batch of data points.</p>
<pre class="python" language="Python"><code>x_ph = ed.placeholder(tf.float32, [M, 28 * 28])</code></pre>
<p>We build a neural network using Keras. It takes a 28 by 28 pixel image and outputs a <span class="math inline">\(10\)</span>-dimensional output, one for the mean (<code>mu</code>) and one for the standard deviation (<code>sigma</code>).</p>
<pre class="python" language="Python"><code>hidden = Dense(256, activation=K.relu)(x_ph)  # (M, 64)
qz = Normal(mu=Dense(10)(hidden),
            sigma=Dense(10, activation=K.softplus)(hidden))</code></pre>
<p>The variational model is parameterized by the neural network’s output.</p>
<p>During each step of inference, we pass in a batch of data points to feed the placeholder. Then we train the variational parameters (inference network’s parameters) according to gradients of the variational objective.</p>
<p>An example script using this variational model can found <a href="https://github.com/blei-lab/edward/blob/master/examples/vae.py">here</a>.</p>
<h3 id="footnotes">Footnotes</h3>
<p>Inference networks are also known as recognition models, recognition networks, or inverse mappings.</p>
<p>Variational models parameterized by inference networks are also known as probabilistic encoders, in analogy from coding theory to <a href="tut_decoder">probabilistic decoders</a>. We recommend against this terminology, in favor of making explicit the separation of model and inference. That is, variational inference with inference networks is a general technique that can be applied to models beyond probabilistic decoders.</p>
<p>This tutorial is taken from text originating in Tran et al. (2016). We thank Kevin Murphy for motivating the tutorial as it is based on our discussions, and also related discussion with Jaan Altosaar.</p>
<h3 id="references">References</h3>
<ul>
<li>Dayan, P., Hinton, G. E., Neal, R. M., &amp; Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889–904.</li>
<li>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. In International Conference on Learning Representations.</li>
<li>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In International Conference on Machine Learning.</li>
<li>Salakhutdinov, R., &amp; and Larochelle, H. (2010). Efficient learning of deep Boltzmann machines. In Artificial Intelligence and Statistics.</li>
<li>Stuhlmüller, A., Taylor, J., &amp; Goodman, N. (2013). Learning Stochastic Inverses. In Neural Information Processing Systems.</li>
<li>Tran, D., Ranganath, R., &amp; Blei, D. M. (2016). The Variational Gaussian Process. In International Conference on Learning Representations.</li>
</ul>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
