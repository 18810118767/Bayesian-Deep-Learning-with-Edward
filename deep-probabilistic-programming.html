<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward â€“ Deep Probabilistic Programming</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/">Home</a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="#">Advanced</a>
    <a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
    <a class="button2 u-full-width" href="/contributing">Contributing</a>
    <a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
    <a class="button2 u-full-width" href="/license">License</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a href="https://github.com/blei-lab/edward">
    <!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="/images/github-mark.svg" class="u-pull-right" style="padding-right:10%"
    alt="Edward on Github" />
    <!-- </object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="deep-probabilistic-programming">Deep Probabilistic Programming</h2>
<p>This webpage is a companion to the article, <a href="https://arxiv.org/abs/1701.03757">Deep Probabilistic Programming</a> <span class="citation">(<span class="citeproc-not-found" data-reference-id="tran2017deep"><strong>???</strong></span>)</span> . Here we provide more details for plug-and-play with the code snippets and also how to reproduce the experiments.</p>
<p>The code snippets assume Edward v1.2.0 and TensorFlow v0.11.0.</p>
<pre class="bash" language="bash"><code>pip install edward==1.2.0
export TF_BINARY_URL =  # see https://www.tensorflow.org/versions/r0.11/get_started/os_setup#download-and-setup
pip install $TF_BINARY_URL</code></pre>
<h2 id="experiments">Experiments</h2>
<p><strong>This section is under construction.</strong></p>
<h2 id="figures">Figures</h2>
<p><strong>Figure 1</strong>. Beta-Bernoulli program.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Bernoulli, Beta

theta = Beta(a=1.0, b=1.0)
x = Bernoulli(p=tf.ones(50) * theta)</code></pre>
<p>For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli.py"><code>examples/beta_bernoulli.py</code></a> in the Github repository.</p>
<p><strong>Figure 2</strong>. Variational auto-encoder for a data set of 28 x 28 pixel images <span class="citation">(Kingma &amp; Welling, 2014; Rezende, Mohamed, &amp; Wierstra, 2014)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from tensorflow.contrib import slim
from edward.models import Bernoulli, Normal

N = # TODO  # number of data points
d = 50  # latent dimension

# Probabilistic model
z = Normal(mu=tf.zeros([N, d]), sigma=tf.ones([N, d]))
h = slim.fully_connected(z, 256, activation_fn=tf.nn.relu)
x = Bernoulli(logits=slim.fully_connected(h, 28 * 28, activation_fn=None))

# Variational model
qx = tf.placeholder(tf.float32, [N, 28 * 28])
qh = slim.fully_connected(qx, 256, activation_fn=tf.nn.relu)
qz = Normal(mu=slim.fully_connected(qh, d, activation_fn=None),
            sigma=slim.fully_connected(qh, d, activation_fn=tf.nn.softplus))</code></pre>
<p>For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/vae.py"><code>examples/vae.py</code></a> in the Github repository.</p>
<p><strong>Figure 3</strong>. Bayesian recurrent neural network <span class="citation">(Radford M Neal, 2012)</span>. The program has an unspecified number of time steps; it uses a symbolic for loop (<code>tf.scan</code>).</p>
<pre language="python"><code># TODO tf.dot
import tensorflow as tf
from edward.models import Normal

H = 50  # number of hidden units
D = 10  # number of features

def rnn_cell(hprev, xt):
  return tf.tanh(tf.dot(hprev, Wh) + tf.dot(xt, Wx) + bh)

Wh = Normal(mu=tf.zeros([H, H]), sigma=tf.ones([H, H]))
Wx = Normal(mu=tf.zeros([D, H]), sigma=tf.ones([D, H]))
Wy = Normal(mu=tf.zeros([H, 1]), sigma=tf.ones([H, 1]))
bh = Normal(mu=tf.zeros(H), sigma=tf.ones(H))
by = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.placeholder(tf.float32, [None, D])
h = tf.scan(rnn_cell, x, initializer=tf.zeros(H))
y = Normal(mu=tf.matmul(h, Wy) + by, sigma=1.0)</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/bayesian_rnn.py"><code>examples/bayesian_rnn.py</code></a> in the Github repository.</p>
<p><strong>Figure 5</strong>. Hierarchical model <span class="citation">(<span class="citeproc-not-found" data-reference-id="gelman2006data"><strong>???</strong></span>)</span>. It is a mixture of Gaussians over <span class="math inline">\(D\)</span>-dimensional data <span class="math inline">\(\{x_n\}\in\mathbb{R}^{N\times D}\)</span>. There are <span class="math inline">\(K\)</span> latent cluster means <span class="math inline">\(\beta\in\mathbb{R}^{K\times D}\)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Categorical, Normal

N = 10000  # number of data points
D = 2  # data dimension
K = 5  # number of clusters

beta = Normal(mu=tf.zeros([K, D]), sigma=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([N, K]))
x = Normal(mu=tf.gather(beta, z), sigma=tf.ones([N, D]))</code></pre>
<p>It is used below in Figure * (Inference), Figure 6 (left/right), Figure * (variational EM), and Figure * (data subsampling).</p>
<p><strong>Figure 6</strong> (left). Variational inference <span class="citation">(Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999)</span>. It performs inference on the model defined in Figure 5.</p>
<pre language="python"><code>import edward as ed
import tensorflow as tf
from edward.models import Categorical, Normal

x_train =  # TODO

qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
  sigma=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_vi.py"><code>examples/mixture_gaussian_vi.py</code></a> in the Github repository.</p>
<p><strong>Figure 6</strong> (right). Monte Carlo <span class="citation">(Robert &amp; Casella, 1999)</span>. It performs inference on the model defined in Figure 5.</p>
<pre language="python"><code>import edward as ed
import tensorflow as tf
from edward.models import Empirical

x_train =  # TODO

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_mc.py"><code>examples/mixture_gaussian_mc.py</code></a> in the Github repository.</p>
<p><strong>Figure 7</strong>. Generative adversarial network <span class="citation">(<span class="citeproc-not-found" data-reference-id="goodfellow2014generative"><strong>???</strong></span>)</span>.</p>
<pre language="python"><code>import edward as ed
import tensorflow as tf

def generative_network(z):
  h = slim.fully_connected(z, 256, activation_fn=tf.nn.relu)
  return slim.fully_connected(h, 28 * 28, activation_fn=None)

def discriminative_network(x):
  h = slim.fully_connected(z, 28 * 28, activation_fn=tf.nn.relu)
  return slim.fully_connected(h, 1, activation_fn=None)

# Probabilistic model
z = Normal(mu=tf.zeros([M, d]), sigma=tf.ones([M, d]))
x = generative_network(z)

# augmentation for GAN-based inference
y_fake = Bernoulli(logits=discriminative_network(x))
y_real = Bernoulli(logits=discriminative_network(x_train))

data = {y_real: tf.ones(N), y_fake: tf.zeros(M)}
# TODO
inference = ed.GANInference(data=data)</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/gan.py"><code>examples/gan.py</code></a> in the Github repository.</p>
<p><strong>Figure *</strong>. Variational EM <span class="citation">(Radford M. Neal &amp; Hinton, 1993)</span>. It performs inference on the model defined in Figure 5.</p>
<pre language="python"><code>import edward as ed
import tensorflow as tf

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference_e = ed.VariationalInference({z: qz}, data={x: x_data, beta: qbeta})
inference_m = ed.MAP({beta: qbeta}, data={x: x_data, z: qz})

inference_e.initialize()
inference_m.initialize()

tf.initialize_all_variables().run()

for _ in range(10000):
  inference_e.update()
  inference_m.update()</code></pre>
<p>For more details, see the <a href="/api/inference-compositionality">inference compositionality</a> webpage.</p>
<p><strong>Figure *</strong>. Data subsampling. It performs inference on the model defined in Figure 5.</p>
<pre language="python"><code>import edward as ed
import tensorflow as tf

N = 10000  # number of data points
D = 2  # data dimension
K = 5  # number of clusters

beta = Normal(mu=tf.zeros([K, D]), sigma=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([M, K]))
x = Normal(mu=tf.gather(beta, z), sigma=tf.ones([M, D]))

qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
               sigma=tf.nn.softplus(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[M, D]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_batch})
inference.initialize(scale={x: float(N) / M, z: float(N) / M})</code></pre>
<p>For more details, see the <a href="/api/data-subsampling">data subsampling</a> webpage.</p>
<p><strong>Figure 9</strong>. Bayesian logistic regression with Hamiltonian Monte Carlo.</p>
<pre language="python"><code>import edward as ed
import tensorflow as tf
from edward.models import Bernoulli, Empirical, Normal

x_data =  # TODO
y_data =  # TODO
T =  # TODO
N =  # TODO
D =  # TODO

# Model
x = tf.Variable(x_data, trainable=False)
beta = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
y = Bernoulli(logits=tf.dot(x, beta))

# Inference
qbeta = Empirical(params=tf.Variable(tf.zeros([T, D])))
inference = ed.HMC({beta: qbeta}, data={y: y_data})
inference.run(step_size=0.5 / N, n_steps=10)</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/hmc.py"><code>examples/hmc.py</code></a> in the Github repository.</p>
<p><strong>Figure 10</strong>. Bayesian neural network for classification <span class="citation">(<span class="citeproc-not-found" data-reference-id="denker1987large"><strong>???</strong></span>)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Bernoulli, Normal

W_0 = Normal(mu=tf.zeros([D, H]), sigma=tf.ones([D, H]))
W_1 = Normal(mu=tf.zeros([H, 1]), sigma=tf.ones([H, 1]))
b_0 = Normal(mu=tf.zeros(H), sigma=tf.ones(L))
b_1 = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.placeholder(tf.float32, [N, D])
y = Bernoulli(logits=tf.matmul(tf.nn.tanh(tf.matmul(x, W_0) + b_0), W_1) + b_1)</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/bayesian_nn_classification.py"><code>examples/bayesian_nn_classification.py</code></a> in the Github repository.</p>
<p><strong>Figure 11</strong>. Restricted Boltzmann machine <span class="citation">(<span class="citeproc-not-found" data-reference-id="smolensky1986information"><strong>???</strong></span>)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Bernoulli

# Model parameters
W = tf.Variable(tf.float32, [N, M])  # N x M weight matrix
b_v = tf.Variable(tf.float32, [N])  # bias vector for v
b_h = tf.Variable(tf.float32, [M])  # bias vector for h

# Model conditionals
v_ph = tf.placeholder(tf.float32, [N])  # mutable state
h = Bernoulli(logits=b_h + tf.dot(v, W))
v = Bernoulli(logits=b_v + tf.dot(W, h))  # v is tied to v_ph during inference</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/rbm.py"><code>examples/rbm.py</code></a> in the Github repository.</p>
<p><strong>Figure 12</strong>. Dirichlet process mixture model <span class="citation">(<span class="citeproc-not-found" data-reference-id="antoniak1974mixtures"><strong>???</strong></span>)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Normal, DirichletProcess

H = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
mu = tf.pack([DirichletProcess(alpha=1.0, base=H) for _ in range(N)])
x = Normal(mu=mu, sigma=tf.ones(N))</code></pre>
<p>The essential component defining the <code>DirichletProcess</code> random variable is a stochastic while loop. We define it below.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Bernoulli, Beta

def dirichlet_process(alpha):
  def cond(k, beta_k):
    flip = Bernoulli(p=beta_k)
    return tf.equal(flip, tf.constant(1))

  def body(k, beta_k):
    beta_k = beta_k * Beta(a=1.0, b=alpha)
    return k + 1, beta_k

  k = tf.constant(0)
  beta_k = Beta(a=1.0, b=alpha)
  stick_num, stick_beta = tf.while_loop(cond, body, loop_vars=[k, beta_k])
  return stick_num</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/dp_mixture_model.py"><code>examples/dp_mixture_model.py</code></a> in the Github repository.</p>
<p><strong>Figure 13</strong>. Latent Dirichlet allocation <span class="citation">(<span class="citeproc-not-found" data-reference-id="blei2003latent"><strong>???</strong></span>)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Categorical, Dirichlet

D = 50  # number of documents
N = [11502, 213, 1523, 1351, ...]  # words per doc
K = 10  # number of topics
V = 100000  # vocabulary size

theta = Dirichlet(alpha=tf.zeros([D, K]) + 0.1)
phi = Dirichlet(alpha=tf.zeros([K, V]) + 0.05)
z = [[0] * N] * D
w = [[0] * N] * D
for d in range(D):
  for n in range(N[d]):
    z[d][n] = Categorical(pi=theta[d, :])
    w[d][n] = Categorical(pi=phi[z[d][n], :])</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/lda.py"><code>examples/lda.py</code></a> in the Github repository.</p>
<p><strong>Figure 14</strong>. Gaussian matrix factorization <span class="citation">(<span class="citeproc-not-found" data-reference-id="salakhutdinov2011probabilistic"><strong>???</strong></span>)</span>.</p>
<pre language="python"><code>import tensorflow as tf
from edward.models import Normal

N = 10
M = 10
K = 5  # latent dimension

U = Normal(mu=tf.zeros([M, K]), sigma=tf.ones([M, K]))
V = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
Y = Normal(mu=tf.matmul(U, V, transpose_b=True), sigma=tf.ones([N, M]))</code></pre>
<p>TODO For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/gaussian_matrix_factorization.py"><code>examples/gaussian_matrix_factorization.py</code></a> in the Github repository.</p>
<p><strong>Figure *</strong>. Stochastic variational inference <span class="citation">(Hoffman, Blei, Wang, &amp; Paisley, 2013)</span>. For more details, see the <a href="/api/data-subsampling">data subsampling</a> webpage.</p>
<p><strong>Figure 15</strong>. Variational auto-encoder <span class="citation">(Kingma &amp; Welling, 2014; Rezende et al., 2014)</span>. See the script <a href="https://github.com/blei-lab/edward/blob/master/examples/vae.py"><code>examples/vae.py</code></a> in the Github repository.</p>
<p>TODO <strong>Figure 16</strong>. Exponential family embedding <span class="citation">(<span class="citeproc-not-found" data-reference-id="rudolph2016exponential"><strong>???</strong></span>)</span>. See the script <a href="https://github.com/blei-lab/edward/blob/master/examples/ef_emb.py"><code>examples/ef_emb.py</code></a> in the Github repository. A Github repo with more features is available at <a href="https://github.com/mariru/exponential_family_embeddings">mariru/exponential_family_embeddings</a>.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-hoffman2013stochastic">
<p>Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. (2013). Stochastic variational inference. <em>The Journal of Machine Learning Research</em>, <em>14</em>(1), 1303â€“1347.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183â€“233.</p>
</div>
<div id="ref-kingma2014auto">
<p>Kingma, D., &amp; Welling, M. (2014). Auto-encoding variational Bayes. In <em>International conference on learning representations</em>.</p>
</div>
<div id="ref-neal2012bayesian">
<p>Neal, R. M. (2012). <em>Bayesian learning for neural networks</em> (Vol. 118). Springer Science &amp; Business Media.</p>
</div>
<div id="ref-neal1993new">
<p>Neal, R. M., &amp; Hinton, G. E. (1993). A new view of the em algorithm that justifies incremental and other variants. In <em>Learning in graphical models</em> (pp. 355â€“368).</p>
</div>
<div id="ref-rezende2014stochastic">
<p>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In <em>ICML</em> (pp. 1278â€“1286).</p>
</div>
<div id="ref-robert1999monte">
<p>Robert, C. P., &amp; Casella, G. (1999). <em>Monte carlo statistical methods</em>. Springer.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
