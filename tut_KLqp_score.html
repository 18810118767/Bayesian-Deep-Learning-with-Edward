<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Score function gradient</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="css/normalize.css" rel="stylesheet">
<link href="css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="getting-started">Getting Started</a>
<a class="button u-full-width" href="delving-in">Delving In</a>
<a class="button u-full-width" href="tutorials">Tutorials</a>
<a class="button u-full-width" href="api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="design-philosophy">Design Philosophy</a>
<a class="button2 u-full-width" href="contributing">Contributing</a>
<a class="button2 u-full-width" href="troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="score-function-gradient">Score function gradient</h2>
<p>(This tutorial follows the <a href="tut_KLqp"><span class="math inline">\(\text{KL}(q\|p)\)</span> minimization</a> tutorial.)</p>
<p>We seek to maximize the ELBO, <span class="math display">\[\begin{aligned}
  \lambda^*
  &amp;=
  \arg \max_\lambda \text{ELBO}(\lambda)\\
  &amp;=\;
  \arg \max_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],\end{aligned}\]</span> using a “black box” algorithm. This means generically inferring the posterior while making few assumptions about the model.</p>
<h3 id="the-score-function-identity">The score function identity</h3>
<p>Gradient descent is a standard approach for optimizing complicated objectives like the ELBO. The idea is to calculate its gradient <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;=
  \nabla_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],\end{aligned}\]</span> and update the current set of parameters proportional to the gradient.</p>
<p>The score function gradient estimator leverages a property of logarithms to write the gradient as <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;=\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \nabla_\lambda \log q(z\;;\;\lambda)
  \:
  \big(
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big)
  \big].\end{aligned}\]</span> The gradient of the ELBO is an expectation over the variational model <span class="math inline">\(q(z\;;\;\lambda)\)</span>; the only new ingredient it requires is the <em>score function</em> <span class="math inline">\(\nabla_\lambda \log q(z\;;\;\lambda)\)</span>. Edward uses automatic differentiation, specifically with TensorFlow’s computational graphs, making this gradient computation both simple and efficient to distribute.</p>
<h3 id="noisy-estimates-using-monte-carlo-integration">Noisy estimates using Monte Carlo integration</h3>
<p>We can use Monte Carlo integration to obtain noisy estimates of both the ELBO and its gradient. The basic procedure follows these steps:</p>
<ol>
<li>draw <span class="math inline">\(S\)</span> samples <span class="math inline">\(\{z_s\}_1^S \sim q(z\;;\;\lambda)\)</span>,</li>
<li>evaluate the argument of the expectation using <span class="math inline">\(\{z_s\}_1^S\)</span>, and</li>
<li>compute the empirical mean of the evaluated quantities.</li>
</ol>
<p>A Monte Carlo estimate of the gradient is then <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;\approx\;
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \big(
  \log p(x, z_s)
  -
  \log q(z_s\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(z_s\;;\;\lambda)
  \big].\end{aligned}\]</span> This is an unbiased estimate of the actual gradient of the ELBO.</p>
<h3 id="implementation">Implementation</h3>
<p>We implement <span class="math inline">\(\text{KL}(q\|p)\)</span> minimization with the score function gradient in the class <code>MFVI</code> (mean-field variational inference). It inherits from <code>VariationalInference</code>, which provides a collection of default methods such as an optimizer.</p>
<pre class="python" language="Python"><code>class MFVI(VariationalInference):
  def __init__(self, *args, **kwargs):
    super(MFVI, self).__init__(*args, **kwargs)

  def initialize(self, n_samples=1, ...):
    ...
    self.n_samples = n_samples
    return super(MFVI, self).initialize(*args, **kwargs)

  def build_score_loss(self):
    x = self.data
    z = {key: rv.sample([self.n_samples])
         for key, rv in six.iteritems(self.latent_vars)}

    p_log_prob = self.model_wrapper.log_prob(x, z)
    q_log_prob = 0.0
    for key, rv in six.iteritems(self.latent_vars):
      q_log_prob += tf.reduce_sum(rv.log_prob(tf.stop_gradient(z[key])),
                                  list(range(1, len(rv.get_batch_shape()) + 1)))

    losses = p_log_prob - q_log_prob
    self.loss = tf.reduce_mean(losses)
    return -tf.reduce_mean(q_log_prob * tf.stop_gradient(losses))</code></pre>
<p>Two methods are added: <code>initialize()</code> and <code>build_score_loss()</code>. The method <code>initialize()</code> follows the same initialization as <code>VariationalInference</code>, and adds an argument: <code>n_samples</code> for the number of samples from the variational model.</p>
<p>The method <code>build_score_loss()</code> implements the ELBO and its gradient. It draws <code>self.n_samples</code> samples from the variational model. It registers the Monte Carlo estimate of the ELBO in TensorFlow’s computational graph, and stores it in <code>self.loss</code>, to track progress of the inference for diagnostics.</p>
<p>The method returns an object whose automatic differentiation is the score function gradient of the ELBO. The TensorFlow function <code>tf.stop_gradient()</code> tells the computational graph to stop traversing nodes to propagate gradients. In this case, the only gradients taken are <span class="math inline">\(\nabla_\lambda \log q(z_s\;;\;\lambda)\)</span>, one for each sample <span class="math inline">\(z_s\)</span>. We multiply it by <code>losses</code> element-wise and return the mean.</p>
<p>There is a nuance here. TensorFlow’s optimizers are configured to <em>minimize</em> an objective function. So the gradient is set to be the negative of the ELBO’s gradient.</p>
<p>See the <a href="api/">API</a> for more details.</p>
<h3 id="references">References</h3>
<ul>
<li>Paisley, J., Blei, D. M., &amp; Jordan, M. (2012). Variational Bayesian Inference with Stochastic Search. In International Conference on Machine Learning.</li>
<li>Ranganath, R., Gerrish, S., &amp; Blei, D. M. (2014). Black Box Variational Inference. In Artificial Intelligence and Statistics.</li>
</ul>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
