<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Data Subsampling</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/design-philosophy">Design Philosophy</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="/images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-composing">Composability</a>
<a class="button4 button-primary" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="data-subsampling">Data Subsampling</h3>
<p>Running algorithms which require the full data set for each update can be expensive when the data is large. In order to scale inferences, we perform batch training: update inference using only a subsample of data at a time. (Note that only certain algorithms can support data subsampling, such as <code>KLqp</code> and <code>SGLD</code>.)</p>
<p>There are two settings of data subsampling: working with the full model (when the data and model fit in memory) and working with a subgraph of the model (when the data and model do not fit in memory). We illustrate these two settings in Edward.</p>
<h3 id="full-graphs">Full graphs</h3>
<p>In the full graph setting, we do data subsampling while working with the full model. This setting is recommended when the data and model fit in memory. It is scalable in that the algorithm’s computational complexity is independent of the data set size.</p>
<p>For example, suppose we are in the following setting.</p>
<pre class="python" language="Python"><code>N = 10000000 # data set size
M = 5 # mini-batch size</code></pre>
<p>We define a hierarchical model, <span class="math display">\[p(\mathbf{y}, \mathbf{z}, \beta)
= p(\beta) \prod_{n=1}^N p(z_n \mid \beta) p(y_n \mid z_n, \beta),\]</span> where there are latent variables <span class="math inline">\(z_n\)</span> for each data point <span class="math inline">\(y_n\)</span> (local variables) and latent variables <span class="math inline">\(\beta\)</span> which are shared across data points (global variables).</p>
<pre class="python" language="Python"><code>beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for n in range(N)]))
y = RandomVariable(par_1=z, par_2=tf.pack([beta for n in range(N)]))</code></pre>
<p>For inference, we define the variational model, <span class="math display">\[q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{n=1}^N q(z_n \mid \beta; \gamma_n),\]</span> parameterized by <span class="math inline">\(\{\lambda, \{\gamma_n\}\}\)</span>.</p>
<pre class="python" language="Python"><code>qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
qz = RandomVariable(par=f(tf.pack([qbeta for n in range(N)]),
                          tf.Variable(tf.random_normal(N)))</code></pre>
<p>We instantiate the inference algorithm to perform inference over <span class="math inline">\(\beta\)</span> and a subset of <span class="math inline">\(\mathbf{z}\)</span>. The subset is determined by a TensorFlow placeholder <code>idx_ph</code>, which we will change at each step of the algorithm. We also pass in a TensorFlow placeholder <code>y_ph</code> for the data, so we can change the data at each step. (Alternatively, <a href="/api/data">batch tensors</a> can be used.)</p>
<pre class="python" language="Python"><code>y_ph = tf.placeholder(tf.float32, [M])
idx_ph = tf.placeholder(tf.int32, [M])
inference = ed.KLqp({beta: qbeta, tf.gather(z, idx_ph): tf.gather(qz, idx_ph)},
                  data={y: y_ph})</code></pre>
<p>We initialize the algorithm with the <code>scale</code> argument, so that computation on <code>z</code> will be scaled appropriately. This enables unbiased estimates for stochastic gradients.</p>
<pre class="python" language="Python"><code>inference.initialize(scale={z: float(N) / M})</code></pre>
<p>We now run the algorithm, assuming there is a <code>next_batch</code> function which provides the next batch of data.</p>
<pre class="python" language="Python"><code>for t in range(10000):
  y_batch = next_batch(size=M)
  local_idx = np.arange(t * M, (t + 1) * M) % N
  inference.update(feed_dict={y_ph: y_batch, idx_ph: local_idx})</code></pre>
<h3 id="subgraphs">Subgraphs</h3>
<p>In the subgraph setting, we do data subsampling while working with a subgraph of the full model. This setting is recommended when the data and model do not fit in memory. It is scalable in that both the algorithm’s computational complexity and memory complexity are independent of the data set size.</p>
<p>For example, suppose we are in the following setting.</p>
<pre class="python" language="Python"><code>N = 10000000 # data set size
M = 5 # mini-batch size</code></pre>
<p>The model is the same as before, <span class="math display">\[p(\mathbf{y}, \mathbf{z}, \beta)
= p(\beta) \prod_{n=1}^N p(z_n \mid \beta) p(y_n \mid z_n, \beta).\]</span> To avoid memory issues, we work on only a subgraph of the model, <span class="math display">\[p(\mathbf{y}, \mathbf{z}, \beta)
= p(\beta) \prod_{m=1}^M p(z_m \mid \beta) p(y_m \mid z_m, \beta)\]</span></p>
<pre class="python" language="Python"><code>beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for m in range(M)]))
y = RandomVariable(par_1=z par_2=tf.pack([beta for m in range(M)]))</code></pre>
<p>For inference, the variational model is the same as before, <span class="math display">\[q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{n=1}^N q(z_n \mid \beta; \gamma_n),\]</span> parameterized by <span class="math inline">\(\{\lambda, \{\gamma_n\}\}\)</span>. To avoid memory issues, we work on only a subgraph of the model, <span class="math display">\[q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{m=1}^M q(z_m \mid \beta; \gamma_m).\]</span> parameterized by <span class="math inline">\(\{\lambda, \{\gamma_m\}\}\)</span>. Importantly, only <span class="math inline">\(M\)</span> parameters are stored in memory for <span class="math inline">\(\{\gamma_m\}\)</span> rather than <span class="math inline">\(N\)</span>.</p>
<pre class="python" language="Python"><code>qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
qz_variables = tf.Variable(tf.random_normal(M)
qz = RandomVariable(par=f(tf.pack([qbeta for m in range(M)]),
                          qz_variables))</code></pre>
<p>We instantiate the inference algorithm to perform inference over <span class="math inline">\(\beta\)</span> and the subset of <span class="math inline">\(\mathbf{z}\)</span>. Unlike the full model setting, we do not do any local indexing because the subgraph is already local. We also pass in a TensorFlow placeholder <code>y_ph</code> for the data, so we can change the data at each step. (Alternatively, <a href="/api/data">batch tensors</a> can be used.)</p>
<pre class="python" language="Python"><code>y_ph = tf.placeholder(tf.float32, [M])
inference = ed.KLqp({beta: qbeta, z: qz}, data={y: y_ph})</code></pre>
<p>We initialize the algorithm with the <code>scale</code> argument, so that computation on <code>z</code> will be scaled appropriately. This enables unbiased estimates for stochastic gradients.</p>
<pre class="python" language="Python"><code>inference.initialize(scale={z: float(N) / M})</code></pre>
<p>We now run the algorithm, assuming there is a <code>next_batch</code> function which provides the next batch of data.</p>
<pre class="python" language="Python"><code>for t in range(10000):
  y_batch = next_batch(size=M)
  inference.update(feed_dict={y_ph: y_batch})
  tf.initialize_variables(qz_variables)</code></pre>
<p>After each iteration, we also reinitialize the parameters for <span class="math inline">\(q(\mathbf{z}\mid\beta)\)</span>; this is because we do inference on a new set of local variational factors for each batch.</p>
<h3 id="advanced-settings">Advanced settings</h3>
<p>Another approach to reduce memory complexity is to use an inference network. This can be applied using a global parameterization of <span class="math inline">\(q(\mathbf{z}, \beta)\)</span>. For more details, see the <a href="/tutorials/inference-networks">inference networks tutorial</a>.</p>
<p>In streaming data, or online inference, the size of the data <span class="math inline">\(N\)</span> may be unknown, or conceptually the size of the data may be infinite and at any time in which we query parameters from the online algorithm, the outputted parameters are from having processed as many data points up to that time. The approach of Bayesian filtering <span class="citation">(Broderick, Boyd, Wibisono, Wilson, &amp; Jordan, 2013; Doucet, Godsill, &amp; Andrieu, 2000)</span> can be applied in Edward using recursive posterior inferences; the approach of population posteriors <span class="citation">(McInerney, Ranganath, &amp; Blei, 2015)</span> is readily applicable from the subgraph setting.</p>
<p>In other settings, working on a subgraph of the model does not apply, such as in time series models when we want to preserve dependencies across time steps in our variational model. Approaches in the literature can be applied in Edward <span class="citation">(Binder, Murphy, &amp; Russell, 1997; Foti, Xu, Laird, &amp; Fox, 2014; Johnson &amp; Willsky, 2014)</span>.</p>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-binder1997space">
<p>Binder, J., Murphy, K., &amp; Russell, S. (1997). Space-efficient inference in dynamic probabilistic networks. <em>Bclr</em>, <em>1</em>, t1.</p>
</div>
<div id="ref-broderick2013streaming">
<p>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., &amp; Jordan, M. I. (2013). Streaming Variational Bayes. In <em>Neural information processing systems</em> (pp. 1727–1735).</p>
</div>
<div id="ref-doucet2000on">
<p>Doucet, A., Godsill, S., &amp; Andrieu, C. (2000). On sequential Monte Carlo sampling methods for Bayesian filtering. <em>Statistics and Computing</em>, <em>10</em>(3), 197–208.</p>
</div>
<div id="ref-foti2014stochastic">
<p>Foti, N., Xu, J., Laird, D., &amp; Fox, E. (2014). Stochastic variational inference for hidden markov models. In <em>Advances in neural information processing systems</em> (pp. 3599–3607).</p>
</div>
<div id="ref-johnson2014stochastic">
<p>Johnson, M., &amp; Willsky, A. S. (2014). Stochastic variational inference for bayesian time series models. In <em>ICML</em> (pp. 1854–1862).</p>
</div>
<div id="ref-mcinerney2015population">
<p>McInerney, J., Ranganath, R., &amp; Blei, D. M. (2015). The Population Posterior and Bayesian Inference on Streams. In <em>Neural information processing systems</em>.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
