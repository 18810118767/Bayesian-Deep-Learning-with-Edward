<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Data Subsampling</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/design-philosophy">Design Philosophy</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="/images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<p><span><span>navbar</span></span></p>
<h3 id="data-subsampling">Data Subsampling</h3>
<p>Batch training is desirable for scalability. In the data section, we described how to feed batches of data at a time. In Bayesian inference we have to deal with the additional problem of local versus global latent variables. In this API, the modeler never explicitly writes down plates, so we can’t tell which subset of the local latent variables are relevant to any batch (TODO although we could prbably do it internally, with batch_shape/event_shape; this also comes up in the bijector class).</p>
<p>Our key idea is to enable manual control of latent variable updates during inference. We illustrate below according to different settings of scalability.</p>
<p><strong>Full graphs.</strong> Suppose the probability model and variational model both fit in memory.</p>
<pre class="python" language="Python"><code>N = 10000000 # num data points
M = 5 # mini-batch size

# p(y, z, beta) = p(beta) prod_{n=1}^N p(z_n | beta) p(y_n | z_n, beta)
beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for n in range(N)]))
y = RandomVariable(par_1=z, par_2=tf.pack([beta for n in range(N)]))

# q(beta, z) = q(beta; lambda) prod_{n=1}^N q(z_n | beta; gamma_n)
qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
qz = RandomVariable(par=f(tf.pack([qbeta for n in range(N)]),
                          tf.Variable(tf.random_normal(N)))

y_ph = tf.placeholder(tf.float32, [M])
idx_ph = tf.placeholder(tf.int32, [M])
z_scale = tf.placeholder(tf.float32, [])
inference = ed.VI({beta: qbeta, tf.gather(z, idx_ph): tf.gather(qz, idx_ph)},
                  data={y: y_ph},
                  scale={z: z_scale})
inference.initialize()
for t in range(10000):
  y_batch = next_batch(size=M)
  local_idx = np.arange(t*M, (t+1)*M) % N
  for _ in range(10):
    inference.update(random_vars={tf.gather(z, idx_ph): tf.gather(qz, idx_ph)},
                     feed_dict={y_ph: y_batch, z_scale: 1.0, idx_ph: local_idx})
  inference.update(random_vars={beta: qbeta},
                   feed_dict={y_ph: y_batch, z_scale: float(N)/M,
                              idx_ph: local_idx})</code></pre>
<p>There is a change to the usual inference:</p>
<ul>
<li>We do separate updates of <code>z</code> and <code>beta</code>. Because we only update a batch of data points and their associated latent variables (<code>z[local_idx]</code>), we must scale computation of <code>z</code> when updating <code>beta</code>, so it is as if we had seen the full data set <code>scale</code> many times. This enables unbiased updates.</li>
</ul>
<p>This approach is scalable in that computational complexity is independent of the size of the data set. The size of parameters however grows with the size of data.</p>
<p><strong>Subgraphs.</strong> Suppose the full probability model and variational model no longer fit in memory.</p>
<pre class="python" language="Python"><code>N = 10000000 # num data points
M = 5 # mini-batch size

# p(y, z, beta) = p(beta) prod_{n=1}^N p(z_n | beta) p(y_n | z_n, beta)
# Define a subgraph, p(beta) prod_{m=1}^M p(z_m | beta) p(y_m | z_m, beta)
beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for m in range(M)]))
y = RandomVariable(par_1=z par_2=tf.pack([beta for m in range(M)]))

# q(beta, z) = q(beta; lambda) prod_{n=1}^N q(z_n | beta; gamma_n)
# Define a subgraph, q(beta; lambda) prod_{m=1}^M q(z_m | beta; gamma_m)
qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
with tf.variable_scope("qz"):
  qz = RandomVariable(par=f(tf.pack([qbeta for m in range(M)]),
                            tf.Variable(tf.random_normal(M)))

y_ph = tf.placeholder(tf.float32, [M])
z_scale = tf.placeholder(tf.float32, [])
inference = ed.VI({beta: qbeta, z: qz}, data={y: y_ph}, scale={z: z_scale})
inference.initialize()
for t in range(10000):
  y_batch = next_batch(size=M)
  for _ in range(10):
    inference.update(random_vars={z: qz},
                     feed_dict={y_ph: y_batch, z_scale: 1.0})
  inference.update(random_vars={beta: qbeta},
                   feed_dict={y_ph: y_batch, z_scale: float(N)/M})
  tf.initialize_variables(tf.get_collection("qz"))</code></pre>
<p>There is a change to the usual probability model:</p>
<ul>
<li>We use <code>M</code> instead of <code>N</code>. This defines a subgraph of the full probability model, representing only the piece relevant to a batch of data during inference. Conceptually <code>M</code> is the plate size of the graphical model.</li>
</ul>
<p>There is a change to the usual variational model:</p>
<ul>
<li>We use <code>M</code> instead of <code>N</code>. This defines a subgraph of the full variational model, representing only the piece relevant to a batch of data during inference. Conceptually <code>M</code> is the plate size of the graphical model.</li>
</ul>
<p>There is a change to the usual inference:</p>
<ul>
<li>We perform updates but no longer require any local indexing because the subgraphs are already all local.</li>
<li>We reset the parameters for <code>qz</code> because we do inference on a new set of local variational factors for each batch. The command reinitializes all <code>tf.Variables()</code> relevant to <code>qz</code>.
<p>If we care about the local parameters, we can save them. For example, if we want to do computation over multiple epochs, and we don’t have enough memory to store all parameters, then we can write the parameters to disk (at the end of the loop) and re-read them.</p></li>
<li>We can amortize computation using an inference network, which easily applies to this setting based on a global parameterization of <code>qbeta</code> and <code>qz</code>. In such a case, no in-place resetting of subgraph parameters are necessary.</li>
</ul>
<p>This approach is the most scalable: both computational complexity and memory complexity is independent of the size of the data set.</p>
<p>Sometimes resetting parameters is not sufficient, such as when <code>qz</code> may have a different distributional form for each data point. In this case, we can create random variables on the fly using local and global inference instantiations.</p>
<pre class="python" language="Python"><code>inference = ed.VI({beta: qbeta}, data={y: y_ph, z: qz},
                  scale={z: float(N)/M})
global_vi.initialize()
for _ in range(10000):
  y_batch = next_batch(size=M)
  qz = RandomVariable(M, par=qbeta)
  local_vi = ed.VI({z: qz}, data={y_ph: y_batch, beta: qbeta})
  for _ in range(10):
    local_vi.update()

  global_vi.update(feed_dict={y_ph: y_batch})</code></pre>
<pre class="python" language="Python"><code>global_vi = ed.VI({beta: qbeta}, data={y: y_ph, z: qz}, scale={z: float(N)/M})
global_vi.initialize()
for _ in range(10000):
  y_batch = next_batch(size=M)
  qz = RandomVariable(M, par=qbeta)
  local_vi = ed.VI({z: qz}, data={y: y_batch, beta: qbeta})
  for _ in range(10):
    local_vi.update()

  global_vi.update(feed_dict={y_ph: y_batch})</code></pre>
<ul>
<li>We define a global VI object and a local VI object. Each abstractly defines inference over different distributions (one does <span class="math inline">\(q(\beta)
  \approx p(\beta \mid x)\)</span>; the other does <span class="math inline">\(\prod q(z_n) \approx \prod
  p(z_n \mid \beta, x))\)</span>. We create the global VI object outside the loop and proceed to update it per iteration. We create the local VI object inside the loop and update it only once.</li>
<li>This construct is very general. We can arbitrarily compose inference algorithms to infer different conditional distributions within the full posterior. For example, instead of using both VI for both inferring <span class="math inline">\(p(\beta \mid x)\)</span> and <span class="math inline">\(\prod p(z_n \mid \beta, x)\)</span>, we can do MCMC to exactly infer only one of these.</li>
<li>The TensorFlow graph is “unrolled”, in the sense that a new node is being added at each step of inference.</li>
</ul>
<p><span><span>autogenerated</span></span></p>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
