<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Inference - API</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/design-philosophy">Design Philosophy</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="/images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
<a class="button4 button-primary" href="/api/inference-api">API</a>
</div>
</div>
<h3 id="inference---api">Inference - API</h3>
<dl class="class">
<dt id="edward.inferences.Inference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Inference</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/inference.py#L20" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Base class for Edward inference methods.</p>
<p class="rubric">Attributes</p>
<table class="docutils">
<colgroup>
<col width="6%"></col>
<col width="94%"></col>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>latent_vars</td>
<td>(dict of RandomVariable to RandomVariable) Collection of random variables to perform inference on. Each random variable is binded to another random variable; the latter will infer the former conditional on data.</td>
</tr>
<tr class="row-even"><td>data</td>
<td>(dict) Data dictionary whose values may vary at each session run.</td>
</tr>
<tr class="row-odd"><td>model_wrapper</td>
<td>(ed.Model or None) An optional wrapper for the probability model. If specified, the random variables in <cite>latent_vars</cite>‘ dictionary keys are strings used accordingly by the wrapper.</td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : dict of RandomVariable to RandomVariable, optional</p>
<blockquote>
<div><p>Collection of random variables to perform inference on. Each
random variable is binded to another random variable; the latter
will infer the former conditional on data.</p>
</div></blockquote>
<p><strong>data</strong> : dict, optional</p>
<blockquote>
<div><p>Data dictionary which binds observed variables (of type
<cite>RandomVariable</cite>) to their realizations (of type <cite>tf.Tensor</cite>).
It can also bind placeholders (of type <cite>tf.Tensor</cite>) used in the
model to their realizations; and prior latent variables (of type
<cite>RandomVariable</cite>) to posterior latent variables (of type
<cite>RandomVariable</cite>).</p>
</div></blockquote>
<p><strong>model_wrapper</strong> : ed.Model, optional</p>
<blockquote class="last">
<div><p>A wrapper for the probability model. If specified, the random
variables in <cite>latent_vars</cite>‘ dictionary keys are strings
used accordingly by the wrapper. <cite>data</cite> is also changed. For
TensorFlow, Python, and Stan models, the key type is a string;
for PyMC3, the key type is a Theano shared variable. For
TensorFlow, Python, and PyMC3 models, the value type is a NumPy
array or TensorFlow tensor; for Stan, the value type is the
type according to the Stan program’s data block.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>If <code class="docutils literal"><span class="pre">data</span></code> is not passed in, the dictionary is empty.</p>
<p>Three options are available for batch training:
1. internally if user passes in data as a dictionary of NumPy</p>
<blockquote>
<div>arrays;</div></blockquote>
<ol class="arabic simple" start="2">
<li>externally if user passes in data as a dictionary of
TensorFlow placeholders (and manually feeds them);</li>
<li>externally if user passes in data as TensorFlow tensors
which are the outputs of data readers.</li>
</ol>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; mu = Normal(mu=tf.constant(0.0), sigma=tf.constant(1.0))
&gt;&gt;&gt; x = Normal(mu=tf.ones(N) * mu, sigma=tf.constant(1.0))
&gt;&gt;&gt;
&gt;&gt;&gt; qmu_mu = tf.Variable(tf.random_normal([1]))
&gt;&gt;&gt; qmu_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([1])))
&gt;&gt;&gt; qmu = Normal(mu=qmu_mu, sigma=qmu_sigma)
&gt;&gt;&gt;
&gt;&gt;&gt; Inference({mu: qmu}, {x: tf.constant([0.0] * N)})

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Inference.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/inference.py#L336" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Function to call after convergence.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_iter=1000</em>, <em>n_print=None</em>, <em>n_minibatch=None</em>, <em>scale=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/inference.py#L245" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize inference algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_iter</strong> : int, optional</p>
<blockquote>
<div><p>Number of iterations for algorithm.</p>
</div></blockquote>
<p><strong>n_print</strong> : int, optional</p>
<blockquote>
<div><p>Number of iterations for each print progress. To suppress print
progress, then specify 0. Default is int(n_iter / 10).</p>
</div></blockquote>
<p><strong>n_minibatch</strong> : int, optional</p>
<blockquote>
<div><p>Number of samples for data subsampling. Default is to use all
the data. <code class="docutils literal"><span class="pre">n_minibatch</span></code> is available only for TensorFlow,
Python, and PyMC3 model wrappers; use <code class="docutils literal"><span class="pre">scale</span></code> for Edward’s
language. All data must be passed in as NumPy arrays. For
subsampling details, see <code class="docutils literal"><span class="pre">tf.train.slice_input_producer</span></code> and
<code class="docutils literal"><span class="pre">tf.train.batch</span></code>.</p>
</div></blockquote>
<p><strong>scale</strong> : dict of RandomVariable to tf.Tensor, optional</p>
<blockquote class="last">
<div><p>A scalar value to scale computation for any random variable that
it is binded to. For example, this is useful for scaling
computations with respect to local latent variables.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/inference.py#L321" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>info_dict</strong> : dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.run">
<code class="descname">run</code><span class="sig-paren">(</span><em>logdir=None</em>, <em>variables=None</em>, <em>use_coordinator=True</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/inference.py#L175" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>A simple wrapper to run inference.</p>
<ol class="arabic simple">
<li>Initialize algorithm via <code class="docutils literal"><span class="pre">initialize</span></code>.</li>
<li>(Optional) Build a <code class="docutils literal"><span class="pre">tf.train.SummaryWriter</span></code> for TensorBoard.</li>
<li>(Optional) Initialize TensorFlow variables.</li>
<li>(Optional) Start queue runners.</li>
<li>Run <code class="docutils literal"><span class="pre">update</span></code> for <code class="docutils literal"><span class="pre">self.n_iter</span></code> iterations.</li>
<li>While running, <code class="docutils literal"><span class="pre">print_progress</span></code>.</li>
<li>Finalize algorithm via <code class="docutils literal"><span class="pre">finalize</span></code>.</li>
<li>(Optional) Stop queue runners.</li>
</ol>
<p>To customize the way inference is run, run these steps
individually.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>logdir</strong> : str, optional</p>
<blockquote>
<div><p>Directory where event file will be written. For details,
see <cite>tf.train.SummaryWriter</cite>. Default is to write nothing.</p>
</div></blockquote>
<p><strong>variables</strong> : list, optional</p>
<blockquote>
<div><p>A list of TensorFlow variables to initialize during inference.
Default is to initialize all variables (this includes
reinitializing variables that were already initialized). To
avoid initializing any variables, pass in an empty list.</p>
</div></blockquote>
<p><strong>use_coordinator</strong> : bool, optional</p>
<blockquote>
<div><p>Whether to start and stop queue runners during inference using a
TensorFlow coordinator. For example, queue runners are necessary
for batch training with the <code class="docutils literal"><span class="pre">n_minibatch</span></code> argument or with
file readers.</p>
</div></blockquote>
<p><strong>*args</strong></p>
<blockquote>
<div><p>Passed into <code class="docutils literal"><span class="pre">initialize</span></code>.</p>
</div></blockquote>
<p><strong>**kwargs</strong></p>
<blockquote class="last">
<div><p>Passed into <code class="docutils literal"><span class="pre">initialize</span></code>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.update">
<code class="descname">update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/inference.py#L310" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.VariationalInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">VariationalInference</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L19" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Base class for variational inference methods.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.VariationalInference.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L162" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function.</p>
<p>Any derived class of <code class="docutils literal"><span class="pre">VariationalInference</span></code> must implement
this method or <code class="docutils literal"><span class="pre">build_loss_and_gradients</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><strong>NotImplementedError</strong></td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>optimizer=None</em>, <em>var_list=None</em>, <em>use_prettytensor=False</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L25" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>optimizer</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the variational
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>var_list</strong> : list of tf.Variable, optional</p>
<blockquote>
<div><p>List of TensorFlow variables to optimize over. Default is all
trainable variables that <code class="docutils literal"><span class="pre">latent_vars</span></code> and <code class="docutils literal"><span class="pre">data</span></code> depend on,
excluding those that are only used in conditionals in <code class="docutils literal"><span class="pre">data</span></code>.</p>
</div></blockquote>
<p><strong>use_prettytensor</strong> : bool, optional</p>
<blockquote class="last">
<div><p><code class="docutils literal"><span class="pre">True</span></code> if aim to use TensorFlow optimizer or <code class="docutils literal"><span class="pre">False</span></code> if aim
to use PrettyTensor optimizer (when using PrettyTensor).
Defaults to TensorFlow.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L150" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L124" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of optimizer for variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
loss function value after one iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[KL( q(z; \lambda) || p(z | x) ).\]</div>
<p>This class minimizes the objective by automatically selecting from a
variety of black box inference techniques.</p>
<p class="rubric">Notes</p>
<p>KLqp also optimizes any model parameters p(z | x;     heta). It does
this by variational EM, minimizing</p>
<div class="math">
\[E_{q(z; \lambda)} [ \log p(x, z;    heta) ]\]</div>
<p>with respect to       heta.</p>
<p>In conditional inference, we infer z in p(z, eta | x) while fixing
inference over eta using another distribution q(eta).
During gradient calculation, instead of using the model’s density</p>
<div class="math">
\[\log p(x, z^{(s)}), where z^{(s)} ~ q(z; \lambda),\]</div>
<p>for each sample s=1,...,S, KLqp uses</p>
<div class="math">
\[\log p(x, z^{(s)}, eta^{(s)}), where
z^{(s)} ~ q(z; \lambda) and eta^{(s)} ~ q(beta).\]</div>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLqp.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L65" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Wrapper for the KLqp loss function.</p>
<div class="math">
\[-ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>KLqp supports</p>
<ol class="arabic simple">
<li>score function gradients (Paisley et al., 2012)</li>
<li>reparameterization gradients (Kingma and Welling, 2014)</li>
</ol>
<p>of the loss function.</p>
<p>If the variational model is a normal distribution and the prior is
standard normal, then loss function can be written as</p>
<div class="math">
\[-E_{[\log p(x | z)] + KL( q(z; \lambda) || p(z) ),\]</div>
<p>where the KL term is computed analytically (Kingma and Welling,
2014).</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLqp.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L53" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<span class="target" id="module-edward.inferences"></span><dl class="function">
<dt id="edward.inferences.build_reparam_loss">
<code class="descclassname">edward.inferences.</code><code class="descname">build_reparam_loss</code><span class="sig-paren">(</span><em>inference</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L303" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>based on the reparameterization trick (Kingma and Welling, 2014).</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>
<dl class="function">
<dt id="edward.inferences.build_reparam_kl_loss">
<code class="descclassname">edward.inferences.</code><code class="descname">build_reparam_kl_loss</code><span class="sig-paren">(</span><em>inference</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L369" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  - ( E_{q(z; \lambda)} [ \log p(x | z) ]
      + KL(q(z; \lambda) || p(z)) )\]</div>
<p>based on the reparameterization trick (Kingma and Welling, 2014).</p>
<p>It assumes the KL is analytic.</p>
<p>For model wrappers, it assumes the prior is <span class="math">\(p(z) =
\mathcal{N}(z; 0, 1)\)</span>.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>
<dl class="function">
<dt id="edward.inferences.build_reparam_entropy_loss">
<code class="descclassname">edward.inferences.</code><code class="descname">build_reparam_entropy_loss</code><span class="sig-paren">(</span><em>inference</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L436" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  -( E_{q(z; \lambda)} [ \log p(x , z) ]
      + H(q(z; \lambda)) )\]</div>
<p>based on the reparameterization trick (Kingma and Welling, 2014).</p>
<p>It assumes the entropy is analytic.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>
<dl class="function">
<dt id="edward.inferences.build_score_loss_and_gradients">
<code class="descclassname">edward.inferences.</code><code class="descname">build_score_loss_and_gradients</code><span class="sig-paren">(</span><em>inference</em>, <em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L502" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function and gradients based on the score function
estimator (Paisley et al., 2012).</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>
<dl class="function">
<dt id="edward.inferences.build_score_kl_loss_and_gradients">
<code class="descclassname">edward.inferences.</code><code class="descname">build_score_kl_loss_and_gradients</code><span class="sig-paren">(</span><em>inference</em>, <em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L571" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function and gradients based on the score function
estimator (Paisley et al., 2012).</p>
<p>It assumes the KL is analytic.</p>
<p>For model wrappers, it assumes the prior is <span class="math">\(p(z) =
\mathcal{N}(z; 0, 1)\)</span>.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>
<dl class="function">
<dt id="edward.inferences.build_score_entropy_loss_and_gradients">
<code class="descclassname">edward.inferences.</code><code class="descname">build_score_entropy_loss_and_gradients</code><span class="sig-paren">(</span><em>inference</em>, <em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L645" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function and gradients based on the score function
estimator (Paisley et al., 2012).</p>
<p>It assumes the entropy is analytic.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLpq">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLpq</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[KL( p(z |x) || q(z) ).\]</div>
<p>To perform the optimization, this class uses a technique from
adaptive importance sampling (Cappe et al., 2008).</p>
<p class="rubric">Notes</p>
<p>KLqp also optimizes any model parameters p(z | x;     heta). It does
this by variational EM, minimizing</p>
<div class="math">
\[E_{p(z | x; \lambda)} [ \log p(x, z;        heta) ]\]</div>
<p>with respect to       heta.</p>
<p>In conditional inference, we infer z in p(z, eta | x) while fixing
inference over eta using another distribution q(eta).
During gradient calculation, instead of using the model’s density</p>
<div class="math">
\[\log p(x, z^{(s)}), where z^{(s)} ~ q(z; \lambda),\]</div>
<p>for each sample s=1,...,S, KLpq uses</p>
<div class="math">
\[\log p(x, z^{(s)}, eta^{(s)}), where
z^{(s)} ~ q(z; \lambda) and eta^{(s)} ~ q(beta).\]</div>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLpq.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L64" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function</p>
<div class="math">
\[KL( p(z |x) || q(z) )
=
E_{p(z | x)} [ \log p(z | x) - \log q(z; \lambda) ]\]</div>
<p>and stochastic gradients based on importance sampling.</p>
<p>The loss function can be estimated as</p>
<div class="math">
\[1/B \sum_{b=1}^B [ w_{norm}(z^b; \lambda) *
                   (\log p(x, z^b) - \log q(z^b; \lambda) ],\]</div>
<p>where</p>
<div class="math">
\[ \begin{align}\begin{aligned}z^b \sim q(z^b; \lambda),\\w_{norm}(z^b; \lambda) = w(z^b; \lambda) / \sum_{b=1}^B (w(z^b; \lambda)),\\w(z^b; \lambda) = p(x, z^b) / q(z^b; \lambda).\end{aligned}\end{align} \]</div>
<p>This provides a gradient,</p>
<div class="math">
\[- 1/B \sum_{b=1}^B [ w_{norm}(z^b; \lambda) *
                     \partial_{\lambda} \log q(z^b; \lambda) ].\]</div>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLpq.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L52" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MAP">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MAP</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Maximum a posteriori.</p>
<p>This class implements gradient-based optimization to solve the
optimization problem,</p>
<div class="math">
\[\min_{z} - p(z | x).\]</div>
<p>This is equivalent to using a <code class="docutils literal"><span class="pre">PointMass</span></code> variational distribution
and minimizing the unnormalized objective,</p>
<div class="math">
\[- E_{q(z; \lambda)} [ \log p(x, z) ].\]</div>
<p class="rubric">Notes</p>
<p>This class is currently restricted to optimization over
differentiable latent variables. For example, it does not solve
discrete optimization.</p>
<p>This class also minimizes the loss with respect to any model
parameters p(z | x;   heta).</p>
<p>In conditional inference, we infer z in p(z, eta | x) while fixing
inference over eta using another distribution q(eta).
MAP optimizes E_{q(eta)} [ log p(x, z, eta) ], leveraging a
single Monte Carlo sample, log p(x, z, eta^*), where eta^* ~
q(eta). This is a lower bound to the marginal density log p(x,
z), and it is exact if q(eta) = p(eta | x) (up to
stochasticity).</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote class="last">
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If
list, each random variable will be implictly optimized
using a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
random variable must be a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Most explicitly, MAP is specified via a dictionary:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; qpi = PointMass(params=ed.to_simplex(tf.Variable(tf.zeros(K-1))))
&gt;&gt;&gt; qmu = PointMass(params=tf.Variable(tf.zeros(K*D)))
&gt;&gt;&gt; qsigma = PointMass(params=tf.nn.softplus(tf.Variable(tf.zeros(K*D))))
&gt;&gt;&gt; MAP({pi: qpi, mu: qmu, sigma: qsigma}, data)

</code>
</pre>
<p>We also automate the specification of <code class="docutils literal"><span class="pre">PointMass</span></code> distributions,
so one can pass in a list of latent variables instead:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; MAP([beta], data)
&gt;&gt;&gt; MAP([pi, mu, sigma], data)

</code>
</pre>
<p>Currently, MAP can only instantiate <code class="docutils literal"><span class="pre">PointMass</span></code> random variables
with unconstrained support. To constrain their support, one must
manually pass in the <code class="docutils literal"><span class="pre">PointMass</span></code> family.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MAP.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L101" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is the gradient of</p>
<div class="math">
\[- \log p(x,z)\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.Laplace">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L146" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Laplace approximation.</p>
<p>It approximates the posterior distribution using a normal
distribution centered at the mode of the posterior.</p>
<p>We implement this by running <code class="docutils literal"><span class="pre">MAP</span></code> to find the posterior mode.
This forms the mean of the normal approximation. We then compute
the Hessian at the mode of the posterior. This forms the
covariance of the normal approximation.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Laplace.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L160" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Function to call after convergence.</p>
<p>Computes the Hessian at the mode.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MonteCarlo">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MonteCarlo</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Base class for Monte Carlo inference methods.</p>
<p class="rubric">Methods</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote>
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If
list, each random variable will be implictly approximated
using a <code class="docutils literal"><span class="pre">Empirical</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
random variable must be a <code class="docutils literal"><span class="pre">Empirical</span></code> random variable.</p>
</div></blockquote>
<p><strong>data</strong> : dict, optional</p>
<blockquote>
<div><p>Data dictionary which binds observed variables (of type
<cite>RandomVariable</cite>) to their realizations (of type <cite>tf.Tensor</cite>).
It can also bind placeholders (of type <cite>tf.Tensor</cite>) used in the
model to their realizations.</p>
</div></blockquote>
<p><strong>model_wrapper</strong> : ed.Model, optional</p>
<blockquote class="last">
<div><p>A wrapper for the probability model. If specified, the random
variables in <cite>latent_vars</cite>‘ dictionary keys are strings used
accordingly by the wrapper. <cite>data</cite> is also changed. For
TensorFlow, Python, and Stan models, the key type is a string;
for PyMC3, the key type is a Theano shared variable. For
TensorFlow, Python, and PyMC3 models, the value type is a NumPy
array or TensorFlow tensor; for Stan, the value type is the
type according to the Stan program’s data block.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The number of Monte Carlo iterations is set according to the
minimum of all Empirical sizes.</p>
<p>Initialization is assumed from params[0, :]. This generalizes
initializing randomly and initializing from user input. Updates
are along this outer dimension, where iteration t updates
params[t, :] in each Empirical random variable.</p>
<p>No warm-up is implemented. Users must run MCMC for a long period
of time, then manually burn in the Empirical random variable.</p>
<p class="rubric">Examples</p>
<p>Most explicitly, MonteCarlo is specified via a dictionary:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; qpi = Empirical(params=tf.Variable(tf.zeros([T, K-1])))
&gt;&gt;&gt; qmu = Empirical(params=tf.Variable(tf.zeros([T, K*D])))
&gt;&gt;&gt; qsigma = Empirical(params=tf.Variable(tf.zeros([T, K*D])))
&gt;&gt;&gt; MonteCarlo({pi: qpi, mu: qmu, sigma: qsigma}, data)

</code>
</pre>
<p>The inferred posterior is comprised of <code class="docutils literal"><span class="pre">Empirical</span></code> random
variables with <code class="docutils literal"><span class="pre">T</span></code> samples. We also automate the specification
of <code class="docutils literal"><span class="pre">Empirical</span></code> random variables. One can pass in a list of
latent variables instead:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; MonteCarlo([beta], data)
&gt;&gt;&gt; MonteCarlo([pi, mu, sigma], data)

</code>
</pre>
<p>It defaults to Empirical random variables with 10,000 samples for
each dimension.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L149" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build update, which returns an assign op for parameters in
the Empirical random variables.</p>
<p>Any derived class of <code class="docutils literal"><span class="pre">MonteCarlo</span></code> <strong>must</strong> implement
this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><strong>NotImplementedError</strong></td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L137" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L103" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of sampling for Monte Carlo.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
acceptance rate of samples since (and including) this iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>We run the increment of t separately from other ops. Whether the
others op run with the t before incrementing or after incrementing
depends on which is run faster in the TensorFlow graph. Running it
separately forces a consistent behavior.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MetropolisHastings">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MetropolisHastings</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>proposal_vars</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Metropolis-Hastings.</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer z in p(z, eta | x) while fixing
inference over eta using another distribution q(eta).
To calculate the acceptance ratio, MetropolisHastings uses an
estimate of the marginal density,</p>
<div class="math">
\[p(x, z) = E_{q(eta)} [ p(x, z, eta) ]
        pprox p(x, z, eta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where eta^* ~
q(eta). This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if q(eta) = p(eta | x).</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>proposal_vars</strong> : dict of RandomVariable to RandomVariable</p>
<blockquote class="last">
<div><p>Collection of random variables to perform inference on; each is
binded to a proposal distribution p(z’ | z).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by tf.Variables().</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; proposal_z = Normal(mu=z, sigma=0.5)
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.MetropolisHastings({z: qz}, {z: proposal_z}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MetropolisHastings.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L58" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Draw sample from proposal conditional on last sample. Then accept
or reject the sample based on the ratio,</p>
<dl class="docutils">
<dt>ratio = log p(x, znew) - log p(x, zold) +</dt>
<dd>log g(znew | zold) - log g(zold | znew)</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.HMC">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">HMC</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Hamiltonian Monte Carlo, also known as hybrid Monte Carlo
(Duane et al., 1987; Neal, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer z in p(z, eta | x) while fixing
inference over eta using another distribution q(eta).
HMC substitutes the model’s log marginal density</p>
<div class="math">
\[log p(x, z) = log E_{q(eta)} [ p(x, z, eta) ]
            pprox log p(x, z, eta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where eta^* ~
q(eta). This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if q(eta) = p(eta | x).</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.HMC({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.HMC.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L59" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Hamiltonian dynamics using a numerical integrator.
Correct for the integrator’s discretization error using an
acceptance ratio.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.HMC.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>n_steps=2</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L45" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote>
<div><p>Step size of numerical integrator.</p>
</div></blockquote>
<p><strong>n_steps</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of steps of numerical integrator.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.HMC.log_joint">
<code class="descname">log_joint</code><span class="sig-paren">(</span><em>z_sample</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L114" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Utility function to calculate model’s log joint density,
log p(x, z), for inputs z (and fixed data x).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>z_sample</strong> : dict</p>
<blockquote class="last">
<div><p>Latent variable keys to samples.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.SGLD">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">SGLD</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Stochastic gradient Langevin dynamics (Welling and Teh, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer z in p(z, eta | x) while fixing
inference over eta using another distribution q(eta).
SGLD substitutes the model’s log marginal density</p>
<div class="math">
\[log p(x, z) = log E_{q(eta)} [ p(x, z, eta) ]
            pprox log p(x, z, eta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where eta^* ~
q(eta). This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if q(eta) = p(eta | x).</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.SGLD({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.SGLD.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L54" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Langevin dynamics using a discretized integrator. Its
discretization error goes to zero as the learning rate decreases.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGLD.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L44" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote class="last">
<div><p>Constant scale factor of learning rate.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGLD.log_joint">
<code class="descname">log_joint</code><span class="sig-paren">(</span><em>z_sample</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L89" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Utility function to calculate model’s log joint density,
log p(x, z), for inputs z (and fixed data x).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>z_sample</strong> : dict</p>
<blockquote class="last">
<div><p>Latent variable keys to samples.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
