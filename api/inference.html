<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Inference</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="/images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="inference">Inference</h3>
<p>We describe how to perform inference in probabilistic models. For background, see the <a href="/tutorials/inference">Inference of Probability Models tutorial</a>.</p>
<p>Suppose we have a model <span class="math inline">\(p(\mathbf{x}, \mathbf{z}, \beta)\)</span> of data <span class="math inline">\(\mathbf{x}_{\text{train}}\)</span> with latent variables <span class="math inline">\((\mathbf{z}, \beta)\)</span>. Consider the posterior inference problem, <span class="math display">\[q(\mathbf{z}, \beta)\approx p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}}),\]</span> in which the task is to approximate the posterior <span class="math inline">\(p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}})\)</span> using a family of distributions, <span class="math inline">\(q(\mathbf{z},\beta; \lambda)\)</span>, indexed by parameters <span class="math inline">\(\lambda\)</span>.</p>
<p>In Edward, let <code>z</code> and <code>beta</code> be latent variables in the model, where we observe the random variable <code>x</code> with data <code>x_train</code>. Let <code>qz</code> and <code>qbeta</code> be random variables defined to approximate the posterior. We write this problem as follows:</p>
<pre class="python" language="Python"><code>inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})</code></pre>
<p><code>Inference</code> is an abstract class which takes two inputs. The first is a collection of latent random variables <code>beta</code> and <code>z</code>, along with “posterior variables” <code>qbeta</code> and <code>qz</code>, which are associated to their respective latent variables. The second is a collection of observed random variables <code>x</code>, which is associated to the data <code>x_train</code>.</p>
<p>Inference adjusts parameters of the distribution of <code>qbeta</code> and <code>qz</code> to be close to the posterior <span class="math inline">\(p(\mathbf{z}, \beta\,|\,\mathbf{x}_{\text{train}})\)</span>.</p>
<p>Running inference is as simple as running one method.</p>
<pre class="python" language="Python"><code>inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.run()</code></pre>
<p>Inference also supports fine control of the training procedure.</p>
<pre class="python" language="Python"><code>inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.initialize()

tf.global_variables_initializer().run()

for _ in range(inference.n_iter):
  info_dict = inference.update()
  inference.print_progress(info_dict)

inference.finalize()</code></pre>
<p><code>initialize()</code> builds the algorithm’s update rules (computational graph) for <span class="math inline">\(\lambda\)</span>; <code>tf.global_variables_initializer().run()</code> initializes <span class="math inline">\(\lambda\)</span> (TensorFlow variables in the graph); <code>update()</code> runs the graph once to update <span class="math inline">\(\lambda\)</span>, which is called in a loop until convergence; <code>finalize()</code> runs any computation as the algorithm terminates.</p>
<p>The <code>run()</code> method is a simple wrapper for this procedure.</p>
<h3 id="other-settings">Other Settings</h3>
<p>We highlight other settings during inference.</p>
<p><strong>Model parameters</strong>. Model parameters are parameters in a model that we will always compute point estimates for and not be uncertain about. They are defined with <code>tf.Variable</code>s, where the inference problem is <span class="math display">\[\hat{\theta} \leftarrow^{\text{optimize}}
p(\mathbf{x}_{\text{train}}; \theta)\]</span></p>
<pre class="python" language="Python"><code>from edward.models import Normal

theta = tf.Variable(0.0)
x = Normal(mu=tf.ones(10) * theta, sigma=1.0)

inference = ed.Inference({}, {x: x_train})</code></pre>
<p>Only a subset of inference algorithms support estimation of model parameters. (Note also that this inference example does not have any latent variables. It is only about estimating <code>theta</code> given that we observe <span class="math inline">\(\mathbf{x} = \mathbf{x}_{\text{train}}\)</span>. We can add them so that inference is both posterior inference and parameter estimation.)</p>
<p>For example, model parameters are useful when applying neural networks from high-level libraries such as Keras and TensorFlow Slim. See the <a href="/api/model-compositionality">model compositionality</a> page for more details.</p>
<p><strong>Conditional inference</strong>. In conditional inference, only a subset of the posterior is inferred while the rest are fixed using other inferences. The inference problem is <span class="math display">\[q(\beta)q(\mathbf{z})\approx
p(\mathbf{z}, \beta\mid\mathbf{x}_{\text{train}})\]</span> where parameters in <span class="math inline">\(q(\beta)\)</span> are estimated and <span class="math inline">\(q(\mathbf{z})\)</span> is fixed. In Edward, we enable conditioning by binding random variables to other random variables in <code>data</code>.</p>
<pre class="python" language="Python"><code>inference = ed.Inference({beta: qbeta}, {x: x_train, z: qz})</code></pre>
<p>In the <a href="/api/inference-compositionality">compositionality page</a>, we describe how to construct inference by composing many conditional inference algorithms.</p>
<p><strong>Implicit prior samples</strong>. Latent variables can be defined in the model without any posterior inference over them. They are implicitly marginalized out with a single sample. The inference problem is <span class="math display">\[q(\beta)\approx
p(\beta\mid\mathbf{x}_{\text{train}}, \mathbf{z}^*)\]</span> where <span class="math inline">\(\mathbf{z}^*\sim p(\mathbf{z}\mid\beta)\)</span> is a prior sample.</p>
<pre class="python" language="Python"><code>inference = ed.Inference({beta: qbeta}, {x: x_train})</code></pre>
<p>For example, implicit prior samples are useful for generative adversarial networks. Their inference problem does not require any inference over the latent variables; it uses samples from the prior.</p>
<hr/>
<dl class="class">
<dt id="edward.inferences.Inference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Inference</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/inferences/inference.py#L22" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Base class for Edward inference methods.</p>
<p class="rubric">Attributes</p>
<table class="docutils">
<colgroup>
<col width="6%"></col>
<col width="94%"></col>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>latent_vars</td>
<td>(dict of RandomVariable to RandomVariable) Collection of random variables to perform inference on. Each random variable is binded to another random variable; the latter will infer the former conditional on data.</td>
</tr>
<tr class="row-even"><td>data</td>
<td>(dict) Data dictionary whose values may vary at each session run.</td>
</tr>
<tr class="row-odd"><td>model_wrapper</td>
<td>(ed.Model or None) An optional wrapper for the probability model. If specified, the random variables in <code class="docutils literal"><span class="pre">latent_vars</span></code>‘ dictionary keys are strings used accordingly by the wrapper.</td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : dict of RandomVariable to RandomVariable, optional</p>
<blockquote>
<div><p>Collection of random variables to perform inference on. Each
random variable is binded to another random variable; the latter
will infer the former conditional on data.</p>
</div></blockquote>
<p><strong>data</strong> : dict, optional</p>
<blockquote>
<div><p>Data dictionary which binds observed variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code>) to their realizations (of type <code class="docutils literal"><span class="pre">tf.Tensor</span></code>).
It can also bind placeholders (of type <code class="docutils literal"><span class="pre">tf.Tensor</span></code>) used in the
model to their realizations; and prior latent variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code>) to posterior latent variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code>).</p>
</div></blockquote>
<p><strong>model_wrapper</strong> : ed.Model, optional</p>
<blockquote class="last">
<div><p>A wrapper for the probability model. If specified, the random
variables in <code class="docutils literal"><span class="pre">latent_vars</span></code>‘ dictionary keys are strings
used accordingly by the wrapper. <code class="docutils literal"><span class="pre">data</span></code> is also changed. For
TensorFlow, Python, and Stan models, the key type is a string;
for PyMC3, the key type is a Theano shared variable. For
TensorFlow, Python, and PyMC3 models, the value type is a NumPy
array or TensorFlow tensor; for Stan, the value type is the
type according to the Stan program’s data block.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>If <code class="docutils literal"><span class="pre">data</span></code> is not passed in, the dictionary is empty.</p>
<p>Three options are available for batch training:</p>
<ol class="arabic simple">
<li>internally if user passes in data as a dictionary of NumPy
arrays;</li>
<li>externally if user passes in data as a dictionary of
TensorFlow placeholders (and manually feeds them);</li>
<li>externally if user passes in data as TensorFlow tensors
which are the outputs of data readers.</li>
</ol>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; mu = Normal(mu=tf.constant(0.0), sigma=tf.constant(1.0))
&gt;&gt;&gt; x = Normal(mu=tf.ones(N) * mu, sigma=tf.constant(1.0))
&gt;&gt;&gt;
&gt;&gt;&gt; qmu_mu = tf.Variable(tf.random_normal([1]))
&gt;&gt;&gt; qmu_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([1])))
&gt;&gt;&gt; qmu = Normal(mu=qmu_mu, sigma=qmu_sigma)
&gt;&gt;&gt;
&gt;&gt;&gt; Inference({mu: qmu}, {x: tf.constant([0.0] * N)})

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Inference.run">
<code class="descname">run</code><span class="sig-paren">(</span><em>variables=None</em>, <em>use_coordinator=True</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/inferences/inference.py#L186" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>A simple wrapper to run inference.</p>
<ol class="arabic simple">
<li>Initialize algorithm via <code class="docutils literal"><span class="pre">initialize</span></code>.</li>
<li>(Optional) Build a TensorFlow summary writer for TensorBoard.</li>
<li>(Optional) Initialize TensorFlow variables.</li>
<li>(Optional) Start queue runners.</li>
<li>Run <code class="docutils literal"><span class="pre">update</span></code> for <code class="docutils literal"><span class="pre">self.n_iter</span></code> iterations.</li>
<li>While running, <code class="docutils literal"><span class="pre">print_progress</span></code>.</li>
<li>Finalize algorithm via <code class="docutils literal"><span class="pre">finalize</span></code>.</li>
<li>(Optional) Stop queue runners.</li>
</ol>
<p>To customize the way inference is run, run these steps
individually.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>variables</strong> : list, optional</p>
<blockquote>
<div><p>A list of TensorFlow variables to initialize during inference.
Default is to initialize all variables (this includes
reinitializing variables that were already initialized). To
avoid initializing any variables, pass in an empty list.</p>
</div></blockquote>
<p><strong>use_coordinator</strong> : bool, optional</p>
<blockquote>
<div><p>Whether to start and stop queue runners during inference using a
TensorFlow coordinator. For example, queue runners are necessary
for batch training with the <code class="docutils literal"><span class="pre">n_minibatch</span></code> argument or with
file readers.</p>
</div></blockquote>
<p><strong>*args</strong></p>
<blockquote>
<div><p>Passed into <code class="docutils literal"><span class="pre">initialize</span></code>.</p>
</div></blockquote>
<p><strong>**kwargs</strong></p>
<blockquote class="last">
<div><p>Passed into <code class="docutils literal"><span class="pre">initialize</span></code>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_iter=1000</em>, <em>n_print=None</em>, <em>n_minibatch=None</em>, <em>scale=None</em>, <em>logdir=None</em>, <em>debug=False</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/inferences/inference.py#L249" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize inference algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_iter</strong> : int, optional</p>
<blockquote>
<div><p>Number of iterations for algorithm.</p>
</div></blockquote>
<p><strong>n_print</strong> : int, optional</p>
<blockquote>
<div><p>Number of iterations for each print progress. To suppress print
progress, then specify 0. Default is <code class="docutils literal"><span class="pre">int(n_iter</span> <span class="pre">/</span> <span class="pre">10)</span></code>.</p>
</div></blockquote>
<p><strong>n_minibatch</strong> : int, optional</p>
<blockquote>
<div><p>Number of samples for data subsampling. Default is to use all
the data. <code class="docutils literal"><span class="pre">n_minibatch</span></code> is available only for TensorFlow,
Python, and PyMC3 model wrappers; use <code class="docutils literal"><span class="pre">scale</span></code> for Edward’s
language. All data must be passed in as NumPy arrays. For
subsampling details, see <code class="docutils literal"><span class="pre">tf.train.slice_input_producer</span></code> and
<code class="docutils literal"><span class="pre">tf.train.batch</span></code>.</p>
</div></blockquote>
<p><strong>scale</strong> : dict of RandomVariable to tf.Tensor, optional</p>
<blockquote>
<div><p>A scalar value to scale computation for any random variable that
it is binded to. For example, this is useful for scaling
computations with respect to local latent variables.</p>
</div></blockquote>
<p><strong>logdir</strong> : str, optional</p>
<blockquote>
<div><p>Directory where event file will be written. For details,
see <code class="docutils literal"><span class="pre">tf.summary.FileWriter</span></code>. Default is to write nothing.</p>
</div></blockquote>
<p><strong>debug</strong> : bool, optional</p>
<blockquote class="last">
<div><p>If True, add checks for <code class="docutils literal"><span class="pre">NaN</span></code> and <code class="docutils literal"><span class="pre">Inf</span></code> to all computations
in the graph. May result in substantially slower execution
times.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/inferences/inference.py#L333" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/inferences/inference.py#L367" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>info_dict</strong> : dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Inference.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/inferences/inference.py#L382" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Function to call after convergence.</p>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
