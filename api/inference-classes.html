<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Classes of Inference</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a class="button2 u-pull-right" href="https://github.com/blei-lab/edward" style="padding-right:10%">
<span style="vertical-align:middle;">Github</span> 
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" src="/images/github-mark.svg" style="vertical-align:middle;"/>
<!--</object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4 button-primary" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="classes-of-inference">Classes of Inference</h3>
<p>Inference is broadly classified under three classes: variational inference, Monte Carlo, and exact inference. We highlight how to use inference algorithms from each class.</p>
<p>As an example, we assume a mixture model with latent mixture assignments <code>z</code>, latent cluster means <code>beta</code>, and observations <code>x</code>: <span class="math display">\[p(\mathbf{x}, \mathbf{z}, \beta)
=
\text{Normal}(\mathbf{x} \mid \beta_{\mathbf{z}}, \mathbf{I})
~
\text{Categorical}(\mathbf{z}\mid \pi)
~
\text{Normal}(\beta\mid \mathbf{0}, \mathbf{I}).\]</span></p>
<h3 id="variational-inference">Variational Inference</h3>
<p>In variational inference, the idea is to posit a family of approximating distributions and to find the closest member in the family to the posterior <span class="citation">(Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999)</span>. We write an approximating family, <span class="math display">\[\begin{aligned}
q(\beta;\mu,\sigma) &amp;= \text{Normal}(\beta; \mu,\sigma), \\[1.5ex]
q(\mathbf{z};\pi) &amp;= \text{Categorical}(\mathbf{z};\pi),\end{aligned}\]</span> using TensorFlow variables to represent its parameters <span class="math inline">\(\lambda=\{\pi,\mu,\sigma\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, Normal

qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
               sigma=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Given an objective function, variational inference optimizes the family with respect to <code>tf.Variable</code>s.</p>
<p>Specific variational inference algorithms inherit from the <code>VariationalInference</code> class to define their own methods, such as a loss function and gradient. For example, we represent MAP estimation with an approximating family (<code>qbeta</code> and <code>qz</code>) of <code>PointMass</code> random variables, i.e., with all probability mass concentrated at a point.</p>
<pre class="python" language="Python"><code>from edward.models import PointMass

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = PointMass(params=tf.Variable(tf.zeros(N)))

inference = ed.MAP({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p><code>MAP</code> inherits from <code>VariationalInference</code> and defines a loss function and update rules; it uses existing optimizers inside TensorFlow.</p>
<h3 id="monte-carlo">Monte Carlo</h3>
<p>Monte Carlo approximates the posterior using samples <span class="citation">(Robert &amp; Casella, 1999)</span>. Monte Carlo is an inference where the approximating family is an empirical distribution, <span class="math display">\[\begin{aligned}
q(\beta; \{\beta^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\beta, \beta^{(t)}), \\[1.5ex]
q(\mathbf{z}; \{\mathbf{z}^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\mathbf{z}, \mathbf{z}^{(t)}).\end{aligned}\]</span> The parameters are <span class="math inline">\(\lambda=\{\beta^{(t)},\mathbf{z}^{(t)}\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Empirical

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Monte Carlo algorithms proceed by updating one sample <span class="math inline">\(\beta^{(t)},\mathbf{z}^{(t)}\)</span> at a time in the empirical approximation. Monte Carlo algorithms proceed by updating one sample <span class="math inline">\(\beta^{(t)},\mathbf{(z)}^{(t)}\)</span> at a time in the empirical approximation. Markov chain Monte Carlo does this sequentially to update the current sample (index <span class="math inline">\(t\)</span> of <code>tf.Variable</code>s) conditional on the last sample (index <span class="math inline">\(t-1\)</span> of <code>tf.Variable</code>s). Specific Monte Carlo samplers determine the update rules; they can use gradients such as in Hamiltonian Monte Carlo <span class="citation">(Neal, 2011)</span> and graph structure such as in sequential Monte Carlo <span class="citation">(Doucet, De Freitas, &amp; Gordon, 2001)</span>.</p>
<h3 id="non-bayesian-methods">Non-Bayesian Methods</h3>
<p>As a library for probabilistic modeling (not necessarily Bayesian modeling), Edward is agnostic to the paradigm for inference. This means Edward can use frequentist (population-based) inferences, strictly point estimation, and alternative foundations for parameter uncertainty.</p>
<p>For example, Edward supports non-Bayesian methods such as generative adversarial networks (GANs) <span class="citation">(Goodfellow et al., 2014)</span>. For more details, see the <a href="/tutorials/gan">GAN tutorial</a>.</p>
<p>In general, we think opening the door to non-Bayesian approaches is a crucial feature for probabilistic programming. This enables advances in other fields such as deep learning to be complementary: all is in service for probabilistic models and thus it makes sense to combine our efforts.</p>
<h3 id="exact-inference">Exact Inference</h3>
<p>This approach also extends to algorithms that usually require tedious algebraic manipulation. With symbolic algebra on the nodes of the computational graph, we can uncover conjugacy relationships between random variables. Users can then integrate out variables to automatically derive classical Gibbs <span class="citation">(Gelfand &amp; Smith, 1990)</span>, mean-field updates <span class="citation">(Bishop, 2006)</span>, and exact inference.</p>
<hr/>
<p>The classes below inherit methods from base inference classes; see the <a href="/api/inference-development">development page</a> for more details.</p>
<dl class="class">
<dt id="edward.inferences.VariationalInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">VariationalInference</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L19" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Base class for variational inference methods.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.VariationalInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>optimizer=None</em>, <em>var_list=None</em>, <em>use_prettytensor=False</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L25" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>optimizer</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the variational
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>var_list</strong> : list of tf.Variable, optional</p>
<blockquote>
<div><p>List of TensorFlow variables to optimize over. Default is all
trainable variables that <code class="docutils literal"><span class="pre">latent_vars</span></code> and <code class="docutils literal"><span class="pre">data</span></code> depend on,
excluding those that are only used in conditionals in <code class="docutils literal"><span class="pre">data</span></code>.</p>
</div></blockquote>
<p><strong>use_prettytensor</strong> : bool, optional</p>
<blockquote class="last">
<div><p><code class="docutils literal"><span class="pre">True</span></code> if aim to use PrettyTensor optimizer (when using
PrettyTensor) or <code class="docutils literal"><span class="pre">False</span></code> if aim to use TensorFlow optimizer.
Defaults to TensorFlow.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L115" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of optimizer for variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
loss function value after one iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L152" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L164" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function.</p>
<p>Any derived class of <code class="docutils literal"><span class="pre">VariationalInference</span></code> <strong>must</strong> implement
this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><strong>NotImplementedError</strong></td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective by automatically selecting from a
variety of black box inference techniques.</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">KLqp</span></code> also optimizes any model parameters <span class="math">\(p(z \mid x;
\theta)\)</span>. It does this by variational EM, minimizing</p>
<div class="math">
\[\mathbb{E}_{q(z; \lambda)} [ \log p(x, z; \theta) ]\]</div>
<p>with respect to <span class="math">\(\theta\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. During gradient calculation, instead
of using the model’s density</p>
<div class="math">
\[\log p(x, z^{(s)}), z^{(s)} \sim q(z; \lambda),\]</div>
<p>for each sample <span class="math">\(s=1,\ldots,S\)</span>, <code class="docutils literal"><span class="pre">KLqp</span></code> uses</p>
<div class="math">
\[\log p(x, z^{(s)}, \beta^{(s)}),\]</div>
<p>where <span class="math">\(z^{(s)} \sim q(z; \lambda)\)</span> and <span class="math">\(\beta^{(s)}
\sim q(\beta)\)</span>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLqp.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L56" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLqp.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L68" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Wrapper for the <code class="docutils literal"><span class="pre">KLqp</span></code> loss function.</p>
<div class="math">
\[-\text{ELBO} =
  -\mathbb{E}_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>KLqp supports</p>
<ol class="arabic simple">
<li>score function gradients (Paisley et al., 2012)</li>
<li>reparameterization gradients (Kingma and Welling, 2014)</li>
</ol>
<p>of the loss function.</p>
<p>If the variational model is a normal distribution and the prior is
standard normal, then loss function can be written as</p>
<div class="math">
\[-\mathbb{E}_{q(z; \lambda)}[\log p(x \mid z)] +
  \text{KL}( q(z; \lambda) \| p(z) ),\]</div>
<p>where the KL term is computed analytically (Kingma and Welling,
2014).</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L126" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationKLKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationKLKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L155" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient and an analytic KL term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationEntropyKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationEntropyKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L184" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient and an analytic entropy term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L214" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function
gradient.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreKLKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreKLKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L243" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function gradient
and an analytic KL term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreEntropyKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreEntropyKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L272" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function gradient
and an analytic entropy term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.GANInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">GANInference</code><span class="sig-paren">(</span><em>data</em>, <em>discriminator</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L12" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Parameter estimation with GAN-style training (Goodfellow et al.,
2014).</p>
<p>Works for the class of implicit (and differentiable) probabilistic
models. These models do not require a tractable density and assume
only a program that generates samples.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>data</strong> : dict</p>
<blockquote>
<div><p>Data dictionary which binds observed variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code> or <code class="docutils literal"><span class="pre">tf.Tensor</span></code>) to their realizations (of
type <code class="docutils literal"><span class="pre">tf.Tensor</span></code>).  It can also bind placeholders (of type
<code class="docutils literal"><span class="pre">tf.Tensor</span></code>) used in the model to their realizations.</p>
</div></blockquote>
<p><strong>discriminator</strong> : function</p>
<blockquote class="last">
<div><p>Function (with parameters) to discriminate samples. It should
output logit probabilities (real-valued) and not probabilities
in [0, 1].</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">GANInference</span></code> does not support model wrappers or latent
variable inference. Note that GAN-style training also samples from
the prior: this does not work well for latent variables that are
shared across many data points (global variables).</p>
<p>In building the computation graph for inference, the
discriminator’s parameters can be accessed with the variable scope
“Disc”.</p>
<p>GANs also only work for one observed random variable in <code class="docutils literal"><span class="pre">data</span></code>.</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=tf.zeros([100, 10]), sigma=tf.ones([100, 10]))
&gt;&gt;&gt; x = generative_network(z)
&gt;&gt;&gt;
&gt;&gt;&gt; inference = ed.GANInference({x: x_data}, discriminator)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.GANInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>optimizer=None</em>, <em>optimizer_d=None</em>, <em>global_step=None</em>, <em>global_step_d=None</em>, <em>var_list=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L60" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>optimizer</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the generator
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>optimizer_d</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the discriminator
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>global_step</strong> : tf.Variable, optional</p>
<blockquote>
<div><p>Optional <code class="docutils literal"><span class="pre">Variable</span></code> to increment by one after the variables
for the generator have been updated. See
<code class="docutils literal"><span class="pre">tf.train.Optimizer.apply_gradients</span></code>.</p>
</div></blockquote>
<p><strong>global_step_d</strong> : tf.Variable, optional</p>
<blockquote>
<div><p>Optional <code class="docutils literal"><span class="pre">Variable</span></code> to increment by one after the variables
for the discriminator have been updated. See
<code class="docutils literal"><span class="pre">tf.train.Optimizer.apply_gradients</span></code>.</p>
</div></blockquote>
<p><strong>var_list</strong> : list of tf.Variable, optional</p>
<blockquote class="last">
<div><p>List of TensorFlow variables to optimize over (in the generative
model). Default is all trainable variables that <code class="docutils literal"><span class="pre">latent_vars</span></code>
and <code class="docutils literal"><span class="pre">data</span></code> depend on.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.GANInference.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em>, <em>variables=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L133" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of optimization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
<p><strong>variables</strong> : str, optional</p>
<blockquote>
<div><p>Which set of variables to update. Either “Disc” or “Gen”.
Default is both.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
iteration number and generative and discriminative losses.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The outputted iteration number is the total number of calls to
<code class="docutils literal"><span class="pre">update</span></code>. Each update may include updating only a subset of
parameters.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.GANInference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L192" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.WGANInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">WGANInference</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/wgan_inference.py#L12" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Parameter estimation with GAN-style training (Goodfellow et al.,
2014), using the Wasserstein distance (Arjovsky et al., 2017).</p>
<p>Works for the class of implicit (and differentiable) probabilistic
models. These models do not require a tractable density and assume
only a program that generates samples.</p>
<p class="rubric">Methods</p>
<p class="rubric">Notes</p>
<p>Argument-wise, the only difference from <code class="docutils literal"><span class="pre">GANInference</span></code> is
conceptual: the <code class="docutils literal"><span class="pre">discriminator</span></code> is better described as a test
function or critic. <code class="docutils literal"><span class="pre">WGANInference</span></code> continues to use
<code class="docutils literal"><span class="pre">discriminator</span></code> only to share methods and attributes with
<code class="docutils literal"><span class="pre">GANInference</span></code>.</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=tf.zeros([100, 10]), sigma=tf.ones([100, 10]))
&gt;&gt;&gt; x = generative_network(z)
&gt;&gt;&gt;
&gt;&gt;&gt; inference = ed.WGANInference({x: x_data}, discriminator)

</code>
</pre>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLpq">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLpq</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( p(z \mid x) \| q(z) ).\]</div>
<p>To perform the optimization, this class uses a technique from
adaptive importance sampling (Cappe et al., 2008).</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">KLpq</span></code> also optimizes any model parameters <span class="math">\(p(z | x;
\theta)\)</span>. It does this by variational EM, minimizing</p>
<div class="math">
\[\mathbb{E}_{p(z \mid x; \lambda)} [ \log p(x, z; \theta) ]\]</div>
<p>with respect to <span class="math">\(\theta\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. During gradient calculation, instead
of using the model’s density</p>
<div class="math">
\[\log p(x, z^{(s)}), z^{(s)} \sim q(z; \lambda),\]</div>
<p>for each sample <span class="math">\(s=1,\ldots,S\)</span>, <code class="docutils literal"><span class="pre">KLpq</span></code> uses</p>
<div class="math">
\[\log p(x, z^{(s)}, \beta^{(s)}),\]</div>
<p>where <span class="math">\(z^{(s)} \sim q(z; \lambda)\)</span> and <span class="math">\(\beta^{(s)}
\sim q(\beta)\)</span>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLpq.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L55" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLpq.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L67" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function</p>
<div class="math">
\[\text{KL}( p(z \mid x) || q(z) )
= \mathbb{E}_{p(z \mid x)} [ \log p(z \mid x) - \log q(z; \lambda) ]\]</div>
<p>and stochastic gradients based on importance sampling.</p>
<p>The loss function can be estimated as</p>
<div class="math">
\[\frac{1}{B} \sum_{b=1}^B [
  w_{norm}(z^b; \lambda) (\log p(x, z^b) - \log q(z^b; \lambda) ],\]</div>
<p>where for <span class="math">\(z^b \sim q(z^b; \lambda)\)</span>,</p>
<div class="math">
\[w_{norm}(z^b; \lambda) = w(z^b; \lambda) / \sum_{b=1}^B w(z^b; \lambda)\]</div>
<p>normalizes the importance weights, <span class="math">\(w(z^b; \lambda) = p(x,
z^b) / q(z^b; \lambda)\)</span>.</p>
<p>This provides a gradient,</p>
<div class="math">
\[- \frac{1}{B} \sum_{b=1}^B [
  w_{norm}(z^b; \lambda) \nabla_{\lambda} \log q(z^b; \lambda) ].\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MAP">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MAP</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Maximum a posteriori.</p>
<p>This class implements gradient-based optimization to solve the
optimization problem,</p>
<div class="math">
\[\min_{z} - p(z \mid x).\]</div>
<p>This is equivalent to using a <code class="docutils literal"><span class="pre">PointMass</span></code> variational distribution
and minimizing the unnormalized objective,</p>
<div class="math">
\[- \mathbb{E}_{q(z; \lambda)} [ \log p(x, z) ].\]</div>
<p class="rubric">Notes</p>
<p>This class is currently restricted to optimization over
differentiable latent variables. For example, it does not solve
discrete optimization.</p>
<p>This class also minimizes the loss with respect to any model
parameters <span class="math">\(p(z \mid x;         heta)\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. <code class="docutils literal"><span class="pre">MAP</span></code> optimizes
<span class="math">\(\mathbb{E}_{q(\beta)} [ \log p(x, z, \beta) ]\)</span>, leveraging
a single Monte Carlo sample, <span class="math">\(\log p(x, z, \beta^*)\)</span>, where
<span class="math">\(\beta^* \sim q(\beta)\)</span>. This is a lower bound to the
marginal density <span class="math">\(\log p(x, z)\)</span>, and it is exact if
<span class="math">\(q(\beta) = p(\beta \mid x)\)</span> (up to stochasticity).</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote class="last">
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If
list, each random variable will be implictly optimized
using a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
random variable must be a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Most explicitly, <code class="docutils literal"><span class="pre">MAP</span></code> is specified via a dictionary:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; qpi = PointMass(params=ed.to_simplex(tf.Variable(tf.zeros(K-1))))
&gt;&gt;&gt; qmu = PointMass(params=tf.Variable(tf.zeros(K*D)))
&gt;&gt;&gt; qsigma = PointMass(params=tf.nn.softplus(tf.Variable(tf.zeros(K*D))))
&gt;&gt;&gt; MAP({pi: qpi, mu: qmu, sigma: qsigma}, data)

</code>
</pre>
<p>We also automate the specification of <code class="docutils literal"><span class="pre">PointMass</span></code> distributions,
so one can pass in a list of latent variables instead:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; MAP([beta], data)
&gt;&gt;&gt; MAP([pi, mu, sigma], data)

</code>
</pre>
<p>Currently, <code class="docutils literal"><span class="pre">MAP</span></code> can only instantiate <code class="docutils literal"><span class="pre">PointMass</span></code> random variables
with unconstrained support. To constrain their support, one must
manually pass in the <code class="docutils literal"><span class="pre">PointMass</span></code> family.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MAP.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L102" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is the gradient of</p>
<div class="math">
\[- \log p(x,z)\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.Laplace">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L154" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Laplace approximation.</p>
<p>It approximates the posterior distribution using a normal
distribution centered at the mode of the posterior.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Laplace.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L163" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Function to call after convergence.</p>
<p>Computes the Hessian at the mode.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MonteCarlo">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MonteCarlo</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Base class for Monte Carlo inference methods.</p>
<p class="rubric">Methods</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote>
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If
list, each random variable will be implictly approximated
using a <code class="docutils literal"><span class="pre">Empirical</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
random variable must be a <code class="docutils literal"><span class="pre">Empirical</span></code> random variable.</p>
</div></blockquote>
<p><strong>data</strong> : dict, optional</p>
<blockquote>
<div><p>Data dictionary which binds observed variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code> or <code class="docutils literal"><span class="pre">tf.Tensor</span></code>) to their realizations (of
type <code class="docutils literal"><span class="pre">tf.Tensor</span></code>). It can also bind placeholders (of type
<code class="docutils literal"><span class="pre">tf.Tensor</span></code>) used in the model to their realizations.</p>
</div></blockquote>
<p><strong>model_wrapper</strong> : ed.Model, optional</p>
<blockquote class="last">
<div><p>A wrapper for the probability model. If specified, the random
variables in <code class="docutils literal"><span class="pre">latent_vars</span></code>‘ dictionary keys are strings used
accordingly by the wrapper. <code class="docutils literal"><span class="pre">data</span></code> is also changed. For
TensorFlow, Python, and Stan models, the key type is a string;
for PyMC3, the key type is a Theano shared variable. For
TensorFlow, Python, and PyMC3 models, the value type is a NumPy
array or TensorFlow tensor; for Stan, the value type is the
type according to the Stan program’s data block.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The number of Monte Carlo iterations is set according to the
minimum of all <code class="docutils literal"><span class="pre">Empirical</span></code> sizes.</p>
<p>Initialization is assumed from <code class="docutils literal"><span class="pre">params[0,</span> <span class="pre">:]</span></code>. This generalizes
initializing randomly and initializing from user input. Updates
are along this outer dimension, where iteration t updates
<code class="docutils literal"><span class="pre">params[t,</span> <span class="pre">:]</span></code> in each <code class="docutils literal"><span class="pre">Empirical</span></code> random variable.</p>
<p>No warm-up is implemented. Users must run MCMC for a long period
of time, then manually burn in the Empirical random variable.</p>
<p class="rubric">Examples</p>
<p>Most explicitly, <code class="docutils literal"><span class="pre">MonteCarlo</span></code> is specified via a dictionary:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; qpi = Empirical(params=tf.Variable(tf.zeros([T, K-1])))
&gt;&gt;&gt; qmu = Empirical(params=tf.Variable(tf.zeros([T, K*D])))
&gt;&gt;&gt; qsigma = Empirical(params=tf.Variable(tf.zeros([T, K*D])))
&gt;&gt;&gt; MonteCarlo({pi: qpi, mu: qmu, sigma: qsigma}, data)

</code>
</pre>
<p>The inferred posterior is comprised of <code class="docutils literal"><span class="pre">Empirical</span></code> random
variables with <code class="docutils literal"><span class="pre">T</span></code> samples. We also automate the specification
of <code class="docutils literal"><span class="pre">Empirical</span></code> random variables. One can pass in a list of
latent variables instead:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; MonteCarlo([beta], data)
&gt;&gt;&gt; MonteCarlo([pi, mu, sigma], data)

</code>
</pre>
<p>It defaults to <code class="docutils literal"><span class="pre">Empirical</span></code> random variables with 10,000 samples for
each dimension.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L103" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of sampling for Monte Carlo.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
acceptance rate of samples since (and including) this iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>We run the increment of <code class="docutils literal"><span class="pre">t</span></code> separately from other ops. Whether the
others op run with the <code class="docutils literal"><span class="pre">t</span></code> before incrementing or after incrementing
depends on which is run faster in the TensorFlow graph. Running it
separately forces a consistent behavior.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L147" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L159" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build update, which returns an assign op for parameters in
the Empirical random variables.</p>
<p>Any derived class of <code class="docutils literal"><span class="pre">MonteCarlo</span></code> <strong>must</strong> implement
this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><strong>NotImplementedError</strong></td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MetropolisHastings">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MetropolisHastings</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>proposal_vars</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Metropolis-Hastings.</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
To calculate the acceptance ratio, <code class="docutils literal"><span class="pre">MetropolisHastings</span></code> uses an
estimate of the marginal density,</p>
<div class="math">
\[p(x, z) = \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
        \approx p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>proposal_vars</strong> : dict of RandomVariable to RandomVariable</p>
<blockquote class="last">
<div><p>Collection of random variables to perform inference on; each is
binded to a proposal distribution <span class="math">\(g(z' \mid z)\)</span>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; proposal_z = Normal(mu=z, sigma=0.5)
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.MetropolisHastings({z: qz}, {z: proposal_z}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MetropolisHastings.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L55" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Draw sample from proposal conditional on last sample. Then accept
or reject the sample based on the ratio,</p>
<div class="math">
\[\text{ratio} = \log p(x, z^{new}) - \log p(x, z^{old}) +
  \log g(z^{new} \mid z^{old}) - \log g(z^{old} \mid z^{new})\]</div>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by tf.Variables().</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.HMC">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">HMC</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Hamiltonian Monte Carlo, also known as hybrid Monte Carlo
(Duane et al., 1987; Neal, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">HMC</span></code> substitutes the model’s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.HMC({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.HMC.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>n_steps=2</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L47" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote>
<div><p>Step size of numerical integrator.</p>
</div></blockquote>
<p><strong>n_steps</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of steps of numerical integrator.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.HMC.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L61" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Hamiltonian dynamics using a numerical integrator.
Correct for the integrator’s discretization error using an
acceptance ratio.</p>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by tf.Variables().</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.SGLD">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">SGLD</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Stochastic gradient Langevin dynamics (Welling and Teh, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">SGLD</span></code> substitutes the model’s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.SGLD({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.SGLD.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L45" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote class="last">
<div><p>Constant scale factor of learning rate.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGLD.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L55" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Langevin dynamics using a discretized integrator. Its
discretization error goes to zero as the learning rate decreases.</p>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by tf.Variables().</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.SGHMC">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">SGHMC</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sghmc.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Stochastic gradient Hamiltonian Monte Carlo (Chen et al., 2014).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">SGHMC</span></code> substitutes the model’s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.SGHMC({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.SGHMC.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>friction=0.1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sghmc.py#L45" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote>
<div><p>Constant scale factor of learning rate.</p>
</div></blockquote>
<p><strong>friction</strong> : float, optional</p>
<blockquote class="last">
<div><p>Constant scale on the friction term in the Hamiltonian system.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGHMC.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sghmc.py#L60" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Hamiltonian dynamics with friction using a discretized
integrator. Its discretization error goes to zero as the learning rate
decreases.</p>
<p>Implements the update equations from (15) of Chen et al. (2014).</p>
</dd></dl>
</dd></dl>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-bishop2006pattern">
<p>Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. Springer New York.</p>
</div>
<div id="ref-doucet2001introduction">
<p>Doucet, A., De Freitas, N., &amp; Gordon, N. (2001). An introduction to sequential monte carlo methods. In <em>Sequential monte carlo methods in practice</em> (pp. 3–14). Springer.</p>
</div>
<div id="ref-gelfand1990sampling">
<p>Gelfand, A. E., &amp; Smith, A. F. (1990). Sampling-based approaches to calculating marginal densities. <em>Journal of the American Statistical Association</em>, <em>85</em>(410), 398–409.</p>
</div>
<div id="ref-goodfellow2014generative">
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183–233.</p>
</div>
<div id="ref-neal2011mcmc">
<p>Neal, R. M. (2011). MCMC using Hamiltonian dynamics. <em>Handbook of Markov Chain Monte Carlo</em>.</p>
</div>
<div id="ref-robert1999monte">
<p>Robert, C. P., &amp; Casella, G. (1999). <em>Monte carlo statistical methods</em>. Springer.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
