<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward â€“ Classes of Inference</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/design-philosophy">Design Philosophy</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="/images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4 button-primary" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="classes-of-inference">Classes of Inference</h3>
<p>Inference is broadly classified under three classes: variational inference, Monte Carlo, and exact inference. We highlight how to use inference algorithms from each class.</p>
<p>As an example, we assume a mixture model with latent mixture assignments <code>z</code>, latent cluster means <code>beta</code>, and observations <code>x</code>: <span class="math display">\[p(\mathbf{x}, \mathbf{z}, \beta)
=
\text{Normal}(\mathbf{x} \mid \beta_{\mathbf{z}}, \mathbf{I})
~
\text{Categorical}(\mathbf{z}\mid \pi)
~
\text{Normal}(\beta\mid \mathbf{0}, \mathbf{I}).\]</span></p>
<h3 id="variational-inference">Variational Inference</h3>
<p>In variational inference, the idea is to posit a family of approximating distributions and to find the closest member in the family to the posterior <span class="citation">(Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999)</span>. We write an approximating family, <span class="math display">\[\begin{aligned}
q(\beta;\mu,\sigma) &amp;= \text{Normal}(\beta; \mu,\sigma), \\[1.5ex]
q(\mathbf{z};\pi) &amp;= \text{Categorical}(\mathbf{z};\pi),\end{aligned}\]</span> using TensorFlow variables to represent its parameters <span class="math inline">\(\lambda=\{\pi,\mu,\sigma\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, Normal

qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
               sigma=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Given an objective function, variational inference optimizes the family with respect to <code>tf.Variable</code>s.</p>
<p>Specific variational inference algorithms inherit from the <code>VariationalInference</code> class to define their own methods, such as a loss function and gradient. For example, we represent MAP estimation with an approximating family of <code>PointMass</code> random variables, i.e., with all probability mass concentrated at a point.</p>
<pre class="python" language="Python"><code>from edward.models import PointMass

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = PointMass(params=tf.Variable(tf.zeros(N)))

inference = ed.MAP({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p><code>MAP</code> inherits from <code>VariationalInference</code> and defines a loss function and update rules; it leverages existing optimizers inside TensorFlow.</p>
<h3 id="monte-carlo">Monte Carlo</h3>
<p>Monte Carlo approximates the posterior using samples <span class="citation">(Robert &amp; Casella, 1999)</span>. We represent Monte Carlo as inference where the approximating family is an empirical distribution, <span class="math display">\[\begin{aligned}
q(\beta; \{\beta^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\beta, \beta^{(t)}), \\[1.5ex]
q(\mathbf{z}; \{\mathbf{z}^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\mathbf{z}, \mathbf{z}^{(t)}).\end{aligned}\]</span> The parameters are <span class="math inline">\(\lambda=\{\beta^{(t)},\mathbf{z}^{(t)}\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Empirical

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Monte Carlo algorithms proceed by updating one sample <span class="math inline">\(\beta^{(t)},\mathbf{z}^{(t)}\)</span> at a time in the empirical approximation. Specific Monte Carlo samplers determine the update rules; they can leverage gradients and graph structure, where applicable. Markov chain Monte Carlo does this sequentially to update the current sample (index <span class="math inline">\(t\)</span> of <code>tf.Variable</code>s) conditional on the last sample (index <span class="math inline">\(t-1\)</span> of <code>tf.Variable</code>s).</p>
<h3 id="exact-inference">Exact Inference</h3>
<p>The approach also extends to exact inference. We are developing a subpackage that does symbolic algebra on the deterministic and stochastic nodes in the computational graph; this uncovers conjugacy relationships between exponential-family random variables. This will allow users to integrate out variables and automatically derive classical Gibbs and mean-field updates <span class="citation">(Bishop, 2006)</span> without tedious algebraic manipulation.</p>
<hr/>
<dl class="class">
<dt id="edward.inferences.KLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L14" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective by automatically selecting from a
variety of black box inference techniques.</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">KLqp</span></code> also optimizes any model parameters <span class="math">\(p(z \mid x;
\theta)\)</span>. It does this by variational EM, minimizing</p>
<div class="math">
\[\mathbb{E}_{q(z; \lambda)} [ \log p(x, z; \theta) ]\]</div>
<p>with respect to <span class="math">\(\theta\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. During gradient calculation, instead
of using the modelâ€™s density</p>
<div class="math">
\[\log p(x, z^{(s)}), z^{(s)} \sim q(z; \lambda),\]</div>
<p>for each sample <span class="math">\(s=1,\ldots,S\)</span>, <code class="docutils literal"><span class="pre">KLqp</span></code> uses</p>
<div class="math">
\[\log p(x, z^{(s)}, \beta^{(s)}),\]</div>
<p>where <span class="math">\(z^{(s)} \sim q(z; \lambda)\)</span> and <span class="math">\(\beta^{(s)}
\sim q(\beta)\)</span>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLqp.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L56" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLqp.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L68" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Wrapper for the <code class="docutils literal"><span class="pre">KLqp</span></code> loss function.</p>
<div class="math">
\[-\text{ELBO} =
  -\mathbb{E}_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>KLqp supports</p>
<ol class="arabic simple">
<li>score function gradients (Paisley et al., 2012)</li>
<li>reparameterization gradients (Kingma and Welling, 2014)</li>
</ol>
<p>of the loss function.</p>
<p>If the variational model is a normal distribution and the prior is
standard normal, then loss function can be written as</p>
<div class="math">
\[-\mathbb{E}_{q(z; \lambda)}[\log p(x \mid z)] +
  \text{KL}( q(z; \lambda) \| p(z) ),\]</div>
<p>where the KL term is computed analytically (Kingma and Welling,
2014).</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L133" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationKLKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationKLKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L162" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient and an analytic KL term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationEntropyKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationEntropyKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L191" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient and an analytic entropy term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L221" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function
gradient.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreKLKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreKLKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L250" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function gradient
and an analytic KL term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreEntropyKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreEntropyKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L279" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function gradient
and an analytic entropy term.</p>
<p class="rubric">Methods</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLpq">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLpq</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( p(z \mid x) \| q(z) ).\]</div>
<p>To perform the optimization, this class uses a technique from
adaptive importance sampling (Cappe et al., 2008).</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">KLpq</span></code> also optimizes any model parameters <span class="math">\(p(z | x;
\theta)\)</span>. It does this by variational EM, minimizing</p>
<div class="math">
\[\mathbb{E}_{p(z \mid x; \lambda)} [ \log p(x, z; \theta) ]\]</div>
<p>with respect to <span class="math">\(\theta\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. During gradient calculation, instead
of using the modelâ€™s density</p>
<div class="math">
\[\log p(x, z^{(s)}), z^{(s)} \sim q(z; \lambda),\]</div>
<p>for each sample <span class="math">\(s=1,\ldots,S\)</span>, <code class="docutils literal"><span class="pre">KLpq</span></code> uses</p>
<div class="math">
\[\log p(x, z^{(s)}, \beta^{(s)}),\]</div>
<p>where <span class="math">\(z^{(s)} \sim q(z; \lambda)\)</span> and <span class="math">\(\beta^{(s)}
\sim q(\beta)\)</span>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLpq.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L55" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLpq.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L67" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function</p>
<div class="math">
\[\text{KL}( p(z \mid x) || q(z) )
= \mathbb{E}_{p(z \mid x)} [ \log p(z \mid x) - \log q(z; \lambda) ]\]</div>
<p>and stochastic gradients based on importance sampling.</p>
<p>The loss function can be estimated as</p>
<div class="math">
\[\frac{1}{B} \sum_{b=1}^B [
  w_{norm}(z^b; \lambda) (\log p(x, z^b) - \log q(z^b; \lambda) ],\]</div>
<p>where for <span class="math">\(z^b \sim q(z^b; \lambda)\)</span>,</p>
<div class="math">
\[w_{norm}(z^b; \lambda) = w(z^b; \lambda) / \sum_{b=1}^B w(z^b; \lambda)\]</div>
<p>normalizes the importance weights, <span class="math">\(w(z^b; \lambda) = p(x,
z^b) / q(z^b; \lambda)\)</span>.</p>
<p>This provides a gradient,</p>
<div class="math">
\[- \frac{1}{B} \sum_{b=1}^B [
  w_{norm}(z^b; \lambda) \nabla_{\lambda} \log q(z^b; \lambda) ].\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MAP">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MAP</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Maximum a posteriori.</p>
<p>This class implements gradient-based optimization to solve the
optimization problem,</p>
<div class="math">
\[\min_{z} - p(z \mid x).\]</div>
<p>This is equivalent to using a <code class="docutils literal"><span class="pre">PointMass</span></code> variational distribution
and minimizing the unnormalized objective,</p>
<div class="math">
\[- \mathbb{E}_{q(z; \lambda)} [ \log p(x, z) ].\]</div>
<p class="rubric">Notes</p>
<p>This class is currently restricted to optimization over
differentiable latent variables. For example, it does not solve
discrete optimization.</p>
<p>This class also minimizes the loss with respect to any model
parameters <span class="math">\(p(z \mid x;         heta)\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. <code class="docutils literal"><span class="pre">MAP</span></code> optimizes
<span class="math">\(\mathbb{E}_{q(\beta)} [ \log p(x, z, \beta) ]\)</span>, leveraging
a single Monte Carlo sample, <span class="math">\(\log p(x, z, \beta^*)\)</span>, where
<span class="math">\(\beta^* \sim q(\beta)\)</span>. This is a lower bound to the
marginal density <span class="math">\(\log p(x, z)\)</span>, and it is exact if
<span class="math">\(q(\beta) = p(\beta \mid x)\)</span> (up to stochasticity).</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote class="last">
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If
list, each random variable will be implictly optimized
using a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
random variable must be a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Most explicitly, <code class="docutils literal"><span class="pre">MAP</span></code> is specified via a dictionary:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; qpi = PointMass(params=ed.to_simplex(tf.Variable(tf.zeros(K-1))))
&gt;&gt;&gt; qmu = PointMass(params=tf.Variable(tf.zeros(K*D)))
&gt;&gt;&gt; qsigma = PointMass(params=tf.nn.softplus(tf.Variable(tf.zeros(K*D))))
&gt;&gt;&gt; MAP({pi: qpi, mu: qmu, sigma: qsigma}, data)

</code>
</pre>
<p>We also automate the specification of <code class="docutils literal"><span class="pre">PointMass</span></code> distributions,
so one can pass in a list of latent variables instead:</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; MAP([beta], data)
&gt;&gt;&gt; MAP([pi, mu, sigma], data)

</code>
</pre>
<p>Currently, <code class="docutils literal"><span class="pre">MAP</span></code> can only instantiate <code class="docutils literal"><span class="pre">PointMass</span></code> random variables
with unconstrained support. To constrain their support, one must
manually pass in the <code class="docutils literal"><span class="pre">PointMass</span></code> family.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MAP.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L102" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is the gradient of</p>
<div class="math">
\[- \log p(x,z)\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.Laplace">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L147" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Laplace approximation.</p>
<p>It approximates the posterior distribution using a normal
distribution centered at the mode of the posterior.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Laplace.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L156" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Function to call after convergence.</p>
<p>Computes the Hessian at the mode.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MetropolisHastings">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MetropolisHastings</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>proposal_vars</em>, <em>data=None</em>, <em>model_wrapper=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Metropolis-Hastings.</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
To calculate the acceptance ratio, <code class="docutils literal"><span class="pre">MetropolisHastings</span></code> uses an
estimate of the marginal density,</p>
<div class="math">
\[p(x, z) = \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
        \approx p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>proposal_vars</strong> : dict of RandomVariable to RandomVariable</p>
<blockquote class="last">
<div><p>Collection of random variables to perform inference on; each is
binded to a proposal distribution <span class="math">\(g(z' \mid z)\)</span>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by tf.Variables().</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; proposal_z = Normal(mu=z, sigma=0.5)
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.MetropolisHastings({z: qz}, {z: proposal_z}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MetropolisHastings.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L59" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Draw sample from proposal conditional on last sample. Then accept
or reject the sample based on the ratio,</p>
<div class="math">
\[\text{ratio} = \log p(x, z^{new}) - \log p(x, z^{old}) +
  \log g(z^{new} \mid z^{old}) - \log g(z^{old} \mid z^{new})\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.HMC">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">HMC</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Hamiltonian Monte Carlo, also known as hybrid Monte Carlo
(Duane et al., 1987; Neal, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">HMC</span></code> substitutes the modelâ€™s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.HMC({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.HMC.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>n_steps=2</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L46" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote>
<div><p>Step size of numerical integrator.</p>
</div></blockquote>
<p><strong>n_steps</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of steps of numerical integrator.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.HMC.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L60" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Hamiltonian dynamics using a numerical integrator.
Correct for the integratorâ€™s discretization error using an
acceptance ratio.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.SGLD">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">SGLD</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Stochastic gradient Langevin dynamics (Welling and Teh, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">SGLD</span></code> substitutes the modelâ€™s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>&gt;&gt;&gt; z = Normal(mu=0.0, sigma=1.0)
&gt;&gt;&gt; x = Normal(mu=tf.ones(10) * z, sigma=1.0)
&gt;&gt;&gt;
&gt;&gt;&gt; qz = Empirical(tf.Variable(tf.zeros([500])))
&gt;&gt;&gt; data = {x: np.array([0.0] * 10, dtype=np.float32)}
&gt;&gt;&gt; inference = ed.SGLD({z: qz}, data)

</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.SGLD.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L45" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote class="last">
<div><p>Constant scale factor of learning rate.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGLD.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L55" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Langevin dynamics using a discretized integrator. Its
discretization error goes to zero as the learning rate decreases.</p>
</dd></dl>
</dd></dl>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-bishop2006pattern">
<p>Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. Springer New York.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183â€“233.</p>
</div>
<div id="ref-robert1999monte">
<p>Robert, C. P., &amp; Casella, G. (1999). <em>Monte carlo statistical methods</em>. Springer.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
