<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Classes of Inference</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Docstrings. */
  dl.class + dl.class, dl.function + dl.function {
    /* Add border inbetween docstrings. */
    border-top: 1px solid #E1E1E1;
  }
  dt { /* Add spacing for top border. */
    margin-top: 1.0rem;
  }
  dd { /* Remove overall indenting in an entry. */
    margin-left: 0.0rem;
  }
  dl th, dl td { /* Remove extraneous padding and decorations. */
    padding: 0 15px 0 0;
    border: none;
  }
  dt em, dt span.sig-paren { /* Keep style of declarations consistent. */
    font-family: monospace, monospace;
    font-style: normal;
    font-size: 14px !important;
  }
  /* Attribute contents within a docstring. */
  dd blockquote, dl blockquote, dt blockquote { /* Reduce margins. */
    margin-left: 0.0rem;
    margin-top: 0.0rem;
    margin-bottom: 0.0rem;
  }
  dl td p { /* Reduce spacing. */
    margin-bottom: 0.75rem;
  }
  dl td.field-body { /* Add indenting. */
    padding-top: 0.75rem;
    padding-left: 2.0rem;
    display: block;
  }
  dl code { /* Keep code font size consistent with rest of contents. */
    font-size: 90%;
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="/community">Community</a>
<a class="button u-full-width" href="/contributing">Contributing</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a class="button2 u-pull-right" href="https://github.com/blei-lab/edward" style="padding-right:10%">
<span style="vertical-align:middle;">Github</span> 
      <img alt="Edward on Github" src="/images/github-mark.svg" style="vertical-align:middle;"/>
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
<a class="button3" href="/api/reference">Reference</a>
</div>
<div class="row">
<a class="button4 button-primary" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="classes-of-inference">Classes of Inference</h3>
<p>Inference is broadly classified under three classes: variational inference, Monte Carlo, and exact inference. We highlight how to use inference algorithms from each class.</p>
<p>As an example, we assume a mixture model with latent mixture assignments <code>z</code>, latent cluster means <code>beta</code>, and observations <code>x</code>: <span class="math display">\[p(\mathbf{x}, \mathbf{z}, \beta)
=
\text{Normal}(\mathbf{x} \mid \beta_{\mathbf{z}}, \mathbf{I})
~
\text{Categorical}(\mathbf{z}\mid \pi)
~
\text{Normal}(\beta\mid \mathbf{0}, \mathbf{I}).\]</span></p>
<h3 id="variational-inference">Variational Inference</h3>
<p>In variational inference, the idea is to posit a family of approximating distributions and to find the closest member in the family to the posterior <span class="citation">(Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999)</span>. We write an approximating family, <span class="math display">\[\begin{aligned}
q(\beta;\mu,\sigma) &amp;= \text{Normal}(\beta; \mu,\sigma), \\[1.5ex]
q(\mathbf{z};\pi) &amp;= \text{Categorical}(\mathbf{z};\pi),\end{aligned}\]</span> using TensorFlow variables to represent its parameters <span class="math inline">\(\lambda=\{\pi,\mu,\sigma\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, Normal

qbeta = Normal(loc=tf.Variable(tf.zeros([K, D])),
               scale=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Given an objective function, variational inference optimizes the family with respect to <code>tf.Variable</code>s.</p>
<p>Specific variational inference algorithms inherit from the <code>VariationalInference</code> class to define their own methods, such as a loss function and gradient. For example, we represent MAP estimation with an approximating family (<code>qbeta</code> and <code>qz</code>) of <code>PointMass</code> random variables, i.e., with all probability mass concentrated at a point.</p>
<pre class="python" language="Python"><code>from edward.models import PointMass

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = PointMass(params=tf.Variable(tf.zeros(N)))

inference = ed.MAP({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p><code>MAP</code> inherits from <code>VariationalInference</code> and defines a loss function and update rules; it uses existing optimizers inside TensorFlow.</p>
<h3 id="monte-carlo">Monte Carlo</h3>
<p>Monte Carlo approximates the posterior using samples <span class="citation">(Robert &amp; Casella, 1999)</span>. Monte Carlo is an inference where the approximating family is an empirical distribution, <span class="math display">\[\begin{aligned}
q(\beta; \{\beta^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\beta, \beta^{(t)}), \\[1.5ex]
q(\mathbf{z}; \{\mathbf{z}^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\mathbf{z}, \mathbf{z}^{(t)}).\end{aligned}\]</span> The parameters are <span class="math inline">\(\lambda=\{\beta^{(t)},\mathbf{z}^{(t)}\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Empirical

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Monte Carlo algorithms proceed by updating one sample <span class="math inline">\(\beta^{(t)},\mathbf{(z)}^{(t)}\)</span> at a time in the empirical approximation. Markov chain Monte Carlo does this sequentially to update the current sample (index <span class="math inline">\(t\)</span> of <code>tf.Variable</code>s) conditional on the last sample (index <span class="math inline">\(t-1\)</span> of <code>tf.Variable</code>s). Specific Monte Carlo samplers determine the update rules; they can use gradients such as in Hamiltonian Monte Carlo <span class="citation">(Neal, 2011)</span> and graph structure such as in sequential Monte Carlo <span class="citation">(Doucet, De Freitas, &amp; Gordon, 2001)</span>.</p>
<h3 id="non-bayesian-methods">Non-Bayesian Methods</h3>
<p>As a library for probabilistic modeling (not necessarily Bayesian modeling), Edward is agnostic to the paradigm for inference. This means Edward can use frequentist (population-based) inferences, strictly point estimation, and alternative foundations for parameter uncertainty.</p>
<p>For example, Edward supports non-Bayesian methods such as generative adversarial networks (GANs) <span class="citation">(Goodfellow et al., 2014)</span>. For more details, see the <a href="/tutorials/gan">GAN tutorial</a>.</p>
<p>In general, we think opening the door to non-Bayesian approaches is a crucial feature for probabilistic programming. This enables advances in other fields such as deep learning to be complementary: all is in service for probabilistic models and thus it makes sense to combine our efforts.</p>
<h3 id="exact-inference">Exact Inference</h3>
<p>In order to uncover conjugacy relationships between random variables (if they exist), we use symbolic algebra on nodes in the computational graph. Users can then integrate out variables to automatically derive classical Gibbs <span class="citation">(Gelfand &amp; Smith, 1990)</span>, mean-field updates <span class="citation">(Bishop, 2006)</span>, and exact inference.</p>
<p>For example, can calculate a conjugate posterior analytically by using the <code>ed.complete_conditional</code> function:</p>
<pre class="python" language="Python"><code>from edward.models import Bernoulli, Beta

# Beta-Bernoulli model
pi = Beta(1.0, 1.0)
x = Bernoulli(probs=pi, sample_shape=10)

# Beta posterior; it conditions on the sample tensor associated to x
pi_cond = ed.complete_conditional(pi)

# Generate samples from p(pi | x = NumPy array)
sess.run(pi_cond, {x: np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])})</code></pre>
<hr/>
<p>The classes below inherit methods from base inference classes; see the <a href="/api/inference-development">development page</a> for more details.</p>
<dl class="class">
<dt id="edward.inferences.VariationalInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">VariationalInference</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L21" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Abstract base class for variational inference. Specific
variational inference methods inherit from <code class="docutils literal"><span class="pre">VariationalInference</span></code>,
sharing methods such as a default optimizer.</p>
<p>To build an algorithm inheriting from <code class="docutils literal"><span class="pre">VariaitonalInference</span></code>, one
must at the minimum implement <code class="docutils literal"><span class="pre">build_loss_and_gradients</span></code>: it
determines the loss function and gradients to apply for a given
optimizer.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.VariationalInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>optimizer=None</em>, <em>var_list=None</em>, <em>use_prettytensor=False</em>, <em>global_step=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L34" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>optimizer</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the variational
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>var_list</strong> : list of tf.Variable, optional</p>
<blockquote>
<div><p>List of TensorFlow variables to optimize over. Default is all
trainable variables that <code class="docutils literal"><span class="pre">latent_vars</span></code> and <code class="docutils literal"><span class="pre">data</span></code> depend on,
excluding those that are only used in conditionals in <code class="docutils literal"><span class="pre">data</span></code>.</p>
</div></blockquote>
<p><strong>use_prettytensor</strong> : bool, optional</p>
<blockquote>
<div><p><code class="docutils literal"><span class="pre">True</span></code> if aim to use PrettyTensor optimizer (when using
PrettyTensor) or <code class="docutils literal"><span class="pre">False</span></code> if aim to use TensorFlow optimizer.
Defaults to TensorFlow.</p>
</div></blockquote>
<p><strong>global_step</strong> : tf.Variable, optional</p>
<blockquote class="last">
<div><p>A TensorFlow variable to hold the global step.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L142" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of optimizer for variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field"><th class="field-name">Returns:</th></tr><tr class="field-even field"><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
loss function value after one iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L178" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.VariationalInference.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/variational_inference.py#L186" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function and its gradients. They will be leveraged
in an optimizer to update the model and variational parameters.</p>
<p>Any derived class of <code class="docutils literal"><span class="pre">VariationalInference</span></code> <strong>must</strong> implement
this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Raises:</th></tr><tr class="field-odd field"><td class="field-body"><strong>NotImplementedError</strong></td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L19" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective by automatically selecting from a
variety of black box inference techniques.</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">KLqp</span></code> also optimizes any model parameters <span class="math">\(p(z \mid x;
\theta)\)</span>. It does this by variational EM, minimizing</p>
<div class="math">
\[\mathbb{E}_{q(z; \lambda)} [ \log p(x, z; \theta) ]\]</div>
<p>with respect to <span class="math">\(\theta\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. During gradient calculation, instead
of using the model’s density</p>
<div class="math">
\[\log p(x, z^{(s)}), z^{(s)} \sim q(z; \lambda),\]</div>
<p>for each sample <span class="math">\(s=1,\ldots,S\)</span>, <code class="docutils literal"><span class="pre">KLqp</span></code> uses</p>
<div class="math">
\[\log p(x, z^{(s)}, \beta^{(s)}),\]</div>
<p>where <span class="math">\(z^{(s)} \sim q(z; \lambda)\)</span> and <span class="math">\(\beta^{(s)}
\sim q(\beta)\)</span>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLqp.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>kl_scaling=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L61" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote>
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
<p><strong>kl_scaling</strong> : dict of RandomVariable to float, optional</p>
<blockquote class="last">
<div><p>Provides option to scale terms when using ELBO with KL divergence.
If the KL divergence terms are</p>
<div class="math">
\[\alpha_p \mathbb{E}_{q(z\mid x, \lambda)} [
    \log q(z\mid x, \lambda) - \log p(z)],\]</div>
<p>then pass {<span class="math">\(p(z)\)</span>: <span class="math">\(\alpha_p\)</span>} as <code class="docutils literal"><span class="pre">kl_scaling</span></code>,
where <span class="math">\(\alpha_p\)</span> is a float that specifies how much to
scale the KL term.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLqp.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L88" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Wrapper for the <code class="docutils literal"><span class="pre">KLqp</span></code> loss function.</p>
<div class="math">
\[-\text{ELBO} =
  -\mathbb{E}_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>KLqp supports</p>
<ol class="arabic simple">
<li>score function gradients (Paisley et al., 2012)</li>
<li>reparameterization gradients (Kingma and Welling, 2014)</li>
</ol>
<p>of the loss function.</p>
<p>If the KL divergence between the variational model and the prior
is tractable, then the loss function can be written as</p>
<div class="math">
\[-\mathbb{E}_{q(z; \lambda)}[\log p(x \mid z)] +
  \text{KL}( q(z; \lambda) \| p(z) ),\]</div>
<p>where the KL term is computed analytically (Kingma and Welling,
2014). We compute this automatically when <span class="math">\(p(z)\)</span> and
<span class="math">\(q(z; \lambda)\)</span> are Normal.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L141" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationKLKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationKLKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L170" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient and an analytic KL term.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ReparameterizationEntropyKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ReparameterizationEntropyKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L214" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the reparameterization
gradient and an analytic entropy term.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L244" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function
gradient.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreKLKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreKLKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L273" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function gradient
and an analytic KL term.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ScoreEntropyKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ScoreEntropyKLqp</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py#L317" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( q(z; \lambda) \| p(z \mid x) ).\]</div>
<p>This class minimizes the objective using the score function gradient
and an analytic entropy term.</p>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.GANInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">GANInference</code><span class="sig-paren">(</span><em>data</em>, <em>discriminator</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L12" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Parameter estimation with GAN-style training (Goodfellow et al.,
2014).</p>
<p>Works for the class of implicit (and differentiable) probabilistic
models. These models do not require a tractable density and assume
only a program that generates samples.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>data</strong> : dict</p>
<blockquote>
<div><p>Data dictionary which binds observed variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code> or <code class="docutils literal"><span class="pre">tf.Tensor</span></code>) to their realizations (of
type <code class="docutils literal"><span class="pre">tf.Tensor</span></code>).  It can also bind placeholders (of type
<code class="docutils literal"><span class="pre">tf.Tensor</span></code>) used in the model to their realizations.</p>
</div></blockquote>
<p><strong>discriminator</strong> : function</p>
<blockquote class="last">
<div><p>Function (with parameters) to discriminate samples. It should
output logit probabilities (real-valued) and not probabilities
in [0, 1].</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">GANInference</span></code> does not support latent variable inference. Note
that GAN-style training also samples from the prior: this does not
work well for latent variables that are shared across many data
points (global variables).</p>
<p>In building the computation graph for inference, the
discriminator’s parameters can be accessed with the variable scope
“Disc”.</p>
<p>GANs also only work for one observed random variable in <code class="docutils literal"><span class="pre">data</span></code>.</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>z = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))
x = generative_network(z)

inference = ed.GANInference({x: x_data}, discriminator)
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.GANInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>optimizer=None</em>, <em>optimizer_d=None</em>, <em>global_step=None</em>, <em>global_step_d=None</em>, <em>var_list=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L60" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize GAN inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>optimizer</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the generator
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>optimizer_d</strong> : str or tf.train.Optimizer, optional</p>
<blockquote>
<div><p>A TensorFlow optimizer, to use for optimizing the discriminator
objective. Alternatively, one can pass in the name of a
TensorFlow optimizer, and default parameters for the optimizer
will be used.</p>
</div></blockquote>
<p><strong>global_step</strong> : tf.Variable, optional</p>
<blockquote>
<div><p>Optional <code class="docutils literal"><span class="pre">Variable</span></code> to increment by one after the variables
for the generator have been updated. See
<code class="docutils literal"><span class="pre">tf.train.Optimizer.apply_gradients</span></code>.</p>
</div></blockquote>
<p><strong>global_step_d</strong> : tf.Variable, optional</p>
<blockquote>
<div><p>Optional <code class="docutils literal"><span class="pre">Variable</span></code> to increment by one after the variables
for the discriminator have been updated. See
<code class="docutils literal"><span class="pre">tf.train.Optimizer.apply_gradients</span></code>.</p>
</div></blockquote>
<p><strong>var_list</strong> : list of tf.Variable, optional</p>
<blockquote class="last">
<div><p>List of TensorFlow variables to optimize over (in the generative
model). Default is all trainable variables that <code class="docutils literal"><span class="pre">latent_vars</span></code>
and <code class="docutils literal"><span class="pre">data</span></code> depend on.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.GANInference.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em>, <em>variables=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L147" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of optimization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
<p><strong>variables</strong> : str, optional</p>
<blockquote>
<div><p>Which set of variables to update. Either “Disc” or “Gen”.
Default is both.</p>
</div></blockquote>
</td>
</tr>
<tr class="field"><th class="field-name">Returns:</th></tr><tr class="field-even field"><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
iteration number and generative and discriminative losses.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The outputted iteration number is the total number of calls to
<code class="docutils literal"><span class="pre">update</span></code>. Each update may include updating only a subset of
parameters.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.GANInference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gan_inference.py#L205" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.BiGANInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">BiGANInference</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>data</em>, <em>discriminator</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/bigan_inference.py#L12" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Adversarially Learned Inference (Dumoulin et al., 2017) or
Bidirectional Generative Adversarial Networks (Donahue et al., 2017)
for joint learning of generator and inference networks.</p>
<p>Works for the class of implicit (and differentiable) probabilistic
models. These models do not require a tractable density and assume
only a program that generates samples.</p>
<p class="rubric">Methods</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">BiGANInference</span></code> matches a mapping from data to latent variables and a
mapping from latent variables to data through a joint
discriminator.</p>
<p>In building the computation graph for inference, the
discriminator’s parameters can be accessed with the variable scope
“Disc”.
In building the computation graph for inference, the
encoder and decoder parameters can be accessed with the variable scope
“Gen”.</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>with tf.variable_scope("Gen"):
  xf = gen_data(z_ph)
  zf = gen_latent(x_ph)
inference = ed.BiGANInference({z_ph: zf}, {xf: x_ph}, discriminator)
</code>
</pre>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.WGANInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">WGANInference</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/wgan_inference.py#L17" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Parameter estimation with GAN-style training (Goodfellow et al.,
2014), using the Wasserstein distance (Arjovsky et al., 2017).</p>
<p>Works for the class of implicit (and differentiable) probabilistic
models. These models do not require a tractable density and assume
only a program that generates samples.</p>
<p class="rubric">Methods</p>
<p class="rubric">Notes</p>
<p>Argument-wise, the only difference from <code class="docutils literal"><span class="pre">GANInference</span></code> is
conceptual: the <code class="docutils literal"><span class="pre">discriminator</span></code> is better described as a test
function or critic. <code class="docutils literal"><span class="pre">WGANInference</span></code> continues to use
<code class="docutils literal"><span class="pre">discriminator</span></code> only to share methods and attributes with
<code class="docutils literal"><span class="pre">GANInference</span></code>.</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>z = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))
x = generative_network(z)

inference = ed.WGANInference({x: x_data}, discriminator)
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.WGANInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>penalty=10.0</em>, <em>clip=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/wgan_inference.py#L44" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialize Wasserstein GAN inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>penalty</strong> : float, optional</p>
<blockquote>
<div><p>Scalar value to enforce gradient penalty that ensures the
gradients have norm equal to 1 (Gulrajani et al., 2017). Set to
None (or 0.0) if using no penalty.</p>
</div></blockquote>
<p><strong>clip</strong> : float, optional</p>
<blockquote class="last">
<div><p>Value to clip weights by. Default is no clipping.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.ImplicitKLqp">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">ImplicitKLqp</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>data=None</em>, <em>discriminator=None</em>, <em>global_vars=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/implicit_klqp.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with implicit probabilistic models
(Tran et al., 2017).</p>
<p>It minimizes the KL divergence</p>
<div class="math">
\[\text{KL}( q(z, \beta; \lambda) \| p(z, \beta \mid x) ),\]</div>
<p>where <span class="math">\(z\)</span> are local variables associated to a data point and
<span class="math">\(\beta\)</span> are global variables shared across data points.</p>
<p>Global latent variables require <code class="docutils literal"><span class="pre">log_prob()</span></code> and need to return a
random sample when fetched from the graph. Local latent variables
and observed variables require only a random sample when fetched
from the graph. (This is true for both <span class="math">\(p\)</span> and <span class="math">\(q\)</span>.)</p>
<p>All variational factors must be reparameterizable: each of the
random variables (<code class="docutils literal"><span class="pre">rv</span></code>) satisfies <code class="docutils literal"><span class="pre">rv.is_reparameterized</span></code> and
<code class="docutils literal"><span class="pre">rv.is_continuous</span></code>.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>discriminator</strong> : function</p>
<blockquote>
<div><p>Function (with parameters). Unlike <code class="docutils literal"><span class="pre">GANInference</span></code>, it is
interpreted as a ratio estimator rather than a discriminator.
It takes three arguments: a data dict, local latent variable
dict, and global latent variable dict. As with GAN
discriminators, it can take a batch of data points and local
variables, of size <span class="math">\(M\)</span>, and output a vector of length
<span class="math">\(M\)</span>.</p>
</div></blockquote>
<p><strong>global_vars</strong> : dict of RandomVariable to RandomVariable, optional</p>
<blockquote class="last">
<div><p>Identifying which variables in <code class="docutils literal"><span class="pre">latent_vars</span></code> are global
variables, shared across data points. These will not be
encompassed in the ratio estimation problem, and will be
estimated with tractable variational approximations.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>Unlike <code class="docutils literal"><span class="pre">GANInference</span></code>, <code class="docutils literal"><span class="pre">discriminator</span></code> takes dict’s as input,
and must subset to the appropriate values through lexical scoping
from the previously defined model and latent variables. This is
necessary as the discriminator can take an arbitrary set of data,
latent, and global variables.</p>
<p>Note the type for <code class="docutils literal"><span class="pre">discriminator</span></code>‘s output changes when one
passes in the <code class="docutils literal"><span class="pre">scale</span></code> argument to <code class="docutils literal"><span class="pre">initialize()</span></code>.</p>
<ul class="simple">
<li>If <code class="docutils literal"><span class="pre">scale</span></code> has at most one item, then <code class="docutils literal"><span class="pre">discriminator</span></code></li>
</ul>
<p>outputs a tensor whose multiplication with that element is
broadcastable. (For example, the output is a tensor and the single
scale factor is a scalar.)
+ If <code class="docutils literal"><span class="pre">scale</span></code> has more than one item, then in order to scale
its corresponding output, <code class="docutils literal"><span class="pre">discriminator</span></code> must output a
dictionary of same size and keys as <code class="docutils literal"><span class="pre">scale</span></code>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.ImplicitKLqp.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>ratio_loss='log'</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/implicit_klqp.py#L85" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>ratio_loss</strong> : str or fn, optional</p>
<blockquote class="last">
<div><p>Loss function minimized to get the ratio estimator. ‘log’ or ‘hinge’.
Alternatively, one can pass in a function of two inputs,
<code class="docutils literal"><span class="pre">psamples</span></code> and <code class="docutils literal"><span class="pre">qsamples</span></code>, and output a point-wise value
with shape matching the shapes of the two inputs.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.ImplicitKLqp.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/implicit_klqp.py#L107" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function</p>
<div class="math">
\[-\Big(\mathbb{E}_{q(\beta)} [\log p(\beta) - \log q(\beta) ] +
  \sum_{n=1}^N \mathbb{E}_{q(\beta)q(z_n\mid\beta)} [
      r^*(x_n, z_n, \beta) ] \Big).\]</div>
<p>We minimize it with respect to parameterized variational
families <span class="math">\(q(z, \beta; \lambda)\)</span>.</p>
<p><span class="math">\(r^*(x_n, z_n, \beta)\)</span> is a function of a single data point
<span class="math">\(x_n\)</span>, single local variable <span class="math">\(z_n\)</span>, and all global
variables <span class="math">\(\beta\)</span>. It is equal to the log-ratio</p>
<div class="math">
\[\log p(x_n, z_n\mid \beta) - \log q(x_n, z_n\mid \beta),\]</div>
<p>where <span class="math">\(q(x_n)\)</span> is the empirical data distribution. Rather
than explicit calculation, <span class="math">\(r^*(x, z, \beta)\)</span> is the
solution to a ratio estimation problem, minimizing the specified
<code class="docutils literal"><span class="pre">ratio_loss</span></code>.</p>
<p>Gradients are taken using the reparameterization trick (Kingma and
Welling, 2014).</p>
<p class="rubric">Notes</p>
<p>This also includes model parameters <span class="math">\(p(x, z, \beta; \theta)\)</span>
and variational distributions with inference networks
<span class="math">\(q(z\mid x)\)</span>.</p>
<p>There are a bunch of extensions we could easily do in this
implementation:</p>
<ul class="simple">
<li>further factorizations can be used to better leverage the
graph structure for more complicated models;</li>
<li>score function gradients for global variables;</li>
<li>use more samples; this would require the <code class="docutils literal"><span class="pre">copy()</span></code> utility
function for q’s as well, and an additional loop. we opt not to
because it complicates the code;</li>
<li>analytic KL/swapping out the penalty term for the globals.</li>
</ul>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.KLpq">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLpq</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Variational inference with the KL divergence</p>
<div class="math">
\[\text{KL}( p(z \mid x) \| q(z) ).\]</div>
<p>To perform the optimization, this class uses a technique from
adaptive importance sampling (Cappe et al., 2008).</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal"><span class="pre">KLpq</span></code> also optimizes any model parameters <span class="math">\(p(z\mid x;
\theta)\)</span>. It does this by variational EM, minimizing</p>
<div class="math">
\[\mathbb{E}_{p(z \mid x; \lambda)} [ \log p(x, z; \theta) ]\]</div>
<p>with respect to <span class="math">\(\theta\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. During gradient calculation, instead
of using the model’s density</p>
<div class="math">
\[\log p(x, z^{(s)}), z^{(s)} \sim q(z; \lambda),\]</div>
<p>for each sample <span class="math">\(s=1,\ldots,S\)</span>, <code class="docutils literal"><span class="pre">KLpq</span></code> uses</p>
<div class="math">
\[\log p(x, z^{(s)}, \beta^{(s)}),\]</div>
<p>where <span class="math">\(z^{(s)} \sim q(z; \lambda)\)</span> and <span class="math">\(\beta^{(s)}
\sim q(\beta)\)</span>.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.KLpq.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L55" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>n_samples</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of samples from variational model for calculating
stochastic gradients.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.KLpq.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/klpq.py#L67" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function</p>
<div class="math">
\[\text{KL}( p(z \mid x) \| q(z) )
= \mathbb{E}_{p(z \mid x)} [ \log p(z \mid x) - \log q(z; \lambda) ]\]</div>
<p>and stochastic gradients based on importance sampling.</p>
<p>The loss function can be estimated as</p>
<div class="math">
\[\frac{1}{S} \sum_{s=1}^S [
  w_{\text{norm}}(z^s; \lambda) (\log p(x, z^s) - \log q(z^s; \lambda) ],\]</div>
<p>where for <span class="math">\(z^s \sim q(z; \lambda)\)</span>,</p>
<div class="math">
\[w_{\text{norm}}(z^s; \lambda) =
    w(z^s; \lambda) / \sum_{s=1}^S w(z^s; \lambda)\]</div>
<p>normalizes the importance weights, <span class="math">\(w(z^s; \lambda) = p(x,
z^s) / q(z^s; \lambda)\)</span>.</p>
<p>This provides a gradient,</p>
<div class="math">
\[- \frac{1}{S} \sum_{s=1}^S [
  w_{\text{norm}}(z^s; \lambda) \nabla_{\lambda} \log q(z^s; \lambda) ].\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MAP">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MAP</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L13" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Maximum a posteriori.</p>
<p>This class implements gradient-based optimization to solve the
optimization problem,</p>
<div class="math">
\[\min_{z} - p(z \mid x).\]</div>
<p>This is equivalent to using a <code class="docutils literal"><span class="pre">PointMass</span></code> variational distribution
and minimizing the unnormalized objective,</p>
<div class="math">
\[- \mathbb{E}_{q(z; \lambda)} [ \log p(x, z) ].\]</div>
<p class="rubric">Notes</p>
<p>This class is currently restricted to optimization over
differentiable latent variables. For example, it does not solve
discrete optimization.</p>
<p>This class also minimizes the loss with respect to any model
parameters <span class="math">\(p(z \mid x; \theta)\)</span>.</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>. <code class="docutils literal"><span class="pre">MAP</span></code> optimizes
<span class="math">\(\mathbb{E}_{q(\beta)} [ \log p(x, z, \beta) ]\)</span>, leveraging
a single Monte Carlo sample, <span class="math">\(\log p(x, z, \beta^*)\)</span>, where
<span class="math">\(\beta^* \sim q(\beta)\)</span>. This is a lower bound to the
marginal density <span class="math">\(\log p(x, z)\)</span>, and it is exact if
<span class="math">\(q(\beta) = p(\beta \mid x)\)</span> (up to stochasticity).</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote class="last">
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If
list, each random variable will be implictly optimized
using a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
value in the dictionary must be a <code class="docutils literal"><span class="pre">PointMass</span></code> random variable.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Most explicitly, <code class="docutils literal"><span class="pre">MAP</span></code> is specified via a dictionary:</p>
<pre class="python" language="Python"><code>qpi = PointMass(params=ed.to_simplex(tf.Variable(tf.zeros(K-1))))
qmu = PointMass(params=tf.Variable(tf.zeros(K*D)))
qsigma = PointMass(params=tf.nn.softplus(tf.Variable(tf.zeros(K*D))))
ed.MAP({pi: qpi, mu: qmu, sigma: qsigma}, data)
</code>
</pre>
<p>We also automate the specification of <code class="docutils literal"><span class="pre">PointMass</span></code> distributions,
so one can pass in a list of latent variables instead:</p>
<pre class="python" language="Python"><code>ed.MAP([beta], data)
ed.MAP([pi, mu, sigma], data)
</code>
</pre>
<p>Currently, <code class="docutils literal"><span class="pre">MAP</span></code> can only instantiate <code class="docutils literal"><span class="pre">PointMass</span></code> random variables
with unconstrained support. To constrain their support, one must
manually pass in the <code class="docutils literal"><span class="pre">PointMass</span></code> family.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MAP.build_loss_and_gradients">
<code class="descname">build_loss_and_gradients</code><span class="sig-paren">(</span><em>var_list</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/map.py#L92" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is the gradient of</p>
<div class="math">
\[- \log p(x,z)\]</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.Laplace">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>data=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/laplace.py#L19" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Laplace approximation (Laplace, 1774).</p>
<p>It approximates the posterior distribution using a multivariate
normal distribution centered at the mode of the posterior.</p>
<p>We implement this by running <code class="docutils literal"><span class="pre">MAP</span></code> to find the posterior mode.
This forms the mean of the normal approximation. We then compute the
inverse Hessian at the mode of the posterior. This forms the
covariance of the normal approximation.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>latent_vars</strong> : list of RandomVariable or</p>
<blockquote class="last">
<div><blockquote>
<div><p>dict of RandomVariable to RandomVariable</p>
</div></blockquote>
<p>Collection of random variables to perform inference on. If list,
each random variable will be implictly optimized using a
<code class="docutils literal"><span class="pre">MultivariateNormalTriL</span></code> random variable that is defined
internally (with unconstrained support). If dictionary, each
random variable must be a <code class="docutils literal"><span class="pre">MultivariateNormalDiag</span></code>,
<code class="docutils literal"><span class="pre">MultivariateNormalTriL</span></code>, or <code class="docutils literal"><span class="pre">Normal</span></code> random variable.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>If <code class="docutils literal"><span class="pre">MultivariateNormalDiag</span></code> or <code class="docutils literal"><span class="pre">Normal</span></code> random variables are
specified as approximations, then the Laplace approximation will
only produce the diagonal. This does not capture correlation among
the variables but it does not require a potentially expensive
matrix inversion.</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>X = tf.placeholder(tf.float32, [N, D])
w = Normal(loc=tf.zeros(D), scale=tf.ones(D))
y = Normal(loc=ed.dot(X, w), scale=tf.ones(N))

qw = MultivariateNormalTriL(
    loc=tf.Variable(tf.random_normal([D])),
    scale_tril=tf.Variable(tf.random_normal([D, D])))

inference = ed.Laplace({w: qw}, data={X: X_train, y: y_train})
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Laplace.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/laplace.py#L107" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Function to call after convergence.</p>
<p>Computes the Hessian at the mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote class="last">
<div><p>Feed dictionary for a TensorFlow session run during evaluation
of Hessian. It is used to feed placeholders that are not fed
during initialization.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MonteCarlo">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MonteCarlo</code><span class="sig-paren">(</span><em>latent_vars=None</em>, <em>data=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L16" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Abstract base class for Monte Carlo. Specific Monte Carlo methods
inherit from <code class="docutils literal"><span class="pre">MonteCarlo</span></code>, sharing methods in this class.</p>
<p>To build an algorithm inheriting from <code class="docutils literal"><span class="pre">MonteCarlo</span></code>, one must at the
minimum implement <code class="docutils literal"><span class="pre">build_update</span></code>: it determines how to assign
the samples in the <code class="docutils literal"><span class="pre">Empirical</span></code> approximations.</p>
<p class="rubric">Methods</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>latent_vars</strong> : list or dict, optional</p>
<blockquote>
<div><p>Collection of random variables (of type <code class="docutils literal"><span class="pre">RandomVariable</span></code> or
<code class="docutils literal"><span class="pre">tf.Tensor</span></code>) to perform inference on. If list, each random
variable will be approximated using a <code class="docutils literal"><span class="pre">Empirical</span></code> random
variable that is defined internally (with unconstrained
support). If dictionary, each value in the dictionary must be a
<code class="docutils literal"><span class="pre">Empirical</span></code> random variable.</p>
</div></blockquote>
<p><strong>data</strong> : dict, optional</p>
<blockquote class="last">
<div><p>Data dictionary which binds observed variables (of type
<code class="docutils literal"><span class="pre">RandomVariable</span></code> or <code class="docutils literal"><span class="pre">tf.Tensor</span></code>) to their realizations (of
type <code class="docutils literal"><span class="pre">tf.Tensor</span></code>). It can also bind placeholders (of type
<code class="docutils literal"><span class="pre">tf.Tensor</span></code>) used in the model to their realizations.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The number of Monte Carlo iterations is set according to the
minimum of all <code class="docutils literal"><span class="pre">Empirical</span></code> sizes.</p>
<p>Initialization is assumed from <code class="docutils literal"><span class="pre">params[0,</span> <span class="pre">:]</span></code>. This generalizes
initializing randomly and initializing from user input. Updates
are along this outer dimension, where iteration t updates
<code class="docutils literal"><span class="pre">params[t,</span> <span class="pre">:]</span></code> in each <code class="docutils literal"><span class="pre">Empirical</span></code> random variable.</p>
<p>No warm-up is implemented. Users must run MCMC for a long period
of time, then manually burn in the Empirical random variable.</p>
<p class="rubric">Examples</p>
<p>Most explicitly, <code class="docutils literal"><span class="pre">MonteCarlo</span></code> is specified via a dictionary:</p>
<pre class="python" language="Python"><code>qpi = Empirical(params=tf.Variable(tf.zeros([T, K-1])))
qmu = Empirical(params=tf.Variable(tf.zeros([T, K*D])))
qsigma = Empirical(params=tf.Variable(tf.zeros([T, K*D])))
ed.MonteCarlo({pi: qpi, mu: qmu, sigma: qsigma}, data)
</code>
</pre>
<p>The inferred posterior is comprised of <code class="docutils literal"><span class="pre">Empirical</span></code> random
variables with <code class="docutils literal"><span class="pre">T</span></code> samples. We also automate the specification
of <code class="docutils literal"><span class="pre">Empirical</span></code> random variables. One can pass in a list of
latent variables instead:</p>
<pre class="python" language="Python"><code>ed.MonteCarlo([beta], data)
ed.MonteCarlo([pi, mu, sigma], data)
</code>
</pre>
<p>It defaults to <code class="docutils literal"><span class="pre">Empirical</span></code> random variables with 10,000 samples for
each dimension.</p>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L107" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of sampling for Monte Carlo.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field"><th class="field-name">Returns:</th></tr><tr class="field-even field"><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
acceptance rate of samples since (and including) this iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>We run the increment of <code class="docutils literal"><span class="pre">t</span></code> separately from other ops. Whether the
others op run with the <code class="docutils literal"><span class="pre">t</span></code> before incrementing or after incrementing
depends on which is run faster in the TensorFlow graph. Running it
separately forces a consistent behavior.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>info_dict</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L150" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Print progress to output.</p>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.MonteCarlo.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/monte_carlo.py#L158" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Build update rules, returning an assign op for parameters in
the <code class="docutils literal"><span class="pre">Empirical</span></code> random variables.</p>
<p>Any derived class of <code class="docutils literal"><span class="pre">MonteCarlo</span></code> <strong>must</strong> implement this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Raises:</th></tr><tr class="field-odd field"><td class="field-body"><strong>NotImplementedError</strong></td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.MetropolisHastings">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MetropolisHastings</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>proposal_vars</em>, <em>data=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L19" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Metropolis-Hastings (Metropolis et al., 1953; Hastings, 1970).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
To calculate the acceptance ratio, <code class="docutils literal"><span class="pre">MetropolisHastings</span></code> uses an
estimate of the marginal density,</p>
<div class="math">
\[p(x, z) = \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
        \approx p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>proposal_vars</strong> : dict of RandomVariable to RandomVariable</p>
<blockquote class="last">
<div><p>Collection of random variables to perform inference on; each is
binded to a proposal distribution <span class="math">\(g(z' \mid z)\)</span>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>z = Normal(loc=0.0, scale=1.0)
x = Normal(loc=tf.ones(10) * z, scale=1.0)

qz = Empirical(tf.Variable(tf.zeros(500)))
proposal_z = Normal(loc=z, scale=0.5)
data = {x: np.array([0.0] * 10, dtype=np.float32)}
inference = ed.MetropolisHastings({z: qz}, {z: proposal_z}, data)
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.MetropolisHastings.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/metropolis_hastings.py#L61" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Draw sample from proposal conditional on last sample. Then
accept or reject the sample based on the ratio,</p>
<div class="math">
\[\text{ratio} =
    \log p(x, z^{\text{new}}) - \log p(x, z^{\text{old}}) +
    \log g(z^{\text{new}} \mid z^{\text{old}}) -
    \log g(z^{\text{old}} \mid z^{\text{new}})\]</div>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by <a href="#id1"><span class="problematic" id="id2">``</span></a>tf.Variable``s.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.Gibbs">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Gibbs</code><span class="sig-paren">(</span><em>latent_vars</em>, <em>proposal_vars=None</em>, <em>data=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gibbs.py#L16" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Gibbs sampling (Geman and Geman, 1984).</p>
<p class="rubric">Methods</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>proposal_vars</strong> : dict of RandomVariable to RandomVariable, optional</p>
<blockquote class="last">
<div><p>Collection of random variables to perform inference on; each is
binded to its complete conditionals which Gibbs cycles draws on.
If not specified, default is to use <code class="docutils literal"><span class="pre">ed.complete_conditional</span></code>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])

p = Beta(1.0, 1.0)
x = Bernoulli(probs=p, sample_shape=10)

qp = Empirical(tf.Variable(tf.zeros(500)))
inference = ed.Gibbs({p: qp}, data={x: x_data})
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.Gibbs.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>scan_order='random'</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gibbs.py#L47" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>scan_order</strong> : list or str, optional</p>
<blockquote class="last">
<div><p>The scan order for each Gibbs update. If list, it is the
deterministic order of latent variables. An element in the list
can be a <code class="docutils literal"><span class="pre">RandomVariable</span></code> or itself a list of
<a href="#id3"><span class="problematic" id="id4">``</span></a>RandomVariable``s (this defines a blocked Gibbs sampler). If
‘random’, will use a random order at each update.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Gibbs.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>feed_dict=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gibbs.py#L62" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Run one iteration of Gibbs sampling.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>feed_dict</strong> : dict, optional</p>
<blockquote>
<div><p>Feed dictionary for a TensorFlow session run. It is used to feed
placeholders that are not fed during initialization.</p>
</div></blockquote>
</td>
</tr>
<tr class="field"><th class="field-name">Returns:</th></tr><tr class="field-even field"><td class="field-body"><p class="first">dict</p>
<blockquote class="last">
<div><p>Dictionary of algorithm-specific information. In this case, the
acceptance rate of samples since (and including) this iteration.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.Gibbs.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/gibbs.py#L130" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by <a href="#id5"><span class="problematic" id="id6">``</span></a>tf.Variable``s.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.HMC">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">HMC</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L19" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Hamiltonian Monte Carlo, also known as hybrid Monte Carlo
(Duane et al., 1987; Neal, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">HMC</span></code> substitutes the model’s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>z = Normal(loc=0.0, scale=1.0)
x = Normal(loc=tf.ones(10) * z, scale=1.0)

qz = Empirical(tf.Variable(tf.zeros(500)))
data = {x: np.array([0.0] * 10, dtype=np.float32)}
inference = ed.HMC({z: qz}, data)
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.HMC.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>n_steps=2</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L52" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote>
<div><p>Step size of numerical integrator.</p>
</div></blockquote>
<p><strong>n_steps</strong> : int, optional</p>
<blockquote class="last">
<div><p>Number of steps of numerical integrator.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.HMC.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py#L66" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Hamiltonian dynamics using a numerical integrator.
Correct for the integrator’s discretization error using an
acceptance ratio.</p>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by <a href="#id7"><span class="problematic" id="id8">``</span></a>tf.Variable``s.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.SGLD">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">SGLD</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L18" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Stochastic gradient Langevin dynamics (Welling and Teh, 2011).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">SGLD</span></code> substitutes the model’s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>z = Normal(loc=0.0, scale=1.0)
x = Normal(loc=tf.ones(10) * z, scale=1.0)

qz = Empirical(tf.Variable(tf.zeros(500)))
data = {x: np.array([0.0] * 10, dtype=np.float32)}
inference = ed.SGLD({z: qz}, data)
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.SGLD.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L50" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote class="last">
<div><p>Constant scale factor of learning rate.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGLD.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sgld.py#L60" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Langevin dynamics using a discretized integrator. Its
discretization error goes to zero as the learning rate decreases.</p>
<p class="rubric">Notes</p>
<p>The updates assume each Empirical random variable is directly
parameterized by <a href="#id9"><span class="problematic" id="id10">``</span></a>tf.Variable``s.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="edward.inferences.SGHMC">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">SGHMC</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sghmc.py#L18" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Stochastic gradient Hamiltonian Monte Carlo (Chen et al., 2014).</p>
<p class="rubric">Notes</p>
<p>In conditional inference, we infer <span class="math">\(z\)</span> in <span class="math">\(p(z, \beta
\mid x)\)</span> while fixing inference over <span class="math">\(\beta\)</span> using another
distribution <span class="math">\(q(\beta)\)</span>.
<code class="docutils literal"><span class="pre">SGHMC</span></code> substitutes the model’s log marginal density</p>
<div class="math">
\[\log p(x, z) = \log \mathbb{E}_{q(\beta)} [ p(x, z, \beta) ]
            \approx \log p(x, z, \beta^*)\]</div>
<p>leveraging a single Monte Carlo sample, where <span class="math">\(\beta^* \sim
q(\beta)\)</span>. This is unbiased (and therefore asymptotically exact as a
pseudo-marginal method) if <span class="math">\(q(\beta) = p(\beta \mid x)\)</span>.</p>
<p class="rubric">Methods</p>
<p class="rubric">Examples</p>
<pre class="python" language="Python"><code>z = Normal(loc=0.0, scale=1.0)
x = Normal(loc=tf.ones(10) * z, scale=1.0)

qz = Empirical(tf.Variable(tf.zeros(500)))
data = {x: np.array([0.0] * 10, dtype=np.float32)}
inference = ed.SGHMC({z: qz}, data)
</code>
</pre>
<p class="rubric">Methods</p>
<dl class="method">
<dt id="edward.inferences.SGHMC.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>step_size=0.25</em>, <em>friction=0.1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sghmc.py#L50" title="Link to definition on GitHub.">[source]</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>step_size</strong> : float, optional</p>
<blockquote>
<div><p>Constant scale factor of learning rate.</p>
</div></blockquote>
<p><strong>friction</strong> : float, optional</p>
<blockquote class="last">
<div><p>Constant scale on the friction term in the Hamiltonian system.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="edward.inferences.SGHMC.build_update">
<code class="descname">build_update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/sghmc.py#L65" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Simulate Hamiltonian dynamics with friction using a discretized
integrator. Its discretization error goes to zero as the learning
rate decreases.</p>
<p>Implements the update equations from (15) of Chen et al. (2014).</p>
</dd></dl>
</dd></dl>
<span class="target" id="module-edward.inferences.conjugacy"></span><dl class="function">
<dt id="edward.inferences.conjugacy.complete_conditional">
<code class="descclassname">edward.inferences.conjugacy.</code><code class="descname">complete_conditional</code><span class="sig-paren">(</span><em>rv</em>, <em>cond_set=None</em><span class="sig-paren">)</span><a class="u-pull-right" href="https://github.com/blei-lab/edward/blob/master/edward/inferences/conjugacy/conjugacy.py#L61" title="Link to definition on GitHub.">[source]</a></dt>
<dd><p>Returns the conditional distribution <code class="docutils literal"><span class="pre">RandomVariable</span></code>
<span class="math">\(p(\text{rv} | \cdot)\)</span>.</p>
<p>This function tries to infer the conditional distribution of <code class="docutils literal"><span class="pre">rv</span></code>
given <code class="docutils literal"><span class="pre">cond_set</span></code>, a set of other <a href="#id11"><span class="problematic" id="id12">``</span></a>RandomVariable``s in the graph. It
will only be able to do this if</p>
<ol class="arabic simple">
<li><span class="math">\(p(\text{rv} | \text{cond_set})\)</span> is in a tractable
exponential family, AND</li>
<li>the truth of assumption 1 is not obscured in the TensorFlow graph.</li>
</ol>
<p>In other words, this function will do its best to recognize conjugate
relationships when they exist. But it may not always be able to do the
necessary algebra.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th></tr><tr class="field-odd field"><td class="field-body"><p class="first"><strong>rv</strong> : RandomVariable</p>
<blockquote>
<div><p>The random variable whose conditional distribution we are interested in.</p>
</div></blockquote>
<p><strong>cond_set</strong> : iterable of RandomVariable, optional</p>
<blockquote class="last">
<div><p>The set of random variables we want to condition on. Default is all
random variables in the graph. (It makes no difference if <code class="docutils literal"><span class="pre">cond_set</span></code>
does or does not include <code class="docutils literal"><span class="pre">rv</span></code>.)</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>When calling <code class="docutils literal"><span class="pre">complete_conditional()</span></code> multiple times, one should
usually pass an explicit <code class="docutils literal"><span class="pre">cond_set</span></code>. Otherwise
<code class="docutils literal"><span class="pre">complete_conditional()</span></code> will try to condition on the
<a href="#id13"><span class="problematic" id="id14">``</span></a>RandomVariable``s returned by previous calls to itself. This may
result in unpredictable behavior.</p>
</dd></dl>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-bishop2006pattern">
<p>Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. Springer New York.</p>
</div>
<div id="ref-doucet2001introduction">
<p>Doucet, A., De Freitas, N., &amp; Gordon, N. (2001). An introduction to sequential Monte Carlo methods. In <em>Sequential monte carlo methods in practice</em> (pp. 3–14). Springer.</p>
</div>
<div id="ref-gelfand1990sampling">
<p>Gelfand, A. E., &amp; Smith, A. F. (1990). Sampling-based approaches to calculating marginal densities. <em>Journal of the American Statistical Association</em>, <em>85</em>(410), 398–409.</p>
</div>
<div id="ref-goodfellow2014generative">
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183–233.</p>
</div>
<div id="ref-neal2011mcmc">
<p>Neal, R. M. (2011). MCMC using Hamiltonian dynamics. <em>Handbook of Markov Chain Monte Carlo</em>.</p>
</div>
<div id="ref-robert1999monte">
<p>Robert, C. P., &amp; Casella, G. (1999). <em>Monte carlo statistical methods</em>. Springer.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
