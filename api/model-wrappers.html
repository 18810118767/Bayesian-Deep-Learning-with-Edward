<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Model Wrappers</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
<a class="button2 u-full-width" href="/license">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="/images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="api-and-documentation">API and Documentation</h2>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3 button-primary" href="/api/model">Model</a>
<a class="button3" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/model-compositionality">Compositionality</a>
<a class="button4 button-primary" href="/api/model-wrappers">Wrappers</a>
<a class="button4" href="/api/model-development">Development</a>
</div>
</div>
<h3 id="model-wrappers">Model Wrappers</h3>
<p><strong>Note: Model wrappers are deprecated since Edward v1.1.5. Use Edward’s native language instead.</strong></p>
<p>Edward also supports other languages for specifying probability models: TensorFlow, Python, PyMC3, and Stan.</p>
<p>For modeling convenience, we recommend using the modeling language that you are most familiar with. For efficiency, we recommend not using a model wrapper, as model wrappers do not expose much model structure for inference algorithms to take advantage of. Internally, all model wrappers are also wrapped in TensorFlow so their computation represents a single node in the graph; this makes it difficult to tease apart and thus distribute their computation.</p>
<p>In general, a model wrapper is a class with the structure</p>
<pre class="python" language="Python"><code>class Model:
  def __init__(...):
    ...
    self.n_vars = ...

  def log_prob(self, xs, zs):
    log_prior = ...
    log_likelihood = ...
    return log_prior + log_likelihood

model = Model(...)</code></pre>
<p>The field <code>n_vars</code> denotes the number of latent variables in the probability model. For example, a model with a Gaussian likelihood with latent mean and variance would have <code>n_vars=2 * N</code> latent variables for <code>N</code> observations.</p>
<p>The method <code>log_prob(xs, zs)</code> calculates the logarithm of the joint density <span class="math inline">\(\log p(x,z)\)</span>. Here <code>xs</code> can be a single data point or a batch of data points. Analogously, <code>zs</code> can be a single set of latent variables, or a batch thereof.</p>
<p>We outline how to write a wrapper for each language below.</p>
<p><strong>TensorFlow.</strong> Write a class with the method <code>log_prob(xs, zs)</code>. The method defines the logarithm of a joint density, where <code>xs</code> and <code>zs</code> are Python dictionaries binding the name of a random variable to a realization. Here is an example:</p>
<pre class="python" language="Python"><code>import tensorflow as tf
from edward.stats import bernoulli, beta

class BetaBernoulli:
  """p(x, p) = Bernoulli(x | p) * Beta(p | 1, 1)"""
  def log_prob(self, xs, zs):
    log_prior = beta.logpdf(zs['p'], a=1.0, b=1.0)
    log_lik = tf.reduce_sum(bernoulli.logpmf(xs['x'], p=zs['p']))
    return log_lik + log_prior

model = BetaBernoulli()</code></pre>
<p><code>BetaBernoulli</code> defines a log joint density with a Bernoulli likelihood (for an unspecified number of data points) and a Beta prior on the Bernoulli’s success probability. <code>xs</code> is a dictionary with string <code>x</code> binded to a vector of observations. <code>zs</code> is a dictionary with string <code>z</code> binded to a sample from the one-dimensional Beta latent variable.</p>
<p>During inference the latent variable string matches the name of the model’s latent variables; the data’s string matches the names used in the model class.</p>
<pre class="python" language="Python"><code>from edward.models import Beta

qp = Beta(a=tf.nn.softplus(tf.Variable(0.0)),
          b=tf.nn.softplus(tf.Variable(0.0)))
data = {'x': np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])}
inference = ed.KLqp({'p': qp}, data, model)</code></pre>
<p>Here is a <a href="https://github.com/blei-lab/edward/blob/master/examples/tf_beta_bernoulli.py">toy script</a> that uses this model. The model class can be more complicated, containing fields or other methods required for other functionality in Edward. See the section below for more details.</p>
<p><strong>Python.</strong> Write a class that inherits from <code>PythonModel</code> and with the method <code>_py_log_prob(xs, zs)</code>. The method defines the logarithm of a joint density with the same concept as in a TensorFlow model, but where <code>xs</code> and <code>zs</code> now use NumPy arrays rather than TensorFlow tensors. Here is an example:</p>
<pre class="python" language="Python"><code>import numpy as np
from edward.models import PythonModel
from scipy.stats import bernoulli, beta

class BetaBernoulli(PythonModel):
  """p(x, p) = Bernoulli(x | p) * Beta(p | 1, 1)"""
  def _py_log_prob(self, xs, zs):
    log_prior = beta.logpdf(zs['p'], a=1.0, b=1.0)
    log_lik = np.sum(bernoulli.logpmf(xs['x'], p=zs['p']))
    return log_lik + log_prior

  model = BetaBernoulli()</code></pre>
<p>During inference the latent variable string matches the name of the model’s latent variables; the data’s string matches the names used in the model class.</p>
<pre class="python" language="Python"><code>from edward.models import Beta

qp = Beta(a=tf.nn.softplus(tf.Variable(0.0)),
          b=tf.nn.softplus(tf.Variable(0.0)))
data = {'x': np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])}
inference = ed.KLqp({'p': qp}, data, model)</code></pre>
<p>Here is a <a href="https://github.com/blei-lab/edward/blob/master/examples/np_beta_bernoulli.py">toy script</a> that uses this model.</p>
<p><strong>Stan.</strong> Write a Stan program in the form of a file or string. Then call it with <code>StanModel(file=file)</code> or <code>StanModel(model_code=model_code)</code>. Here is an example:</p>
<pre class="python" language="Python"><code>from edward.models import StanModel

model_code = """
  data {
    int&lt;lower=0&gt; N;
    int&lt;lower=0,upper=1&gt; x[N];
  }
  parameters {
    real&lt;lower=0,upper=1&gt; p;
  }
  model {
    p ~ beta(1.0, 1.0);
    for (n in 1:N)
    x[n] ~ bernoulli(p);
  }
"""
model = StanModel(model_code=model_code)</code></pre>
<p>During inference the latent variable string matches the name of the parameters from the parameter block. Analogously, the data’s string matches the name of the data from the data block.</p>
<pre class="python" language="Python"><code>from edward.models import Beta

qp = Beta(a=tf.nn.softplus(tf.Variable(0.0)),
          b=tf.nn.softplus(tf.Variable(0.0)))
data = {'N': 10, 'x': [0, 1, 0, 0, 0, 0, 0, 0, 0, 1]}
inference = ed.KLqp({'p': qp}, data, model)</code></pre>
<p>Here is a <a href="https://github.com/blei-lab/edward/blob/master/examples/stan_beta_bernoulli.py">toy script</a> that uses this model. Stan programs are convenient as <a href="https://github.com/stan-dev/example-models/wiki">there are many online examples</a>, although they are limited to probability models with differentiable latent variables. <code>StanModel</code> objects also contain no structure about the model besides how to calculate its joint density.</p>
<p><strong>PyMC3.</strong> Write a PyMC3 model whose observed values are Theano shared variables, and whose latent variables use <code>transform=None</code> to keep them on their original (constrained) domain. The values in the Theano shared variables can be plugged at a later time. Here is an example:</p>
<pre class="python" language="Python"><code>import numpy as np
import pymc3 as pm
import theano
from edward.models import PyMC3Model

x_obs = theano.shared(np.zeros(1))
with pm.Model() as pm_model:
  p = pm.Beta('p', 1, 1, transform=None)
  x = pm.Bernoulli('x', p, observed=x_obs)

model = PyMC3Model(pm_model)</code></pre>
<p>During inference the latent variable string matches the name of the model’s latent variables; the data’s string matches the Theano shared variables.</p>
<pre class="python" language="Python"><code>from edward.models import Beta

qp = Beta(a=tf.nn.softplus(tf.Variable(0.0)),
          b=tf.nn.softplus(tf.Variable(0.0)))
data = {x_obs: np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])}
inference = ed.KLqp({'p': qp}, data, model)</code></pre>
<p>Here is a <a href="https://github.com/blei-lab/edward/blob/master/examples/pymc3_beta_bernoulli.py">toy script</a> that uses this model. PyMC3 can be used to define models with both differentiable latent variables and non-differentiable (e.g., discrete) latent variables. <code>PyMC3Model</code> objects contain no structure about the model besides how to calculate its joint density.</p>
<p>Note also that for these model wrappers, all three use cases for training models with data are supported. However, Stan is limited to training over the full data per step. (This because Stan’s data structure requires data subsampling on arbitrary data types, which we don’t know how to automate.)</p>
<h3 id="criticism-point-based-evaluation">Criticism: Point-based evaluation</h3>
<p>For model wrappers, Edward implements point-based evaluations through the <code>predict()</code> method in the class object. It predicts the label given samples from the posterior <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span>, i.e., it is the mean of <span class="math inline">\(p(\mathbf{x}_\text{new} \mid \mathbf{z})\)</span> for a posterior sample.</p>
<pre class="python" language="Python"><code>class BayesianLinearRegression:
  ...
  def predict(self, xs, zs):
    """Return a prediction for each data point, via the likelihood's
    mean."""
    x = xs['x']
    w, b = zs['w'], zs['b']
    return ed.dot(x, w) + b

model = BayesianLinearRegression()</code></pre>
<p>Examples of evaluation on model wrappers are</p>
<pre class="python" language="Python"><code>ed.evaluate('categorical_accuracy', data={'y': y_train, 'x': x_train},
            latent_vars={'z': qz}, model_wrapper=model)
ed.evaluate('mean_absolute_error', data={'y': y_train, 'x': x_train},
            latent_vars={'z': qz}, model_wrapper=model)</code></pre>
<h3 id="criticism-posterior-predictive-checks">Criticism: Posterior predictive checks</h3>
<p>For model wrappers, Edward implements PPCs through the <code>sample_likelihood()</code> method in the class object. This method samples a dataset <span class="math inline">\(\mathbf{x}_{\text{new}}\mid \mathbf{z}\sim
p(\mathbf{x}_{\text{new}}\mid \mathbf{z})\)</span> from the model likelihood given a set of latent variables.</p>
<pre class="python" language="Python"><code>class BetaBernoulli:
  ...
  def sample_likelihood(self, zs):
    """x | p ~ p(x | p)"""
    return {'x': bernoulli.sample(p=tf.ones(10) * zs['p'])}</code></pre>
<p>An examples of a PPC for model wrappers is</p>
<pre class="python" language="Python"><code>def T(xs, zs):
  return tf.reduce_mean(xs['x'])

ed.ppc(T, data={'x': x_train}, latent_vars={'z': qz}, model_wrapper=model)</code></pre>
<p>For model wrappers, the latent variables must always be passed in regardless of whether <code>T</code> is a function of them.</p>
<h3 id="remarks-lists-during-inference">Remarks: Lists during Inference</h3>
<p>Inference algorithms which accept lists as an argument to <code>latent_vars</code> do not necessary accept a list when using model wrappers. We outline this below.</p>
<p><strong>MAP.</strong> For model wrappers, the list can only have one element as an argument to <code>latent_vars</code>:</p>
<pre class="python" language="Python"><code>ed.MAP(['z'], data, model_wrapper)</code></pre>
<p>For example, the following is not supported:</p>
<pre class="python" language="Python"><code>ed.MAP(['pi', 'mu', 'sigma'], data, model_wrapper)</code></pre>
<p>This is because internally with model wrappers, we have no way of knowing the dimensions in which to optimize each distribution; further, we do not know their support. For more than one random variable, or for constrained support, one must explicitly pass in the point mass distributions.</p>
<p><strong>MonteCarlo.</strong> For model wrappers, lists are not supported as an argument to <code>latent_vars</code>, e.g.,</p>
<pre class="python" language="Python"><code>ed.MonteCarlo(['z'], data, model_wrapper)</code></pre>
<p>This is because internally with model wrappers, we have no way of knowing the dimensions in which to infer each latent variable. One must explicitly pass in the Empirical random variables.</p>
<h3 id="model-wrapper-api">Model Wrapper API</h3>
<p>This outlines the current spec for all methods in the model object. It includes all modeling languages, where certain methods are implemented by wrapping around other methods. For example, a Python model builds a <code>_py_log_prob()</code> method and inherits from <code>PythonModel</code>; <code>PythonModel</code> implements <code>log_prob()</code> by wrapping around <code>_py_log_prob()</code> as a TensorFlow operation.</p>
<pre class="python" language="Python"><code>class Model:
  def log_prob(self, xs, zs):
    """
    Used in: (most) inference.

    Parameters
    ----------
    xs : dict of str to tf.Tensor
      Data dictionary. Each key names a data structure used in the
      model (str), and its value is the corresponding realization
      (tf.Tensor).
    zs : dict of str to tf.Tensor
      Latent variable dictionary. Each key names a latent variable
      used in the model (str), and its value is the corresponding
      realization (tf.Tensor).

    Returns
    -------
    tf.Tensor
      Scalar, the log joint density log p(xs, zs).
    """
    pass

  def log_lik(self, xs, zs):
    """
    Used in: inference with analytic KL.

    Parameters
    ----------
    xs : dict of str to tf.Tensor
      Data dictionary. Each key names a data structure used in the
      model (str), and its value is the corresponding realization
      (tf.Tensor).
    zs : dict of str to tf.Tensor
      Latent variable dictionary. Each key names a latent variable
      used in the model (str), and its value is the corresponding
      realization (tf.Tensor).

    Returns
    -------
    tf.Tensor
      Scalar, the log-likelihood log p(xs | zs).
    """

  def predict(self, xs, zs):
    """
    Used in: ed.evaluate().

    Parameters
    ----------
    xs : dict of str to tf.Tensor
      Data dictionary. Each key names a data structure used in the
      model (str), and its value is the corresponding realization
      (tf.Tensor).
    zs : dict of str to tf.Tensor
      Latent variable dictionary. Each key names a latent variable
      used in the model (str), and its value is the corresponding
      realization (tf.Tensor).

    Returns
    -------
    tf.Tensor
      Tensor of predictions, one for each data point. The prediction
      is the likelihood's mean. For example, in supervised learning
      of i.i.d. categorical data, it is a vector of labels.
    """
    pass

  def sample_prior(self):
    """
    Used in: ed.ppc().

    Returns
    -------
    dict of str to tf.Tensor
      Latent variable dictionary. Each key names a latent variable
      used in the model (str), and its value is the corresponding
      realization (tf.Tensor).
    """
    pass

  def sample_likelihood(self, zs):
    """
    Used in: ed.ppc().

    Parameters
    ----------
    zs : dict of str to tf.Tensor
      Latent variable dictionary. Each key names a latent variable
      used in the model (str), and its value is the corresponding
      realization (tf.Tensor).

    Returns
    -------
    dict of str to tf.Tensor
      Data dictionary. It is a replicated data set, where each key
      and value matches the same type as any observed data set that
      the model aims to capture.
    """
    pass</code></pre>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
