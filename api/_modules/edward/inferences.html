

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>edward.inferences &mdash; Edward 1.0.9 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Edward 1.0.9 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Edward
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../inferences.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../criticisms.html">Criticism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../edward.html">edward package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">Edward</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../index.html">Module code</a> &raquo;</li>
      
    <li>edward.inferences</li>
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for edward.inferences</h1><div class="highlight"><pre>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">StanModel</span><span class="p">,</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">PointMass</span>
<span class="kn">from</span> <span class="nn">edward.util</span> <span class="k">import</span> <span class="n">get_dims</span><span class="p">,</span> <span class="n">get_session</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> \
    <span class="n">kl_multivariate_normal</span><span class="p">,</span> <span class="n">log_sum_exp</span>

<span class="k">try</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">prettytensor</span> <span class="k">as</span> <span class="nn">pt</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
  <span class="k">pass</span>


<div class="viewcode-block" id="Inference"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.Inference">[docs]</a><span class="k">class</span> <span class="nc">Inference</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for Edward inference methods.</span>

<span class="sd">  Attributes</span>
<span class="sd">  ----------</span>
<span class="sd">  latent_vars : dict of str to RandomVariable</span>
<span class="sd">    Collection of random variables to perform inference on. Each</span>
<span class="sd">    random variable (of type `str`) is binded to another random</span>
<span class="sd">    variable (of type `RandomVariable`); the latter will infer the</span>
<span class="sd">    former conditional on data.</span>
<span class="sd">  data : dict</span>
<span class="sd">    Data dictionary whose values may vary at each session run.</span>
<span class="sd">  model_wrapper : ed.Model</span>
<span class="sd">    Probability model.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="o">=</span><span class="k">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    latent_vars : dict of str to RandomVariable</span>
<span class="sd">      Collection of random variables to perform inference on. Each</span>
<span class="sd">      random variable (of type `str`) is binded to another random</span>
<span class="sd">      variable (of type `RandomVariable`); the latter will infer the</span>
<span class="sd">      former conditional on data.</span>
<span class="sd">    data : dict, optional</span>
<span class="sd">      Data dictionary. For TensorFlow, Python, and Stan models,</span>
<span class="sd">      the key type is a string; for PyMC3, the key type is a</span>
<span class="sd">      Theano shared variable. For TensorFlow, Python, and PyMC3</span>
<span class="sd">      models, the value type is a NumPy array or TensorFlow</span>
<span class="sd">      tensor; for Stan, the value type is the type</span>
<span class="sd">      according to the Stan program&#39;s data block.</span>
<span class="sd">    model_wrapper : ed.Model</span>
<span class="sd">      Probability model.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    If ``data`` is not passed in, the dictionary is empty.</span>

<span class="sd">    Three options are available for batch training:</span>
<span class="sd">    1. internally if user passes in data as a dictionary of NumPy</span>
<span class="sd">       arrays;</span>
<span class="sd">    2. externally if user passes in data as a dictionary of</span>
<span class="sd">       TensorFlow placeholders (and manually feeds them);</span>
<span class="sd">    3. externally if user passes in data as TensorFlow tensors</span>
<span class="sd">       which are the outputs of data readers.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; model = LinearModel()</span>
<span class="sd">    &gt;&gt;&gt; qz = Normal(model.n_vars)</span>
<span class="sd">    &gt;&gt;&gt; data = {&#39;x&#39;: np.array(), &#39;y&#39;: np.array()}</span>
<span class="sd">    &gt;&gt;&gt; Inference({&#39;z&#39;: qz}, data, model)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">get_session</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="k">None</span><span class="p">:</span>
      <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">model_wrapper</span> <span class="ow">is</span> <span class="k">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span> <span class="o">=</span> <span class="n">latent_vars</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span> <span class="o">=</span> <span class="n">model_wrapper</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_wrapper</span><span class="p">,</span> <span class="n">StanModel</span><span class="p">):</span>
      <span class="c"># Stan models do no support data subsampling because they</span>
      <span class="c"># take arbitrary data structure types in the data block</span>
      <span class="c"># and not just NumPy arrays (this makes it unamenable to</span>
      <span class="c"># TensorFlow placeholders). Therefore fix the data</span>
      <span class="c"># dictionary ``self.data`` at compile time to ``data``.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
          <span class="c"># If ``data`` has TensorFlow placeholders, the user</span>
          <span class="c"># must manually feed them at each step of</span>
          <span class="c"># inference.</span>
          <span class="c"># If ``data`` has tensors that are the output of</span>
          <span class="c"># data readers, then batch training operates</span>
          <span class="c"># according to the reader.</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
          <span class="c"># If ``data`` has NumPy arrays, store the data</span>
          <span class="c"># in the computational graph.</span>
          <span class="n">placeholder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
          <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">placeholder</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="k">False</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="p">[])</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span>
          <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span> <span class="p">{</span><span class="n">placeholder</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>


<div class="viewcode-block" id="MonteCarlo"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MonteCarlo">[docs]</a><span class="k">class</span> <span class="nc">MonteCarlo</span><span class="p">(</span><span class="n">Inference</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for Monte Carlo inference methods.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="o">=</span><span class="k">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    latent_vars : dict of str to RandomVariable</span>
<span class="sd">      Collection of random variables to perform inference on. Each</span>
<span class="sd">      random variable (of type `str`) is binded to another random</span>
<span class="sd">      variable (of type `RandomVariable`); the latter will infer the</span>
<span class="sd">      former conditional on data.</span>
<span class="sd">    data : dict, optional</span>
<span class="sd">      Data dictionary. For TensorFlow, Python, and Stan models,</span>
<span class="sd">      the key type is a string; for PyMC3, the key type is a</span>
<span class="sd">      Theano shared variable. For TensorFlow, Python, and PyMC3</span>
<span class="sd">      models, the value type is a NumPy array or TensorFlow</span>
<span class="sd">      placeholder; for Stan, the value type is the type</span>
<span class="sd">      according to the Stan program&#39;s data block.</span>
<span class="sd">    model_wrapper : ed.Model</span>
<span class="sd">      Probability model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MonteCarlo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="p">)</span></div>


<div class="viewcode-block" id="VariationalInference"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference">[docs]</a><span class="k">class</span> <span class="nc">VariationalInference</span><span class="p">(</span><span class="n">Inference</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for variational inference methods.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="o">=</span><span class="k">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    latent_vars : dict of str to RandomVariable</span>
<span class="sd">      Collection of random variables to perform inference on. Each</span>
<span class="sd">      random variable (of type `str`) is binded to another random</span>
<span class="sd">      variable (of type `RandomVariable`); the latter will infer the</span>
<span class="sd">      former conditional on data.</span>
<span class="sd">    data : dict, optional</span>
<span class="sd">      Data dictionary. For TensorFlow, Python, and Stan models,</span>
<span class="sd">      the key type is a string; for PyMC3, the key type is a</span>
<span class="sd">      Theano shared variable. For TensorFlow, Python, and PyMC3</span>
<span class="sd">      models, the value type is a NumPy array or TensorFlow</span>
<span class="sd">      placeholder; for Stan, the value type is the type</span>
<span class="sd">      according to the Stan program&#39;s data block.</span>
<span class="sd">    model_wrapper : ed.Model</span>
<span class="sd">      Probability model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">VariationalInference</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="p">)</span>

<div class="viewcode-block" id="VariationalInference.run"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference.run">[docs]</a>  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A simple wrapper to run variational inference.</span>

<span class="sd">    1. Initialize via ``initialize``.</span>
<span class="sd">    2. Run ``update`` for ``self.n_iter`` iterations.</span>
<span class="sd">    3. While running, ``print_progress``.</span>
<span class="sd">    4. Finalize via ``finalize``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    *args</span>
<span class="sd">      Passed into ``initialize``.</span>
<span class="sd">    **kwargs</span>
<span class="sd">      Passed into ``initialize``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">print_progress</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span></div>

<div class="viewcode-block" id="VariationalInference.initialize"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference.initialize">[docs]</a>  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_minibatch</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">n_print</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">logdir</span><span class="o">=</span><span class="k">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize variational inference algorithm.</span>

<span class="sd">    Set up ``tf.train.AdamOptimizer`` with a decaying scale factor.</span>

<span class="sd">    Initialize all variables.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_iter : int, optional</span>
<span class="sd">      Number of iterations for optimization.</span>
<span class="sd">    n_minibatch : int, optional</span>
<span class="sd">      Number of samples for data subsampling. Default is to use</span>
<span class="sd">      all the data. Subsampling is available only if all data</span>
<span class="sd">      passed in are NumPy arrays and the model is not a Stan</span>
<span class="sd">      model. For subsampling details, see</span>
<span class="sd">      ``tf.train.slice_input_producer`` and ``tf.train.batch``.</span>
<span class="sd">    n_print : int, optional</span>
<span class="sd">      Number of iterations for each print progress. To suppress print</span>
<span class="sd">      progress, then specify None.</span>
<span class="sd">    optimizer : str, optional</span>
<span class="sd">      Whether to use TensorFlow optimizer or PrettyTensor</span>
<span class="sd">      optimizer when using PrettyTensor. Defaults to TensorFlow.</span>
<span class="sd">    scope : str, optional</span>
<span class="sd">      Scope of TensorFlow variable objects to optimize over.</span>
<span class="sd">    logdir : str, optional</span>
<span class="sd">      Directory where event file will be written. For details,</span>
<span class="sd">      see `tf.train.SummaryWriter`. Default is to write nothing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_minibatch</span> <span class="o">=</span> <span class="n">n_minibatch</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_print</span> <span class="o">=</span> <span class="n">n_print</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_minibatch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="k">None</span> <span class="ow">and</span> \
       <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="p">,</span> <span class="n">StanModel</span><span class="p">):</span>
      <span class="c"># Re-assign data to batch tensors, with size given by</span>
      <span class="c"># ``n_minibatch``.</span>
      <span class="n">values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
      <span class="n">slices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">slice_input_producer</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
      <span class="c"># By default use as many threads as CPUs.</span>
      <span class="n">batches</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">slices</span><span class="p">,</span> <span class="n">n_minibatch</span><span class="p">,</span>
                               <span class="n">num_threads</span><span class="o">=</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">())</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batches</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c"># ``tf.train.batch`` returns tf.Tensor if ``slices`` is a</span>
        <span class="c"># list of size 1.</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">batches</span><span class="p">]</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span>
                   <span class="nb">zip</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">iterkeys</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">batches</span><span class="p">)}</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_loss</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="k">None</span><span class="p">:</span>
      <span class="n">var_list</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
                                   <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
      <span class="c"># Use ADAM with a decaying scale factor.</span>
      <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="k">False</span><span class="p">)</span>
      <span class="n">starter_learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
      <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">(</span><span class="n">starter_learning_rate</span><span class="p">,</span>
                                                 <span class="n">global_step</span><span class="p">,</span>
                                                 <span class="mi">100</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">staircase</span><span class="o">=</span><span class="k">True</span><span class="p">)</span>
      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">,</span>
                                      <span class="n">var_list</span><span class="o">=</span><span class="n">var_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="k">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s">&quot;PrettyTensor optimizer does not accept &quot;</span>
                                  <span class="s">&quot;a variable scope.&quot;</span><span class="p">)</span>

      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">apply_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">losses</span><span class="o">=</span><span class="p">[</span><span class="n">loss</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">logdir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="k">None</span><span class="p">:</span>
      <span class="n">train_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">SummaryWriter</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>

    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

    <span class="c"># Start input enqueue threads.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coord</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Coordinator</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">threads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">start_queue_runners</span><span class="p">(</span><span class="n">coord</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">coord</span><span class="p">)</span></div>

<div class="viewcode-block" id="VariationalInference.update"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference.update">[docs]</a>  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run one iteration of optimizer for variational inference.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : double</span>
<span class="sd">      Loss function values after one iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">get_session</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="VariationalInference.print_progress"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference.print_progress">[docs]</a>  <span class="k">def</span> <span class="nf">print_progress</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Print progress to output.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    t : int</span>
<span class="sd">      Iteration counter.</span>
<span class="sd">    loss : double</span>
<span class="sd">      Loss function value at iteration ``t``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_print</span> <span class="ow">is</span> <span class="ow">not</span> <span class="k">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_print</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s">&quot;iter {:d} loss {:.2f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="n">rv</span><span class="p">)</span></div>

<div class="viewcode-block" id="VariationalInference.finalize"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference.finalize">[docs]</a>  <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Function to call after convergence.</span>

<span class="sd">    Any class based on ``VariationalInference`` **may**</span>
<span class="sd">    overwrite this method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># Ask threads to stop.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coord</span><span class="o">.</span><span class="n">request_stop</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coord</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">threads</span><span class="p">)</span></div>

<div class="viewcode-block" id="VariationalInference.build_loss"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.VariationalInference.build_loss">[docs]</a>  <span class="k">def</span> <span class="nf">build_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function.</span>

<span class="sd">    Empty method.</span>

<span class="sd">    Any class based on ``VariationalInference`` **must**</span>
<span class="sd">    implement this method.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    NotImplementedError</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="MFVI"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI">[docs]</a><span class="k">class</span> <span class="nc">MFVI</span><span class="p">(</span><span class="n">VariationalInference</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Mean-field variational inference.</span>

<span class="sd">  This class implements a variety of &quot;black-box&quot; variational inference</span>
<span class="sd">  techniques (Ranganath et al., 2014) that minimize</span>

<span class="sd">  .. math::</span>

<span class="sd">    KL( q(z; \lambda) || p(z | x) ).</span>

<span class="sd">  This is equivalent to maximizing the objective function (Jordan et al., 1999)</span>

<span class="sd">  .. math::</span>

<span class="sd">    ELBO =  E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MFVI</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="MFVI.initialize"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.initialize">[docs]</a>  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int, optional</span>
<span class="sd">      Number of samples from variational model for calculating</span>
<span class="sd">      stochastic gradients.</span>
<span class="sd">    score : bool, optional</span>
<span class="sd">      Whether to force inference to use the score function</span>
<span class="sd">      gradient estimator. Otherwise default is to use the</span>
<span class="sd">      reparameterization gradient if available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">score</span> <span class="ow">is</span> <span class="k">None</span> <span class="ow">and</span> \
       <span class="nb">all</span><span class="p">([</span><span class="n">rv</span><span class="o">.</span><span class="n">is_reparameterized</span> <span class="ow">and</span> <span class="n">rv</span><span class="o">.</span><span class="n">is_continuous</span>
            <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)]):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">=</span> <span class="k">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">=</span> <span class="k">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">MFVI</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="MFVI.build_loss"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_loss">[docs]</a>  <span class="k">def</span> <span class="nf">build_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper for the MFVI loss function.</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]</span>

<span class="sd">    MFVI supports</span>

<span class="sd">    1. score function gradients</span>
<span class="sd">    2. reparameterization gradients</span>

<span class="sd">    of the loss function.</span>

<span class="sd">    If the variational model is a Gaussian distribution, then part of the</span>
<span class="sd">    loss function can be computed analytically.</span>

<span class="sd">    If the variational model is a normal distribution and the prior is</span>
<span class="sd">    standard normal, then part of the loss function can be computed</span>
<span class="sd">    analytically following Kingma and Welling (2014),</span>

<span class="sd">    .. math::</span>

<span class="sd">      E[\log p(x | z) + KL],</span>

<span class="sd">    where the KL term is computed analytically.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result :</span>
<span class="sd">      an appropriately selected loss function form</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">q_is_normal</span> <span class="o">=</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">rv</span><span class="p">,</span> <span class="n">Normal</span><span class="p">)</span> <span class="k">for</span>
                       <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)])</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">score</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">q_is_normal</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="p">,</span> <span class="s">&#39;log_lik&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_score_loss_kl</span><span class="p">()</span>
      <span class="c"># Analytic entropies may lead to problems around</span>
      <span class="c"># convergence; for now it is deactivated.</span>
      <span class="c"># elif is_entropy:</span>
      <span class="c">#    return self.build_score_loss_entropy()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_score_loss</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">q_is_normal</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="p">,</span> <span class="s">&#39;log_lik&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_reparam_loss_kl</span><span class="p">()</span>
      <span class="c"># elif is_entropy:</span>
      <span class="c">#    return self.build_reparam_loss_entropy()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_reparam_loss</span><span class="p">()</span></div>

<div class="viewcode-block" id="MFVI.build_score_loss"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_score_loss">[docs]</a>  <span class="k">def</span> <span class="nf">build_score_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]</span>

<span class="sd">    based on the score function estimator. (Paisley et al., 2012)</span>

<span class="sd">    Computed by sampling from :math:`q(z;\lambda)` and evaluating the</span>
<span class="sd">    expectation using Monte Carlo sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="n">p_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">q_log_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">):</span>
      <span class="n">q_log_prob</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">key</span><span class="p">])),</span>
                                  <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">get_batch_shape</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="n">p_log_prob</span> <span class="o">-</span> <span class="n">q_log_prob</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q_log_prob</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span></div>

<div class="viewcode-block" id="MFVI.build_reparam_loss"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_reparam_loss">[docs]</a>  <span class="k">def</span> <span class="nf">build_reparam_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]</span>

<span class="sd">    based on the reparameterization trick. (Kingma and Welling, 2014)</span>

<span class="sd">    Computed by sampling from :math:`q(z;\lambda)` and evaluating the</span>
<span class="sd">    expectation using Monte Carlo sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="n">p_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">q_log_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">):</span>
      <span class="n">q_log_prob</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">key</span><span class="p">]),</span>
                                  <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">get_batch_shape</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">p_log_prob</span> <span class="o">-</span> <span class="n">q_log_prob</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span></div>

<div class="viewcode-block" id="MFVI.build_score_loss_kl"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_score_loss_kl">[docs]</a>  <span class="k">def</span> <span class="nf">build_score_loss_kl</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  - ( E_{q(z; \lambda)} [ \log p(x | z) ]</span>
<span class="sd">             + KL(q(z; \lambda) || p(z)) )</span>

<span class="sd">    based on the score function estimator. (Paisley et al., 2012)</span>

<span class="sd">    It assumes the KL is analytic.</span>

<span class="sd">    It assumes the prior is :math:`p(z) = \mathcal{N}(z; 0, 1)`.</span>

<span class="sd">    Computed by sampling from :math:`q(z;\lambda)` and evaluating the</span>
<span class="sd">    expectation using Monte Carlo sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="n">p_log_lik</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_lik</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">q_log_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">):</span>
      <span class="n">q_log_prob</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">key</span><span class="p">])),</span>
                                  <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">get_batch_shape</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="n">rv</span><span class="o">.</span><span class="n">mu</span> <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)])</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="n">rv</span><span class="o">.</span><span class="n">sigma</span> <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)])</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="n">kl_multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">p_log_lik</span><span class="p">)</span> <span class="o">-</span> <span class="n">kl</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q_log_prob</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">p_log_lik</span><span class="p">))</span> <span class="o">-</span> <span class="n">kl</span><span class="p">)</span></div>

<div class="viewcode-block" id="MFVI.build_score_loss_entropy"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_score_loss_entropy">[docs]</a>  <span class="k">def</span> <span class="nf">build_score_loss_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  - ( E_{q(z; \lambda)} [ \log p(x, z) ]</span>
<span class="sd">            + H(q(z; \lambda)) )</span>

<span class="sd">    based on the score function estimator. (Paisley et al., 2012)</span>

<span class="sd">    It assumes the entropy is analytic.</span>

<span class="sd">    Computed by sampling from :math:`q(z;\lambda)` and evaluating the</span>
<span class="sd">    expectation using Monte Carlo sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="n">p_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">q_log_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">q_entropy</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">):</span>
      <span class="n">q_log_prob</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">key</span><span class="p">])),</span>
                                  <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rv</span><span class="o">.</span><span class="n">get_batch_shape</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>
      <span class="n">q_entropy</span> <span class="o">+=</span> <span class="n">rv</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">p_log_prob</span><span class="p">)</span> <span class="o">+</span> <span class="n">q_entropy</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q_log_prob</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">p_log_prob</span><span class="p">))</span> <span class="o">+</span>
             <span class="n">q_entropy</span><span class="p">)</span></div>

<div class="viewcode-block" id="MFVI.build_reparam_loss_kl"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_reparam_loss_kl">[docs]</a>  <span class="k">def</span> <span class="nf">build_reparam_loss_kl</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  - ( E_{q(z; \lambda)} [ \log p(x | z) ]</span>
<span class="sd">            + KL(q(z; \lambda) || p(z)) )</span>

<span class="sd">    based on the reparameterization trick. (Kingma and Welling, 2014)</span>

<span class="sd">    It assumes the KL is analytic.</span>

<span class="sd">    It assumes the prior is :math:`p(z) = \mathcal{N}(z; 0, 1)`</span>

<span class="sd">    Computed by sampling from :math:`q(z;\lambda)` and evaluating the</span>
<span class="sd">    expectation using Monte Carlo sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="n">p_log_lik</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_lik</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="n">rv</span><span class="o">.</span><span class="n">mu</span> <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)])</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="n">rv</span><span class="o">.</span><span class="n">sigma</span> <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">p_log_lik</span><span class="p">)</span> <span class="o">-</span> \
        <span class="n">kl_multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span></div>

<div class="viewcode-block" id="MFVI.build_reparam_loss_entropy"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MFVI.build_reparam_loss_entropy">[docs]</a>  <span class="k">def</span> <span class="nf">build_reparam_loss_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>

<span class="sd">      -ELBO =  -( E_{q(z; \lambda)} [ \log p(x , z) ]</span>
<span class="sd">            + H(q(z; \lambda)) )</span>

<span class="sd">    based on the reparameterization trick. (Kingma and Welling, 2014)</span>

<span class="sd">    It assumes the entropy is analytic.</span>

<span class="sd">    Computed by sampling from :math:`q(z;\lambda)` and evaluating the</span>
<span class="sd">    expectation using Monte Carlo sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="n">p_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">q_entropy</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iterkeys</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
      <span class="n">q_entropy</span> <span class="o">+=</span> <span class="n">rv</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">p_log_prob</span><span class="p">)</span> <span class="o">+</span> <span class="n">q_entropy</span>
    <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span></div></div>


<div class="viewcode-block" id="KLpq"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.KLpq">[docs]</a><span class="k">class</span> <span class="nc">KLpq</span><span class="p">(</span><span class="n">VariationalInference</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A variational inference method that minimizes the Kullback-Leibler</span>
<span class="sd">  divergence from the posterior to the variational model (Cappe et al., 2008)</span>

<span class="sd">  .. math::</span>

<span class="sd">    KL( p(z |x) || q(z) ).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">KLpq</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="KLpq.initialize"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.KLpq.initialize">[docs]</a>  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int, optional</span>
<span class="sd">      Number of samples from variational model for calculating</span>
<span class="sd">      stochastic gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">KLpq</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="KLpq.build_loss"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.KLpq.build_loss">[docs]</a>  <span class="k">def</span> <span class="nf">build_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is a stochastic gradient of</span>

<span class="sd">    .. math::</span>
<span class="sd">      KL( p(z |x) || q(z) )</span>
<span class="sd">      =</span>
<span class="sd">      E_{p(z | x)} [ \log p(z | x) - \log q(z; \lambda) ]</span>

<span class="sd">    based on importance sampling.</span>

<span class="sd">    Computed as</span>

<span class="sd">    .. math::</span>
<span class="sd">      1/B \sum_{b=1}^B [ w_{norm}(z^b; \lambda) *</span>
<span class="sd">                (\log p(x, z^b) - \log q(z^b; \lambda) ]</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">      z^b \sim q(z^b; \lambda)</span>

<span class="sd">      w_{norm}(z^b; \lambda) = w(z^b; \lambda) / \sum_{b=1}^B (w(z^b; \lambda))</span>

<span class="sd">      w(z^b; \lambda) = p(x, z^b) / q(z^b; \lambda)</span>

<span class="sd">    which gives a gradient</span>

<span class="sd">    .. math::</span>
<span class="sd">      - 1/B \sum_{b=1}^B</span>
<span class="sd">      w_{norm}(z^b; \lambda) \partial_{\lambda} \log q(z^b; \lambda)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">])</span>
         <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>

    <span class="c"># normalized importance weights</span>
    <span class="n">q_log_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">):</span>
        <span class="n">q_log_prob</span> <span class="o">+=</span> <span class="n">rv</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>

    <span class="n">log_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="n">q_log_prob</span>
    <span class="n">log_w_norm</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">-</span> <span class="n">log_sum_exp</span><span class="p">(</span><span class="n">log_w</span><span class="p">)</span>
    <span class="n">w_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_w_norm</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">w_norm</span> <span class="o">*</span> <span class="n">log_w</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q_log_prob</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">w_norm</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="MAP"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MAP">[docs]</a><span class="k">class</span> <span class="nc">MAP</span><span class="p">(</span><span class="n">VariationalInference</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Maximum a posteriori inference.</span>

<span class="sd">  We implement this using a ``PointMass`` variational distribution to</span>
<span class="sd">  solve the following optimization problem</span>

<span class="sd">  .. math::</span>

<span class="sd">    \min_{z} - \log p(x,z)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="o">=</span><span class="k">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    latent_vars : list of str or dict of str to RandomVariable</span>
<span class="sd">      Collection of random variables to perform inference on. If</span>
<span class="sd">      list, each random variable will be implictly optimized using a</span>
<span class="sd">      ``PointMass` distribution that is defined internally (with</span>
<span class="sd">      real-valued support). If dictionary, each random variable is</span>
<span class="sd">      binded to a ``PointMass`` distribution that will be used to</span>
<span class="sd">      infer the former conditional on data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Most explicitly, MAP is specified via a dictionary:</span>

<span class="sd">    &gt;&gt;&gt; qpi = PointMass(params=ed.to_simplex(tf.Variable(tf.zeros(K-1))))</span>
<span class="sd">    &gt;&gt;&gt; qmu = PointMass(params=tf.Variable(tf.zeros(K*D)))</span>
<span class="sd">    &gt;&gt;&gt; qsigma = PointMass(params=tf.exp(tf.Variable(tf.zeros(K*D))))</span>
<span class="sd">    &gt;&gt;&gt; MAP({&#39;pi&#39;: qpi, &#39;mu&#39;: qmu, &#39;sigma&#39;: qsigma}, data, model_wrapper)</span>

<span class="sd">    We also automate the specification of ``PointMass`` distributions</span>
<span class="sd">    (with real-valued support), so one can pass in a list of latent</span>
<span class="sd">    variables instead. However, for model wrappers, the list can only</span>
<span class="sd">    have one element:</span>

<span class="sd">    &gt;&gt;&gt; MAP([&#39;z&#39;], data, model_wrapper)</span>

<span class="sd">    For example, the following is not supported:</span>

<span class="sd">    &gt;&gt;&gt; MAP([&#39;pi&#39;, &#39;mu&#39;, &#39;sigma&#39;], data, model_wrapper)</span>

<span class="sd">    This is because internally with model wrappers, we have no way</span>
<span class="sd">    of knowing the dimensions in which to optimize each</span>
<span class="sd">    distribution; further, we do not know their support. For more</span>
<span class="sd">    than one random variable, or for constrained support, one must</span>
<span class="sd">    explicitly pass in the point mass distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">latent_vars</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">&quot;variational&quot;</span><span class="p">):</span>
          <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model_wrapper</span><span class="p">,</span> <span class="s">&#39;n_vars&#39;</span><span class="p">):</span>
            <span class="n">latent_vars</span> <span class="o">=</span> <span class="p">{</span><span class="n">latent_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">PointMass</span><span class="p">(</span>
                <span class="n">params</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">n_vars</span><span class="p">])))}</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">latent_vars</span> <span class="o">=</span> <span class="p">{</span><span class="n">latent_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">PointMass</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]))}</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s">&quot;A list of more than one element is &quot;</span>
                                  <span class="s">&quot;not supported. See documentation.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">()</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">MAP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="p">)</span>

<div class="viewcode-block" id="MAP.build_loss"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.MAP.build_loss">[docs]</a>  <span class="k">def</span> <span class="nf">build_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build loss function. Its automatic differentiation</span>
<span class="sd">    is the gradient of</span>

<span class="sd">    .. math::</span>
<span class="sd">      - \log p(x,z)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>
    <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span></div></div>


<div class="viewcode-block" id="Laplace"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.Laplace">[docs]</a><span class="k">class</span> <span class="nc">Laplace</span><span class="p">(</span><span class="n">MAP</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Laplace approximation.</span>

<span class="sd">  It approximates the posterior distribution using a normal</span>
<span class="sd">  distribution centered at the mode of the posterior.</span>

<span class="sd">  We implement this by running ``MAP`` to find the posterior mode.</span>
<span class="sd">  This forms the mean of the normal approximation. We then compute</span>
<span class="sd">  the Hessian at the mode of the posterior. This forms the</span>
<span class="sd">  covariance of the normal approximation.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="k">None</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="o">=</span><span class="k">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Laplace</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model_wrapper</span><span class="p">)</span>

<div class="viewcode-block" id="Laplace.finalize"><a class="viewcode-back" href="../../edward.inferences.html#edward.inferences.Laplace.finalize">[docs]</a>  <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Function to call after convergence.</span>

<span class="sd">    Computes the Hessian at the mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># use only a batch of data to estimate hessian</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">rv</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_vars</span><span class="p">)}</span>
    <span class="n">var_list</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
                                 <span class="n">scope</span><span class="o">=</span><span class="s">&#39;variational&#39;</span><span class="p">)</span>
    <span class="n">inv_cov</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">var_list</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s">&quot;Precision matrix:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">inv_cov</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Laplace</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Edward Development Team.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1.0.9',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>