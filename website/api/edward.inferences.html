

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>edward.inferences module &mdash; Edward 1.0.9 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Edward 1.0.9 documentation" href="index.html"/>
        <link rel="up" title="edward package" href="edward.html"/>
        <link rel="next" title="edward.util module" href="edward.util.html"/>
        <link rel="prev" title="edward.criticisms module" href="edward.criticisms.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Edward
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="models_models.html">Probabilistic Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="models_distributions.html">Variational Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="inferences.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="criticisms.html">Criticism</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="edward.html">edward package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="edward.html#subpackages">Subpackages</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="edward.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="edward.criticisms.html">edward.criticisms module</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">edward.inferences module</a></li>
<li class="toctree-l3"><a class="reference internal" href="edward.util.html">edward.util module</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Edward</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="edward.html">edward package</a> &raquo;</li>
      
    <li>edward.inferences module</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/edward.inferences.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-edward.inferences">
<span id="edward-inferences-module"></span><h1>edward.inferences module<a class="headerlink" href="#module-edward.inferences" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="edward.inferences.Inference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Inference</code><span class="sig-paren">(</span><em>model</em>, <em>data=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#Inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.Inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Base class for Edward inference methods.</p>
<dl class="attribute">
<dt id="edward.inferences.Inference.model">
<code class="descname">model</code><a class="headerlink" href="#edward.inferences.Inference.model" title="Permalink to this definition">¶</a></dt>
<dd><p><em>ed.Model</em> &#8211; probability model</p>
</dd></dl>

<dl class="attribute">
<dt id="edward.inferences.Inference.data">
<code class="descname">data</code><a class="headerlink" href="#edward.inferences.Inference.data" title="Permalink to this definition">¶</a></dt>
<dd><p><em>dict of tf.Tensor</em> &#8211; Data dictionary whose values may vary at each session run.</p>
</dd></dl>

<p>Initialization.</p>
<p>Calls <code class="docutils literal"><span class="pre">util.get_session()</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>model</strong> (<em>ed.Model</em>) &#8211; probability model</li>
<li><strong>data</strong> (<em>dict, optional</em>) &#8211; Data dictionary. For TensorFlow, Python, and Stan models,
the key type is a string; for PyMC3, the key type is a
Theano shared variable. For TensorFlow, Python, and PyMC3
models, the value type is a NumPy array or TensorFlow
tensor; for Stan, the value type is the type
according to the Stan program&#8217;s data block.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>If <cite>data</cite> is not passed in, the dictionary is empty.</p>
<p>Three options are available for batch training:
1. internally if user passes in data as a dictionary of NumPy</p>
<blockquote>
<div>arrays;</div></blockquote>
<ol class="arabic simple" start="2">
<li>externally if user passes in data as a dictionary of
TensorFlow placeholders (and manually feeds them);</li>
<li>externally if user passes in data as TensorFlow tensors
which are the outputs of data readers.</li>
</ol>
</dd></dl>

<dl class="class">
<dt id="edward.inferences.KLpq">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">KLpq</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#KLpq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.KLpq" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#edward.inferences.VariationalInference" title="edward.inferences.VariationalInference"><code class="xref py py-class docutils literal"><span class="pre">edward.inferences.VariationalInference</span></code></a></p>
<p>A variational inference method that minimizes the Kullback-Leibler
divergence from the posterior to the variational model (Cappe et al., 2008)</p>
<div class="math">
\[KL( p(z |x) || q(z) ).\]</div>
<dl class="method">
<dt id="edward.inferences.KLpq.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#KLpq.build_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.KLpq.build_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[KL( p(z |x) || q(z) )
=
E_{p(z | x)} [ \log p(z | x) - \log q(z; \lambda) ]\]</div>
<p>based on importance sampling.</p>
<p>Computed as</p>
<div class="math">
\[1/B \sum_{b=1}^B [ w_{norm}(z^b; \lambda) *
                    (\log p(x, z^b) - \log q(z^b; \lambda) ]\]</div>
<p>where</p>
<div class="math">
\[ \begin{align}\begin{aligned}z^b \sim q(z^b; \lambda)\\w_{norm}(z^b; \lambda) = w(z^b; \lambda) / \sum_{b=1}^B ( w(z^b; \lambda) )\\w(z^b; \lambda) = p(x, z^b) / q(z^b; \lambda)\end{aligned}\end{align} \]</div>
<p>which gives a gradient</p>
<div class="math">
\[- 1/B \sum_{b=1}^B
w_{norm}(z^b; \lambda) \partial_{\lambda} \log q(z^b; \lambda)\]</div>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.KLpq.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#KLpq.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.KLpq.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n_samples</strong> (<em>int, optional</em>) &#8211; Number of samples from variational model for calculating
stochastic gradients.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="edward.inferences.Laplace">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>model</em>, <em>data=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#Laplace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.Laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#edward.inferences.MAP" title="edward.inferences.MAP"><code class="xref py py-class docutils literal"><span class="pre">edward.inferences.MAP</span></code></a></p>
<p>Laplace approximation.</p>
<p>It approximates the posterior distribution using a normal
distribution centered at the mode of the posterior.</p>
<p>We implement this by running <code class="docutils literal"><span class="pre">MAP</span></code> to find the posterior mode.
This forms the mean of the normal approximation. We then compute
the Hessian at the mode of the posterior. This forms the
covariance of the normal approximation.</p>
<dl class="method">
<dt id="edward.inferences.Laplace.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#Laplace.finalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.Laplace.finalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to call after convergence.</p>
<p>Computes the Hessian at the mode.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="edward.inferences.MAP">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MAP</code><span class="sig-paren">(</span><em>model</em>, <em>data=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MAP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MAP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#edward.inferences.VariationalInference" title="edward.inferences.VariationalInference"><code class="xref py py-class docutils literal"><span class="pre">edward.inferences.VariationalInference</span></code></a></p>
<p>Maximum a posteriori inference.</p>
<p>We implement this using a <code class="docutils literal"><span class="pre">PointMass</span></code> variational distribution to
solve the following optimization problem</p>
<div class="math">
\[\min_{z} - \log p(x,z)\]</div>
<dl class="method">
<dt id="edward.inferences.MAP.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MAP.build_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MAP.build_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is the gradient of</p>
<div class="math">
\[- \log p(x,z)\]</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="edward.inferences.MFVI">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MFVI</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#edward.inferences.VariationalInference" title="edward.inferences.VariationalInference"><code class="xref py py-class docutils literal"><span class="pre">edward.inferences.VariationalInference</span></code></a></p>
<p>Mean-field variational inference.</p>
<p>This class implements a variety of &#8220;black-box&#8221; variational inference
techniques (Ranganath et al., 2014) that minimize</p>
<div class="math">
\[KL( q(z; \lambda) || p(z | x) ).\]</div>
<p>This is equivalent to maximizing the objective function (Jordan et al., 1999)</p>
<div class="math">
\[ELBO =  E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ].\]</div>
<dl class="method">
<dt id="edward.inferences.MFVI.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper for the MFVI loss function.</p>
<div class="math">
\[-ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>MFVI supports</p>
<ol class="arabic simple">
<li>score function gradients</li>
<li>reparameterization gradients</li>
</ol>
<p>of the loss function.</p>
<p>If the variational model is a Gaussian distribution, then part of the
loss function can be computed analytically.</p>
<p>If the variational model is a normal distribution and the prior is
standard normal, then part of the loss function can be computed
analytically following Kingma and Welling (2014),</p>
<div class="math">
\[E[\log p(x | z) + KL],\]</div>
<p>where the KL term is computed analytically.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">an appropriately selected loss function form</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">result</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.build_reparam_loss">
<code class="descname">build_reparam_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_reparam_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_reparam_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>based on the reparameterization trick. (Kingma and Welling, 2014)</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.build_reparam_loss_entropy">
<code class="descname">build_reparam_loss_entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_reparam_loss_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_reparam_loss_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  -( E_{q(z; \lambda)} [ \log p(x , z) ]
            + H(q(z; \lambda)) )\]</div>
<p>based on the reparameterization trick. (Kingma and Welling, 2014)</p>
<p>It assumes the entropy is analytic.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.build_reparam_loss_kl">
<code class="descname">build_reparam_loss_kl</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_reparam_loss_kl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_reparam_loss_kl" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  - ( E_{q(z; \lambda)} [ \log p(x | z) ]
            + KL(q(z; \lambda) || p(z)) )\]</div>
<p>based on the reparameterization trick. (Kingma and Welling, 2014)</p>
<p>It assumes the KL is analytic.</p>
<p>It assumes the prior is <span class="math">\(p(z) = \mathcal{N}(z; 0, 1)\)</span></p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.build_score_loss">
<code class="descname">build_score_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_score_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_score_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  -E_{q(z; \lambda)} [ \log p(x, z) - \log q(z; \lambda) ]\]</div>
<p>based on the score function estimator. (Paisley et al., 2012)</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.build_score_loss_entropy">
<code class="descname">build_score_loss_entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_score_loss_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_score_loss_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  - ( E_{q(z; \lambda)} [ \log p(x, z) ]
            + H(q(z; \lambda)) )\]</div>
<p>based on the score function estimator. (Paisley et al., 2012)</p>
<p>It assumes the entropy is analytic.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.build_score_loss_kl">
<code class="descname">build_score_loss_kl</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.build_score_loss_kl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.build_score_loss_kl" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function. Its automatic differentiation
is a stochastic gradient of</p>
<div class="math">
\[-ELBO =  - ( E_{q(z; \lambda)} [ \log p(x | z) ]
             + KL(q(z; \lambda) || p(z)) )\]</div>
<p>based on the score function estimator. (Paisley et al., 2012)</p>
<p>It assumes the KL is analytic.</p>
<p>It assumes the prior is <span class="math">\(p(z) = \mathcal{N}(z; 0, 1)\)</span>.</p>
<p>Computed by sampling from <span class="math">\(q(z;\lambda)\)</span> and evaluating the
expectation using Monte Carlo sampling.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.MFVI.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_samples=1</em>, <em>score=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MFVI.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MFVI.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_samples</strong> (<em>int, optional</em>) &#8211; Number of samples from variational model for calculating
stochastic gradients.</li>
<li><strong>score</strong> (<em>bool, optional</em>) &#8211; Whether to force inference to use the score function
gradient estimator. Otherwise default is to use the
reparameterization gradient if available.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="edward.inferences.MonteCarlo">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">MonteCarlo</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#MonteCarlo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.MonteCarlo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#edward.inferences.Inference" title="edward.inferences.Inference"><code class="xref py py-class docutils literal"><span class="pre">edward.inferences.Inference</span></code></a></p>
<p>Base class for Monte Carlo inference methods.</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>model</strong> (<em>ed.Model</em>) &#8211; probability model</li>
<li><strong>data</strong> (<em>dict, optional</em>) &#8211; Data dictionary. For TensorFlow, Python, and Stan models,
the key type is a string; for PyMC3, the key type is a
Theano shared variable. For TensorFlow, Python, and PyMC3
models, the value type is a NumPy array or TensorFlow
placeholder; for Stan, the value type is the type
according to the Stan program&#8217;s data block.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="edward.inferences.VariationalInference">
<em class="property">class </em><code class="descclassname">edward.inferences.</code><code class="descname">VariationalInference</code><span class="sig-paren">(</span><em>model</em>, <em>variational</em>, <em>data=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#edward.inferences.Inference" title="edward.inferences.Inference"><code class="xref py py-class docutils literal"><span class="pre">edward.inferences.Inference</span></code></a></p>
<p>Base class for variational inference methods.</p>
<p>Initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>model</strong> (<em>ed.Model</em>) &#8211; probability model</li>
<li><strong>variational</strong> (<em>ed.Variational</em>) &#8211; variational model or distribution</li>
<li><strong>data</strong> (<em>dict, optional</em>) &#8211; Data dictionary. For TensorFlow, Python, and Stan models,
the key type is a string; for PyMC3, the key type is a
Theano shared variable. For TensorFlow, Python, and PyMC3
models, the value type is a NumPy array or TensorFlow
placeholder; for Stan, the value type is the type
according to the Stan program&#8217;s data block.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="edward.inferences.VariationalInference.build_loss">
<code class="descname">build_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference.build_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference.build_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Build loss function.</p>
<p>Empty method.</p>
<p>Any class based on <code class="docutils literal"><span class="pre">VariationalInference</span></code> <strong>must</strong>
implement this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><code class="xref py py-exc docutils literal"><span class="pre">NotImplementedError</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.VariationalInference.finalize">
<code class="descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference.finalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference.finalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to call after convergence.</p>
<p>Any class based on <code class="docutils literal"><span class="pre">VariationalInference</span></code> <strong>may</strong>
overwrite this method.</p>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.VariationalInference.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>n_iter=1000</em>, <em>n_minibatch=None</em>, <em>n_print=100</em>, <em>optimizer=None</em>, <em>scope=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize variational inference algorithm.</p>
<p>Set up <code class="docutils literal"><span class="pre">tf.train.AdamOptimizer</span></code> with a decaying scale factor.</p>
<p>Initialize all variables</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_iter</strong> (<em>int, optional</em>) &#8211; Number of iterations for optimization.</li>
<li><strong>n_minibatch</strong> (<em>int, optional</em>) &#8211; Number of samples for data subsampling. Default is to use
all the data. Subsampling is available only if all data
passed in are NumPy arrays and the model is not a Stan
model. For subsampling details, see
<cite>tf.train.slice_input_producer</cite> and <cite>tf.train.batch</cite>.</li>
<li><strong>n_print</strong> (<em>int, optional</em>) &#8211; Number of iterations for each print progress. To suppress print
progress, then specify None.</li>
<li><strong>optimizer</strong> (<em>str, optional</em>) &#8211; Whether to use TensorFlow optimizer or PrettyTensor
optimizer when using PrettyTensor. Defaults to TensorFlow.</li>
<li><strong>scope</strong> (<em>str, optional</em>) &#8211; Scope of TensorFlow variable objects to optimize over.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.VariationalInference.print_progress">
<code class="descname">print_progress</code><span class="sig-paren">(</span><em>t</em>, <em>loss</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference.print_progress"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference.print_progress" title="Permalink to this definition">¶</a></dt>
<dd><p>Print progress to output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>t</strong> (<em>int</em>) &#8211; Iteration counter</li>
<li><strong>loss</strong> (<em>double</em>) &#8211; Loss function value at iteration <code class="docutils literal"><span class="pre">t</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.VariationalInference.run">
<code class="descname">run</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference.run" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple wrapper to run variational inference.</p>
<ol class="arabic simple">
<li>Initialize via <code class="docutils literal"><span class="pre">initialize</span></code>.</li>
<li>Run <code class="docutils literal"><span class="pre">update</span></code> for <code class="docutils literal"><span class="pre">self.n_iter</span></code> iterations.</li>
<li>While running, <code class="docutils literal"><span class="pre">print_progress</span></code>.</li>
<li>Finalize via <code class="docutils literal"><span class="pre">finalize</span></code>.</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>*args</strong> &#8211; Passed into <code class="docutils literal"><span class="pre">initialize</span></code>.</li>
<li><strong>**kwargs</strong> &#8211; Passed into <code class="docutils literal"><span class="pre">initialize</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="edward.inferences.VariationalInference.update">
<code class="descname">update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/edward/inferences.html#VariationalInference.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#edward.inferences.VariationalInference.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Run one iteration of optimizer for variational inference.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>loss</strong> &#8211; Loss function values after one iteration</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">double</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="edward.util.html" class="btn btn-neutral float-right" title="edward.util module" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="edward.criticisms.html" class="btn btn-neutral" title="edward.criticisms module" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Edward Development Team.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0.9',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>