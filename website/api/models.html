

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Models &mdash; Edward 1.0.9 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Edward 1.0.9 documentation" href="index.html"/>
        <link rel="next" title="Inference" href="inferences.html"/>
        <link rel="prev" title="Data" href="data.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Edward
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-api">Model API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="inferences.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="criticisms.html">Criticism</a></li>
<li class="toctree-l1"><a class="reference internal" href="edward.html">edward package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Edward</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Models</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/models.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">Â¶</a></h1>
<p>A probabilistic model specifies a joint distribution <code class="docutils literal"><span class="pre">p(x,</span> <span class="pre">z)</span></code>
of data <code class="docutils literal"><span class="pre">x</span></code> and latent variables <code class="docutils literal"><span class="pre">z</span></code>.
For more details, see the
<a class="reference external" href="../tut_model.html">Probability Models tutorial</a>.</p>
<p>All models in Edward are written as a class. To write a new model,
it can be written in any of the currently supported modeling
languages: TensorFlow, Python, Stan, and PyMC3.</p>
<p><strong>TensorFlow.</strong>
Write a class with the method <code class="docutils literal"><span class="pre">log_prob(xs,</span> <span class="pre">zs)</span></code>. The method defines
the logarithm of a joint density. <code class="docutils literal"><span class="pre">xs</span></code> can be a single data point or
a batch, and analogously, <code class="docutils literal"><span class="pre">zs</span></code> can be a single set or multiple sets
of latent variables. The method outputs a vector of the joint density
evaluations <code class="docutils literal"><span class="pre">[log</span> <span class="pre">p(xs,</span> <span class="pre">zs[0,:]),</span> <span class="pre">log</span> <span class="pre">p(xs,</span> <span class="pre">zs[1,:]),</span> <span class="pre">...]</span></code>, with an
evaluation for each set of latent variables. Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">edward.stats</span> <span class="k">import</span> <span class="n">bernoulli</span><span class="p">,</span> <span class="n">beta</span>

<span class="k">class</span> <span class="nc">BetaBernoulli</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
        <span class="n">log_prior</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">log_lik</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">],</span> <span class="n">z</span><span class="p">))</span>
                           <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="n">zs</span><span class="p">)])</span>
        <span class="k">return</span> <span class="n">log_lik</span> <span class="o">+</span> <span class="n">log_prior</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BetaBernoulli</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_tf.py">toy script</a>
that uses this model. The model class can be more complicated,
containing fields or other methods required for certain functions in
Edward, and which can provide more information about the model&#8217;s
structure. See the section below for more details.</p>
<p><strong>Python.</strong>
Write a class that inherits from <code class="docutils literal"><span class="pre">PythonModel</span></code> and with the method
<code class="docutils literal"><span class="pre">_py_log_prob(xs,</span> <span class="pre">zs)</span></code>. The method defines the logarithm of a joint
density with the same concept as in a TensorFlow model, but where
<code class="docutils literal"><span class="pre">xs</span></code> and <code class="docutils literal"><span class="pre">zs</span></code> now use NumPy arrays rather than TensorFlow tensors.
Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">PythonModel</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="k">import</span> <span class="n">bernoulli</span><span class="p">,</span> <span class="n">beta</span>

<span class="k">class</span> <span class="nc">BetaBernoulli</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_py_log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
        <span class="c"># This example is written for pedagogy. We recommend</span>
        <span class="c"># vectorizing operations in practice.</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">zs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">lp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">lp</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:],</span> <span class="n">a</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">lp</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">zs</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:])</span>

        <span class="k">return</span> <span class="n">lp</span>
            <span class="k">return</span> <span class="n">log_lik</span> <span class="o">+</span> <span class="n">log_prior</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">BetaBernoulli</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_np.py">toy script</a>
that uses this model.</p>
<p><strong>Stan.</strong>
Write a Stan program in the form of a file or string. Then
call it with <code class="docutils literal"><span class="pre">StanModel(file=file)</span></code> or
<code class="docutils literal"><span class="pre">StanModel(model_code=model_code)</span></code>. Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">StanModel</span>

<span class="n">model_code</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
<span class="s">    data {</span>
<span class="s">      int&lt;lower=0&gt; N;</span>
<span class="s">      int&lt;lower=0,upper=1&gt; y[N];</span>
<span class="s">    }</span>
<span class="s">    parameters {</span>
<span class="s">      real&lt;lower=0,upper=1&gt; theta;</span>
<span class="s">    }</span>
<span class="s">    model {</span>
<span class="s">      theta ~ beta(1.0, 1.0);</span>
<span class="s">      for (n in 1:N)</span>
<span class="s">        y[n] ~ bernoulli(theta);</span>
<span class="s">    }</span>
<span class="s">&quot;&quot;&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">StanModel</span><span class="p">(</span><span class="n">model_code</span><span class="o">=</span><span class="n">model_code</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_stan.py">toy
script</a>
that uses this model. Stan programs are convenient as <a class="reference external" href="https://github.com/stan-dev/example-models/wiki">there are many
online examples</a>,
although they are limited to probability models with differentiable
latent variables. <code class="docutils literal"><span class="pre">StanModel</span></code> objects also contain no structure about
the model besides how to calculate its joint density.</p>
<p><strong>PyMC3.</strong>
Write a PyMC3 model whose observed values are Theano shared variables.
The values in the Theano shared variables can be plugged at a later
time. Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">PyMC3Model</span>

<span class="n">x_obs</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pm_model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s">&#39;beta&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="k">None</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s">&#39;x&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">x_obs</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">PyMC3Model</span><span class="p">(</span><span class="n">pm_model</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_pymc3.py">toy
script</a>
that uses this model. PyMC3 can be used to define models with both
differentiable latent variables and non-differentiable (e.g., discrete)
latent variables. <code class="docutils literal"><span class="pre">PyMC3Model</span></code> objects contain no structure about the
model besides how to calculate its joint density.</p>
<p>For modeling convenience, we recommend using the modeling language that
you are most familiar with. For efficiency, we recommend using
TensorFlow, as Edward uses TensorFlow as the computational backend.
Internally, other languages are wrapped in TensorFlow so their
computation represents a single node in the graph (making it difficult
to tease apart and thus distribute their computation).</p>
<p>For examples of models built in Edward, see the model
<a class="reference external" href="../tutorials.html">tutorials</a>.</p>
<div class="section" id="model-api">
<h2>Model API<a class="headerlink" href="#model-api" title="Permalink to this headline">Â¶</a></h2>
<p>This outlines the current spec for all methods in the model object.
It includes all modeling languages, where certain methods are
implemented by wrapping around other methods. For example, by a Python
model builds a <code class="docutils literal"><span class="pre">_py_log_prob()</span></code> method and inherits from
<code class="docutils literal"><span class="pre">PythonModel</span></code>; <code class="docutils literal"><span class="pre">PythonModel</span></code> implements <code class="docutils literal"><span class="pre">log_prob()</span></code> by wrapping
around <code class="docutils literal"><span class="pre">_py_log_prob()</span></code> as a TensorFlow operation.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used in: (most) inference.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        xs : dict</span>
<span class="sd">            Data dictionary. Each key names a data structure used in</span>
<span class="sd">            the model (str), and its value is the corresponding</span>
<span class="sd">            corresponding realization (np.ndarray or tf.Tensor).</span>
<span class="sd">        zs : list or tf.Tensor</span>
<span class="sd">            A list of tf.Tensor&#39;s if multiple varational families,</span>
<span class="sd">            otherwise a tf.Tensor if single variational family.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tf.Tensor</span>
<span class="sd">            S-vector of type tf.float32,</span>
<span class="sd">            [log p(xs, zs[1,:]), .., log p(xs, zs[S,:])].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">log_lik</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used in: inference with analytic KL.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        xs : dict</span>
<span class="sd">            Data dictionary. Each key names a data structure used in</span>
<span class="sd">            the model (str), and its value is the corresponding</span>
<span class="sd">            corresponding realization (np.ndarray or tf.Tensor).</span>
<span class="sd">        zs : list or tf.Tensor</span>
<span class="sd">            A list of tf.Tensor&#39;s if multiple varational families,</span>
<span class="sd">            otherwise a tf.Tensor if single variational family.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tf.Tensor</span>
<span class="sd">            S-vector of type tf.float32,</span>
<span class="sd">            [log p(xs | zs[1,:]), .., log p(xs | zs[S,:])].</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used in: ed.evaluate().</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        xs : dict</span>
<span class="sd">            Data dictionary. Each key names a data structure used in</span>
<span class="sd">            the model (str), and its value is the corresponding</span>
<span class="sd">            corresponding realization (np.ndarray or tf.Tensor).</span>
<span class="sd">        zs : list or tf.Tensor</span>
<span class="sd">            A list of tf.Tensor&#39;s if multiple varational families,</span>
<span class="sd">            otherwise a tf.Tensor if single variational family.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tf.Tensor</span>
<span class="sd">            Vector of predictions, one for each data point.</span>

<span class="sd">            For supervised tasks, the predicted value is the mean of the</span>
<span class="sd">            output&#39;s likelihood given features from the ith data point and</span>
<span class="sd">            averaged over the latent variable samples:</span>
<span class="sd">                + Binary classification. The probability of the success</span>
<span class="sd">                label.</span>
<span class="sd">                + Multi-class classification. The probability of each</span>
<span class="sd">                label, with the entire output of shape N x K.</span>
<span class="sd">                + Regression. The mean response.</span>
<span class="sd">            For unsupervised, the predicted value is the log-marginal</span>
<span class="sd">            likelihood evaluated at the ith data point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">sample_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used in: ed.ppc().</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        size : int, optional</span>
<span class="sd">            Number of latent variable samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tf.Tensor</span>
<span class="sd">            size x d matrix, where each row is a set of latent variables.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">sample_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">zs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used in: ed.ppc().</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        zs : list or tf.Tensor</span>
<span class="sd">            A list of tf.Tensor&#39;s if multiple varational families,</span>
<span class="sd">            otherwise a tf.Tensor if single variational family.</span>
<span class="sd">        size : int, optional</span>
<span class="sd">            Number of data points to generate per set of latent variables.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list of dict&#39;s of tf.Tensor&#39;s</span>
<span class="sd">            List of replicated data sets from the likelihood,</span>
<span class="sd">            [x^{rep, 1}, ..., x^{rep, S}],</span>
<span class="sd">            where x^{rep, s} ~ p(x | zs[s, :]) and x^{rep, s} has</span>
<span class="sd">            size-many data points. Type-wise, each x^{rep, s} is a</span>
<span class="sd">            dictionary with the same items and shape of values as the</span>
<span class="sd">            test data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="inferences.html" class="btn btn-neutral float-right" title="Inference" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="data.html" class="btn btn-neutral" title="Data" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Edward Development Team.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0.9',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>