<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – KL(p||q) minimization</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="css/normalize.css" rel="stylesheet">
<link href="css/skeleton.css" rel="stylesheet">
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="index.html">Edward</a></h1>
<a href="index.html">
<img alt="Edward" class="u-full-width" src="images/edward.png" style="margin-bottom:15%"/>
</a>
<a class="button u-full-width" href="index.html">Home</a>
<a class="button u-full-width" href="getting-started.html">Getting Started</a>
<a class="button u-full-width" href="delving-in.html">Delving In</a>
<a class="button u-full-width" href="tutorials.html">Tutorials</a>
<a class="button u-full-width" href="api/index.html">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="design-philosophy.html">Design Philosophy</a>
<a class="button2 u-full-width" href="developer-process.html">Developer Process</a>
<a class="button2 u-full-width" href="troubleshooting.html">Troubleshooting</a>
<a class="button2 u-full-width" href="license.html">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="textklpq-minimization"><span class="math inline">\(\text{KL}(p\|q)\)</span> minimization</h2>
<p>One form of variational inference minimizes the Kullback-Leibler divergence <strong>from</strong> <span class="math inline">\(p(z \mid x)\)</span> <strong>to</strong> <span class="math inline">\(q(z\;;\;\lambda)\)</span>, <span class="math display">\[\begin{aligned}
  \lambda^*
  &amp;=
  \arg\min_\lambda \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )\\
  &amp;=
  \arg\min_\lambda\;
  \mathbb{E}_{p(z \mid x)}
  \big[
  \log p(z \mid x)
  -
  \log q(z\;;\;\lambda)
  \big].\end{aligned}\]</span> The KL divergence is a non-symmetric, information theoretic measure of similarity between two probability distributions.</p>
<h3 id="minimizing-an-intractable-objective-function">Minimizing an intractable objective function</h3>
<p>The <span class="math inline">\(\text{KL}(p\|q)\)</span> objective we seek to minimize is intractable; it directly involves the posterior <span class="math inline">\(p(z \mid x)\)</span>. Ignoring this for the moment, consider its gradient <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &amp;=
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big].\end{aligned}\]</span> Both <span class="math inline">\(\text{KL}(p\|q)\)</span> and its gradient are intractable because of the posterior expectation. We can use <a href="tut_KLpq_ais.html">importance sampling</a> to both estimate the objective and calculate stochastic gradients.</p>
<h3 id="importance-sampling">Importance sampling</h3>
<p>First rewrite the expectation to be with respect to the variational distribution, <span class="math display">\[\begin{aligned}
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big]
  &amp;=
  -
  \mathbb{E}_{q(z\;;\;\lambda)}
  \Bigg[
  \frac{p(z \mid x)}{q(z\;;\;\lambda)}
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \Bigg].\end{aligned}\]</span></p>
<p>We then use importance sampling to obtain a noisy estimate of this gradient. The basic procedure follows these steps:</p>
<ol>
<li>draw <span class="math inline">\(S\)</span> samples <span class="math inline">\(\{z_s\}_1^S \sim q(z\;;\;\lambda)\)</span>,</li>
<li>evaluate <span class="math inline">\(\nabla_\lambda\; \log q(z_s\;;\;\lambda)\)</span>,</li>
<li>compute the normalized importance weights <span class="math display">\[\begin{aligned}
    w_s
    &amp;=
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
    \Bigg/
    \sum_{s=1}^{S}
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
  \end{aligned}\]</span></li>
<li>compute the weighted mean.</li>
</ol>
<p>The key insight is that we can use the joint <span class="math inline">\(p(x,z)\)</span> instead of the posterior when estimating the normalized importance weights <span class="math display">\[\begin{aligned}
  w_s
  &amp;=
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} \\
  &amp;=
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}.\end{aligned}\]</span> This follows from Bayes’ rule <span class="math display">\[\begin{aligned}
  p(z \mid x)
  &amp;=
  p(x, z) / p(x)\\
  &amp;=
  p(x, z) / \text{a constant function of }z.\end{aligned}\]</span></p>
<p>Importance sampling thus gives the following noisy yet unbiased gradient estimate <span class="math display">\[\begin{aligned}
\nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &amp;=
  -
  \frac{1}{S}
  \sum_{s=1}^S
  w_s
  \nabla_\lambda\; \log q(z_s\;;\;\lambda).\end{aligned}\]</span> The objective <span class="math inline">\(\text{KL}(p\|q)\)</span> can be calculated in a similar fashion. The only new ingredient for its gradient is the score function <span class="math inline">\(\nabla_\lambda \log q(z\;;\;\lambda)\)</span>. Edward uses automatic differentiation, specifically with TensorFlow’s computational graphs, making this gradient computation both simple and efficient to distribute.</p>
<p>Adaptive importance sampling follows this gradient to a local optimum using stochastic optimization. It is adaptive because the variational distribution <span class="math inline">\(q(z\;;\;\lambda)\)</span> iteratively gets closer to the posterior <span class="math inline">\(p(z \mid x)\)</span>.</p>
<h3 id="implementation">Implementation</h3>
<p>We implement <span class="math inline">\(\text{KL}(q\|p)\)</span> minimization with the importance sampling gradient in the class <code>KLpq</code>. It inherits from <code>VariationalInference</code>, which provides a collection of default methods such as an optimizer.</p>
<pre class="python" language="Python"><code>class KLpq(VariationalInference):
    def __init__(self, *args, **kwargs):
        super(KLpq, self).__init__(*args, **kwargs)

    def initialize(self, n_samples=1, *args, **kwargs):
        self.n_samples = n_samples
        return super(KLpq, self).initialize(*args, **kwargs)

    def build_loss(self):
        x = self.data
        z = self.variational.sample(self.n_samples)

        # normalized importance weights
        q_log_prob = self.variational.log_prob(stop_gradient(z))
        log_w = self.model.log_prob(x, z) - q_log_prob
        log_w_norm = log_w - log_sum_exp(log_w)
        w_norm = tf.exp(log_w_norm)

        self.loss = tf.reduce_mean(w_norm * log_w)
        return -tf.reduce_mean(q_log_prob * stop_gradient(w_norm))</code></pre>
<p>Two methods are added: <code>initialize()</code> and <code>build_loss()</code>. The method <code>initialize()</code> follows the same initialization as <code>VariationalInference</code>, and adds an argument: <code>n_samples</code> for the number of samples from the variational model.</p>
<p>The method <code>build_loss()</code> implements the <span class="math inline">\(\text{KL}(q\|p)\)</span> objective and its gradient. It draws <code>self.n_samples</code> samples from the variational model. It registers the Monte Carlo estimate of <span class="math inline">\(\text{KL}(q\|p)\)</span> in TensorFlow’s computational graph, and stores it in <code>self.loss</code>, to track progress of the inference for diagnostics.</p>
<p>The method returns an object whose automatic differentiation is a stochastic gradient of <span class="math inline">\(\text{KL}(p\|q)\)</span>. The TensorFlow function <code>stop_gradient()</code> tells the computational graph to stop traversing nodes to propagate gradients. In this case, the only gradients taken are <span class="math inline">\(\nabla_\lambda \log q(z_s\;;\;\lambda)\)</span>, one for each sample <span class="math inline">\(z_s\)</span>. We multiply it by <code>w_norm</code> element-wise and return the mean.</p>
<p>Computing the normalized importance weights is a numerically challenging task. Underflow is a common issue. Here we use the <code>log_sum_exp</code> trick to compute weights in the log space, available in <code>edward.util</code>. However, the normalized weights are still exponentiated to compute the gradient. As with importance sampling in general, this inference method does not scale to high dimensions.</p>
<p>See the <a href="api/index.html">API</a> for more details.</p>
<h3 id="references">References</h3>
<ul>
<li>Oh, M. S., &amp; Berger, J. O. (1992). Adaptive importance sampling in Monte Carlo integration. Journal of Statistical Computation and Simulation, 41(3-4), 143-168.</li>
</ul>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
