<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward â€“ Bayesian neural network</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="css/normalize.css" rel="stylesheet">
<link href="css/skeleton.css" rel="stylesheet">
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="index.html">Edward</a></h1>
<a href="index.html">
<img alt="Edward" class="u-full-width" src="images/edward.png" style="margin-bottom:15%"/>
</a>
<a class="button u-full-width" href="index.html">Home</a>
<a class="button u-full-width" href="getting-started.html">Getting Started</a>
<a class="button u-full-width" href="delving-in.html">Delving In</a>
<a class="button u-full-width" href="tutorials.html">Tutorials</a>
<a class="button u-full-width" href="api/index.html">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="design-philosophy.html">Design Philosophy</a>
<a class="button2 u-full-width" href="developer-process.html">Developer Process</a>
<a class="button2 u-full-width" href="troubleshooting.html">Troubleshooting</a>
<a class="button2 u-full-width" href="license.html">License</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a href="https://github.com/blei-lab/edward">
<!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" class="u-pull-right" src="images/github-mark.svg" style="padding-right:10%"/>
<!-- </object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="bayesian-neural-network">Bayesian neural network</h2>
<ul>
<li><strong>Probability Model:</strong> Bayesian neural network regression</li>
<li><strong>Variational Model:</strong> Fully factorized normal</li>
<li><strong>Data:</strong> Simulated data</li>
<li><strong>Inference:</strong> Mean-field variational inference</li>
<li><strong>Criticism:</strong> None</li>
</ul>
<p>Full Python code in: <a href="https://github.com/blei-lab/edward/blob/master/examples/bayesian_nn.py">bayesian_nn.py</a></p>
<h3 id="probability-model">Probability Model</h3>
<p>Define the likelihood of an observation <span class="math inline">\((y_n, x_n)\)</span> <span class="math display">\[\begin{aligned}
  p(y_n \mid \mathbf{z} \;;\; x_n, \sigma^2)
  &amp;=
  \mathcal{N}(y_n \;;\; \mu(x_n\;;\;\mathbf{z}), \sigma^2)\end{aligned}\]</span> where <span class="math inline">\(\mu\)</span> is a neural network with weights (latent) variables <span class="math inline">\(\mathbf{z}\)</span>. Here <span class="math inline">\(x_n\)</span> is the (known) covariate and <span class="math inline">\(\sigma^2\)</span> is the (known) variance of the observation.</p>
<p>Define the prior on <span class="math inline">\(\mathbf{z}\)</span> to be a factorized normal <span class="math display">\[\begin{aligned}
  p(\mathbf{z})
  &amp;=
  \mathcal{N}(\mathbf{z} \;;\; \mathbf{0}, I)\end{aligned}\]</span></p>
<p>The model in Edward is</p>
<pre class="python" language="Python"><code>class BayesianNN:
    """
    Bayesian neural network for regressing outputs y on inputs x.

    p((x,y), z) = Normal(y | NN(x; z), lik_variance) *
                  Normal(z | 0, prior_variance),

    where z are neural network weights, and with known lik_variance
    and prior_variance.

    Parameters
    ----------
    layer_sizes : list
        The size of each layer, ordered from input to output.
    nonlinearity : function, optional
        Non-linearity after each linear transformation in the neural
        network; aka activation function.
    lik_variance : float, optional
        Variance of the normal likelihood; aka noise parameter,
        homoscedastic variance, scale parameter.
    prior_variance : float, optional
        Variance of the normal prior on weights; aka L2
        regularization parameter, ridge penalty, scale parameter.
    """
    def __init__(self, layer_sizes, nonlinearity=tf.nn.tanh,
        lik_variance=0.01, prior_variance=1):
        self.layer_sizes = layer_sizes
        self.nonlinearity = nonlinearity
        self.lik_variance = lik_variance
        self.prior_variance = prior_variance

        self.num_layers = len(layer_sizes)
        self.weight_dims = zip(layer_sizes[:-1], layer_sizes[1:])
        self.num_vars = sum((m+1)*n for m, n in self.weight_dims)

    def unpack_weights(self, z):
        """Unpack weight matrices and biases from a flattened vector."""
        for m, n in self.weight_dims:
            yield tf.reshape(z[:m*n],        [m, n]), \
                  tf.reshape(z[m*n:(m*n+n)], [1, n])
            z = z[(m+1)*n:]

    def mapping(self, x, z):
        """
        mu = NN(x; z)

        Note this is one sample of z at a time.

        Parameters
        -------
        x : tf.tensor
            n_data x D

        z : tf.tensor
            num_vars

        Returns
        -------
        tf.tensor
            vector of length n_data
        """
        h = x
        for W, b in self.unpack_weights(z):
            # broadcasting to do (h*W) + b (e.g. 40x10 + 1x10)
            h = self.nonlinearity(tf.matmul(h, W) + b)

        h = tf.squeeze(h) # n_data x 1 to n_data
        return h

    def log_prob(self, xs, zs):
        """Returns a vector [log p(xs, zs[1,:]), ..., log p(xs, zs[S,:])]."""
        # Data must have labels in the first column and features in
        # subsequent columns.
        y = xs[:, 0]
        x = xs[:, 1:]
        log_prior = -self.prior_variance * tf.reduce_sum(zs*zs, 1)
        mus = tf.pack([self.mapping(x, z) for z in tf.unpack(zs)])
        # broadcasting to do mus - y (n_minibatch x n_data - n_data)
        log_lik = -tf.reduce_sum(tf.pow(mus - y, 2), 1) / self.lik_variance
        return log_lik + log_prior

model = BayesianNN(layer_sizes=[1, 10, 10, 1], nonlinearity=rbf)</code></pre>
<h3 id="variational-model">Variational Model</h3>
<p>Define the variational model to be a fully factorized normal <span class="math display">\[\begin{aligned}
  q(\mathbf{z} \;;\; \lambda)
  &amp;=
  \mathcal{N}(\mathbf{z} \;;\; \lambda_\mu, \lambda_{\sigma^2} I).\end{aligned}\]</span></p>
<p>The model in Edward is</p>
<pre class="python" language="Python"><code>variational = Variational()
variational.add(Normal(model.num_vars))</code></pre>
<h3 id="data">Data</h3>
<p>Consider a simulated dataset</p>
<pre class="python" language="Python"><code>def build_toy_dataset(n_data=40, noise_std=0.1):
    ed.set_seed(0)
    D = 1
    x  = np.concatenate([np.linspace(0, 2, num=n_data/2),
                         np.linspace(6, 8, num=n_data/2)])
    y = np.cos(x) + norm.rvs(0, noise_std, size=n_data).reshape((n_data,))
    x = (x - 4.0) / 4.0
    x = x.reshape((n_data, D))
    y = y.reshape((n_data, 1))
    data = np.concatenate((y, x), axis=1) # n_data x (D+1)
    data = tf.constant(data, dtype=tf.float32)
    return ed.Data(data)

data = build_toy_dataset()</code></pre>
<h3 id="inference">Inference</h3>
<p>Inference is mean-field variational inference</p>
<pre class="python" language="Python"><code>inference = ed.MFVI(model, variational, data)
inference.run(n_iter=1000, n_print=10)</code></pre>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
