% Define the subtitle of the page
\title{KL(q||p) minimization}

% Begin the content of the page
\subsection{$\text{KL}(q\|p)$ minimization}

(We recommend reading the 
\href{tut_bayesian_inference.html}{Bayesian inference} tutorial first.)

Variational inference is a powerful approach to approximating the posterior. The
core idea has two steps:
\begin{enumerate}
   \item propose a tractable approximation to the posterior $q(z\;;\;\lambda)$,
   and
   \item match the approximation to the posterior.
 \end{enumerate} 
This strategy converts the problem of computing the posterior $p(z \mid x)$ into
an optimization problem of minimizing some divergence metric
\begin{align*}
  \lambda^* 
  &=
  \arg\min_\lambda \text{divergence}( 
  p(z \mid x)
  ,
  q(z\;;\;\lambda)
  ).
\end{align*}

One form of variational inference minimizes the Kullback-Leibler divergence 
\textbf{from} $q(z\;;\;\lambda)$ \textbf{to} $p(z \mid x)$,
\begin{align*}
  \lambda^* 
  &=
  \arg\min_\lambda \text{KL}( 
  q(z\;;\;\lambda)
  \|
  p(z \mid x)
  )\\
  &=
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(z \mid x)
  -
  \log q(z\;;\;\lambda)
  \big].
\end{align*}
The KL divergence is a non-symmetric, information theoretic metric of
similiarity between two probability distributions.
