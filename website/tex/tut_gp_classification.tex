% Define the subtitle of the page
\title{Gaussian process classification}

% Begin the content of the page
\subsection{Gaussian process classification}

\begin{itemize}
  \item \textbf{Probability Model:} Gaussian process classification
  \item \textbf{Variational Model:} Fully factorized normal
  \item \textbf{Data:} Crabs dataset (\href
  {https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/crabs.html}{link})
  \item \textbf{Inference:} Mean-field variational inference
  \item \textbf{Criticism:} None
\end{itemize}

Full Python code in: 
\href{https://github.com/blei-lab/edward/blob/master/examples/gp_classification.py}
{gp_classification.py}


\subsubsection{Probability Model}
Define the likelihood of an observation $(y_n, x_n)$ as
\begin{align*}
  p(y_n \mid \mathbf{z} \;;\; x_n)
  &=
  \text{Bernoulli}(y_n \mid \text{logit}^{-1}(x_n * \mathbf{z})).
\end{align*}

Define the prior to be a multivariate normal
\begin{align*}
  p(\mathbf{z})
  &=
  \mathcal{N}(\mathbf{z} \;;\; \mathbf{0}, K)
\end{align*} 
 where $\mathbf{z}$ are weights drawn from a Gaussian process with covariance
 matrix
 given by $k(x, x^\prime)$ for each pair of inputs $(x, x^\prime)$, and with a
 squared-exponential kernel and known kernel hyperparameters.

The model in Edward is 
\begin{lstlisting}[language=Python]
class GaussianProcess:
    """
    Gaussian process classification

    p((x,y), z) = Bernoulli(y | logit^{-1}(x*z)) *
                  Normal(z | 0, K),

    where z are weights drawn from a GP with covariance given by k(x,
    x') for each pair of inputs (x, x'), and with squared-exponential
    kernel and known kernel hyperparameters.

    Parameters
    ----------
    N : int
        Number of data points.
    sigma : float, optional
        Signal variance parameter.
    l : float, optional
        Length scale parameter.
    """
    def __init__(self, N, sigma=1.0, l=1.0):
        self.N = N
        self.sigma = sigma
        self.l = l

        self.num_vars = N
        self.inverse_link = tf.sigmoid

    def kernel(self, x):
        mat = []
        for i in range(self.N):
            mat += [[]]
            xi = x[i, :]
            for j in range(self.N):
                if j == i:
                    mat[i] += [multivariate_rbf(xi, xi, self.sigma, self.l)]
                else:
                    xj = x[j, :]
                    mat[i] += [multivariate_rbf(xi, xj, self.sigma, self.l)]

            mat[i] = tf.pack(mat[i])

        return tf.pack(mat)

    def log_prob(self, xs, zs):
        """Returns a vector [log p(xs, zs[1,:]), ..., log p(xs, zs[S,:])]."""
        # Data must have labels in the first column and features in
        # subsequent columns.
        y = xs[:, 0]
        x = xs[:, 1:]
        log_prior = multivariate_normal.logpdf(zs, cov=self.kernel(x))
        log_lik = tf.pack([tf.reduce_sum(
            bernoulli.logpmf(y, self.inverse_link(tf.mul(y, z)))
            ) for z in tf.unpack(zs)])
        return log_prior + log_lik

model = GaussianProcess(N=number_of_datapoints)
\end{lstlisting}


\subsubsection{Variational Model}
Define the variational model to be a fully factorized normal
\begin{align*}
  q(\mathbf{z} \;;\; \lambda)
  &=
  \mathcal{N}(\mathbf{z} \;;\; \lambda_\mu, \lambda_{\sigma^2} I).
\end{align*} 

The model in Edward is
\begin{lstlisting}[language=Python]
variational = Variational()
variational.add(Normal(model.num_vars))
\end{lstlisting}


\subsubsection{Data}

The \texttt{crabs} dataset is
\begin{lstlisting}[language=Python]
df = np.loadtxt('data/crabs_train.txt', dtype='float32', delimiter=',')[:25, :]
data = ed.Data(tf.constant(df, dtype=tf.float32))
\end{lstlisting}


\subsubsection{Inference}

Inference is mean-field variational inference
\begin{lstlisting}[language=Python]
inference = ed.MFVI(model, variational, data)
inference.run(n_iter=500)
\end{lstlisting}
