% Define the subtitle of the page
\title{Metric-based evaluations}

% Begin the content of the page
\subsection{Metric-based evaluations}

Edward supports metric-based evaluations. The simplest example is 
a classification model. After inferring the posterior, we can predict the
label for each observation in the data. Edward implements a
variety of metrics, such as classification error and mean absolute error, based
on these predictions.

\subsubsection{Implementation}

Edward implements metric-based evaluations through the 
\texttt{predict()} function in the probability model. This
predicts the label given samples from the posterior $p(z \mid x)$
\begin{lstlisting}[language=Python]
class BayesianLinearRegression:
    def __init__(self, lik_variance=0.01, prior_variance=0.01):
        self.lik_variance = lik_variance
        self.prior_variance = prior_variance
        self.num_vars = 11

    def log_prob(self, xs, zs):
        x, y = xs['x'], xs['y']
        log_prior = -self.prior_variance * tf.reduce_sum(zs*zs, 1)
        b = zs[:, 0]
        W = tf.transpose(zs[:, 1:])
        mus = tf.matmul(x, W) + b
        y = tf.expand_dims(y, 1)
        log_lik = -tf.reduce_sum(tf.pow(mus - y, 2), 0) / self.lik_variance
        return log_lik + log_prior

    def predict(self, xs, zs):
        """Returns a prediction for each data point, averaging over
        each set of latent variables z in zs; and also return the true
        value."""
        x_test = xs['x']
        b = zs[:, 0]
        W = tf.transpose(zs[:, 1:])
        y_pred = tf.reduce_mean(tf.matmul(x_test, W) + b, 1)
        return y_pred  
\end{lstlisting}

The \texttt{ed.evaluate()} method then evaluates a model as
\begin{lstlisting}[language=Python]
print(ed.evaluate('categorical_accuracy', my_model, my_variational, my_data))
print(ed.evaluate('mean_absolute_error', my_model, my_variational, my_data))
\end{lstlisting}
Swapping \texttt{my_data} with held-out data makes it easy to implement
cross-validation and other model assessment techniques.
