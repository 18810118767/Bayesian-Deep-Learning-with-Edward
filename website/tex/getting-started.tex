% Define the subtitle of the page
\title{Getting Started}

% Begin the content of the page
\subsection{Getting Started}
Getting started with Edward is easy.

\subsubsection{Quick Installation}
To install the latest stable version, run

\begin{lstlisting}[language=Java]
pip install edward
\end{lstlisting}

To install the latest development version, run

\begin{lstlisting}[language=Java]
pip install -e "git+https://github.com/blei-lab/edward.git#egg=edward"
\end{lstlisting}

See the \href{troubleshooting.html}{troubleshooting page} for detailed
installation instructions.


\subsubsection{Your first Edward program}

Probabilistic models are built in Edward using one of the available
modeling languages. Here we use TensorFlow to define a Bayesian neural
network. It is a neural network with prior distributions on its
weights.
(This is abridged; complete example
\href{https://github.com/blei-lab/edward/blob/master/examples/getting_started_example.py}
{here}.)
\begin{lstlisting}[language=Python]
import tensorflow as tf
from edward.stats import norm

class BayesianNN:
    """Bayesian neural network for regression"""
    def log_prob(self, xs, zs):
        # Extract inputs/outputs from data `xs`.
        x, y = xs['x'], xs['y']
        # Specify the prior on neural network weights `zs`.
        log_prior = tf.reduce_sum(norm.logpdf(zs, loc=0, scale=1), 1)
        # Specify the likelihood. Its mean is the output of a neural
        # network taking `x` as input with weights `zs`.
        mus = self.neural_network(x, zs)
        log_lik = tf.reduce_sum(norm.logpdf(y, loc=mus, scale=1), 1)
        return log_prior + log_lik
\end{lstlisting}

Instantiating this model as
\begin{lstlisting}[language=Python]
model = BayesianNN(layer_sizes=[1, 2, 2, 1])
\end{lstlisting}
defines a two-layer Bayesian neural network with $\tanh$
nonlinearities.

Let's use a toy dataset of 50 observations with a cosine relationship.
\begin{lstlisting}[language=Python]
import numpy as np

data = {'x': np.array(...), 'y': np.array(...)}
\end{lstlisting}
\includegraphics[width=700px]{images/getting-started-fig0.png}

Next, make inferences about the model from data.
Edward focuses on variational inference. Specify a mean-field normal
approximation
\begin{lstlisting}[language=Python]
from edward.models import Variational, Normal

variational = Variational()
variational.add(Normal(model.n_vars))
\end{lstlisting}
with the number of latent variables given by the model.

Run mean-field variational inference. We use \texttt{1000} iterations
with a batch of \texttt{10} datapoints per iteration.
\begin{lstlisting}[language=Python]
import edward as ed

inference = ed.MFVI(model, variational, data)
inference.run(n_iter=1000, n_minibatch=10)
\end{lstlisting}

Finally, criticize the model fit. Bayesian neural networks define a
distribution over neural networks, so let's do a graphical check. Draw
neural networks from the inferred model and visualize how well it
fits the data.

\includegraphics[width=700px]{images/getting-started-fig2.png}

To learn more about Edward, \href{delving-in.html}{delve in}!

If you prefer to learn via examples, then check out some
\href{tutorials.html}{tutorials}.
