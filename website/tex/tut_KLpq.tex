% Define the subtitle of the page
\title{KL(p||q) minimization}

% Begin the content of the page
\subsection{$\text{KL}(p\|q)$ minimization}

Variational inference is a powerful approach for inferring the posterior. The
core idea involves two steps:
\begin{enumerate}
   \item posit a family of distributions $q(z\;;\;\lambda)$ over the
   posterior latent variables;
   \item match $q(z\;;\;\lambda)$ to the posterior by optimizing over its
   parameters $\lambda$.
 \end{enumerate}
This strategy converts the problem of computing the posterior $p(z \mid x)$ into
an optimization problem, which minimizes a divergence measure
\begin{align*}
  \lambda^*
  &=
  \arg\min_\lambda \text{divergence}(
  p(z \mid x)
  ,
  q(z\;;\;\lambda)
  ).
\end{align*}
The optimized distribution $q(z\;;\;\lambda)$ is then used as a
proxy to the posterior $p(z\mid x)$.

One form of variational inference minimizes the Kullback-Leibler divergence
\textbf{from} $p(z \mid x)$ \textbf{to} $q(z\;;\;\lambda)$,
\begin{align*}
  \lambda^*
  &=
  \arg\min_\lambda \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )\\
  &=
  \arg\min_\lambda\;
  \mathbb{E}_{p(z \mid x)}
  \big[
  \log p(z \mid x)
  -
  \log q(z\;;\;\lambda)
  \big].
\end{align*}
The KL divergence is a non-symmetric, information theoretic measure of
similarity between two probability distributions.

\subsubsection{Minimizing an intractable objective function}

The $\text{KL}(p\|q)$ objective we seek to minimize is intractable; it directly
involves the posterior $p(z \mid x)$. Ignoring this for the moment, consider its
gradient
\begin{align*}
  \nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &=
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big].
\end{align*}
Both $\text{KL}(p\|q)$ and its gradient are intractable
because of the posterior expectation.
We can use \href{tut_KLpq_ais.html}{importance sampling} to both
estimate the objective and calculate stochastic gradients.
