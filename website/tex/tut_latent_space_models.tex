\title{Latent Space Models}

\subsection{Latent Space Models}

Many interesting data sets are networks. For example, social networks or biological neural networks.

Often the question arises what we can learn about the nodes of the network from the connectivity patterns. This question can be addressed by fitting a latent space model [1].
Latent space models are latent variable models which embed the nodes of the network in a latent space which captures attributes of the nodes. The likelihood to form an edge between two nodes depends on their distance in the latent space.

We demonstrate how to do this in Edward with an example from neuroscience. The script can be found \href{https://github.com/blei-lab/edward/blob/master/examples/latent_space_model.py}{here}.

\subsubsection{Data}

The data comes from \href{http://www-personal.umich.edu/~mejn/netdata/}{here}.
It is a weighted, directed network representing the neural network of the nematode \href{https://en.wikipedia.org/wiki/Caenorhabditis_elegans}{C. Elegans} compiled by Duncan Watts and Steven Strogatz [2] from original experimental data by White et al. [3]. 

The neural network consists of around $300$ neurons. Each connection is associated with a weight (positive integer) capturing the strength of the connection.

In the example we load the data with:
\begin{lstlisting}[language=Python]
data, N = load_celegans_brain()
\end{lstlisting}

\subsubsection{Model}

Here we will fit a latent space model to the C.Elegans neural network. What can we learn about the neurons from how they are connected? We will learn a latent embedding for each neuron to capture the similarities between them.

Each neuron $n$ is a node in the network and is associated with a latent $K$-dimensional vector $z_n$.

We place a Gaussian prior on each of the latent vectors.
In [1] the log-odds of an edge between node $i$ and $j$ is proportional to the euclidean distance between the latent representations of the nodes $|z_i- z_j|$. Here, we model instead model the weights ($Y_{ij}$) of the edges with a Poisson likelihood. The rate is the reciprocal of the distance in latent space. This is the generative process:

For each node $n$:
\begin{align}
z_n \sim N(0,I)
\end{align}
For each edge $(i,j)$:
\begin{align}
Y_{ij} \sim \text{Poisson}(\frac{1}{|z_i - z_j|})
\end{align}

Define and instantiate the model in edward as follows:
\begin{lstlisting}[language=Python]
class Latent_Space_Model():
    def __init__(self,N,K):
        self.num_vars = N * K
        self.N = N
        self.K = K

    def log_prob(self, xs, zs):
        """Returns a vector [log p(xs, zs[1,:]), ..., log p(xs, zs[S,:])]."""
        log_prior = - tf.reduce_sum(zs*zs)
        z = tf.reshape(zs,[self.N,self.K])
	# compute reciprocal of euclidean distance
        xp = tf.matmul(tf.ones([1,self.N]),tf.reduce_sum(z*z,1,keep_dims=True))
        xp = xp + tf.transpose(xp) - 2*tf.matmul(z,z,transpose_b = True)
        xp = 1.0/xp
        log_lik = tf.reduce_sum(poisson.logpmf(xs,xp))
        return log_lik + log_prior

K = 3
model = Latent_Space_Model(N,K)
\end{lstlisting}

\subsubsection{Inference}

Maximum a posteriori (MAP) inference is simple in Edward. Two lines are required: Instantiating inference and running it.
\begin{lstlisting}[language=Python]
inference = ed.MAP(model, data)
var = inference.run(n_iter=5000, n_print=500)
\end{lstlisting}

\href{tut_MAP.html}{Here}
is a more extensive tutorial on MAP inference in Edward.

One could instead run variational inference. This requires specifying a variational model and instantiating MFVI.
\begin{lstlisting}[language=Python]
variational = Variational()
variational.add(Normal(model.num_vars))
inference = ed.MFVI(model, variational,data)
\end{lstlisting}
Finally, the following line runs the inference procedure:
\begin{lstlisting}[language=Python]
var = inference.run(n_iter=5000, n_print=500)
\end{lstlisting}
For a tutorial on VI in Edward click \href{tut_MAP.html}{here}.

\subsubsection{References}
\begin{thebibliography}{}
\bibitem[\protect\astroncite{Hoff et~al.}{2002}]{hoff2002latent}
[1] Hoff, P.~D., Raftery, A.~E., and Handcock, M.~S. (2002).
\newblock Latent space approaches to social network analysis.
\newblock {\em Journal of the american Statistical association},
  97(460):1090--1098.

\bibitem[\protect\astroncite{Watts and Strogatz}{1998}]{watts1998collective}
[2] Watts, D.~J. and Strogatz, S.~H. (1998).
\newblock Collective dynamics of ‘small-world’networks.
\newblock {\em nature}, 393(6684):440--442.

\bibitem[\protect\astroncite{White et~al.}{1986}]{white1986structure}
[3] White, J.~G., Southgate, E., Thomson, J.~N., and Brenner, S. (1986).
\newblock The structure of the nervous system of the nematode caenorhabditis
  elegans.
\newblock {\em Philos Trans R Soc Lond B Biol Sci}, 314(1165):1--340.
\end{thebibliography}

