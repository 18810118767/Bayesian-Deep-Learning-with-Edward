% Define the subtitle of the page
\title{Design Philosophy}

% Begin the content of the page
\subsection{Design Philosophy}

\textbf{Edward} serves two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  to serve as a foundation for research;
\item
  to provide a unified library for inference and criticism, with
  modeling tools at our disposal.
\end{enumerate}

As a research tool, the code base serves as a testbed for fast
experimentation, so that we can easily play with extensions of current
paradigms. For example, to evaluate new inference algorithms, the
experiments only need to be written once and we can simply swap the
inference with the newly proposed algorithm. For certain inference
algorithms, this corresponds to inheriting from a current inference
class and simply writing one function. This idea applies to all
components of probabilistic modeling: we can leverage the built-in
inference algorithms to develop new complex models, or use the
built-in models and inference to develop new criticism techniques.

As an applied tool, Edward supports a wide variety of settings,
ranging from classical hierarchical models on small data sets to
complex deep probabilistic models on large data sets. With
\href{https://www.tensorflow.org}{TensorFlow} as a backend, Edward can
leverage features such as computational graphs, distributed training,
and CPU/GPU integration to deploy probabilistic modeling at scale. It
also has the fundamentals: for example, linear model examples, mixture
model examples, probabilistic neural networks, and stochastic and
black box variational inference.

\subsection{Related Software}\label{related-software}

There are several notable themes in Edward.

\textbf{Probabilistic programming.}
There has been outstanding work on generic
languages for specifying probabilistic models, or probabilistic
programs. Among these include
\href{http://probcomp.csail.mit.edu/venture/}{Venture},
\href{http://www.robots.ox.ac.uk/~fwood/anglican/literature/index.html}{Anglican},
\href{https://www.cra.com/work/case-studies/figaro}{Figaro},
\href{http://mc-stan.org}{Stan}, and
\href{http://dippl.org/chapters/02-webppl.html}{WebPPL}.
Edward focuses not only on easily building models and
automated inference engines, but on
all aspects of data analysis.
We are agnostic to the modeling language.
For example,
we use Stan's modeling language as an easy way to specify models in
Edward, and our library enables fast experimentation on their
inference and criticism.

\textbf{Black box inference.}
Black box algorithms are typically Monte Carlo methods, and
by design make very few assumptions about the model.
The most important distinction in Edward comes from our motivation.
We are interested in deploying probabilistic models to a myriad of
real world applications, ranging from the size of data and
data structure, such as large text corpora or many brief audio signals,
to the size of model and class of models, such as small nonparametric
models or deep generative models. This presents a new set of
challenges in both research design and software implementation.
As one consequence, we focus on variational inference.
As another consequence, our inference
algorithms aim to take advantage of as much structure as possible from
the model. Thus Edward supports all types of inference, whether they
be black box or model-specific.

\textbf{Computational frameworks.}
There are many computational frameworks, primarily built for deep
learning: as of this date, this includes
\href{https://www.tensorflow.org}{TensorFlow},
\href{http://deeplearning.net/software/theano/}{Theano},
\href{http://torch.ch}{Torch},
\href{http://rll.berkeley.edu/cgt/}{Computational Graph Toolkit},
\href{https://github.com/NervanaSystems/neon}{neon}, and
\href{https://github.com/stan-dev/math}{Stan Math Library}. These are
incredible tools which Edward employs as a backend. In
terms of abstraction, Edward sits at one level higher. For example,
we chose to use TensorFlow among the bunch due to Python as our language
of choice for machine learning research and as an investment in Google's
massive engineering.

\textbf{High-level deep learning libraries.}
Neural network libraries such as
\href{https://github.com/fchollet/keras}{Keras} and
\href{https://github.com/Lasagne/Lasagne}{Lasagne} are at a similar
abstraction level as us, but they are primarily interested in
parameterizing complicated functions for supervised learning on large
datasets. We are interested in probabilistic models which apply
to a wide array of learning tasks, and which can have both
complicated likelihood and complicated priors (neural networks are an
option but not a necessity). Therefore our goals are orthogonal and in
fact mutually benefit each other. For example, we use Keras'
abstraction as a way to easily specify models parameterized by deep
neural networks.
