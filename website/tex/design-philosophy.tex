% Define the subtitle of the page
\title{Design Philosophy}

% Begin the content of the page
\subsection{Design Philosophy}

\textbf{Edward} serves two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  to serve as a foundation for research;
\item
  to provide an open-source, unified library for inference and
  criticism, with modeling tools at our disposal.
\end{enumerate}

As a research tool, the code base is easily interchangeable so that we
can play with extensions for further avenues of investigation, without
having to re-implement everything from scratch. For example, for testing
new inference algorithms, the model experiments only need to be written
once, and we can simply swap the inference object with the newly
proposed algorithm. For certain inference algorithms, this corresponds
to inheriting from a current inference class and writing a new function
for computing stochastic gradients. The same idea applies at a lower
level: we can leverage the optimizers and visualization tools already
available.

As a practical tool, Edward has many desirable features such as
computational graphs for automatic differentiation, GPU support and
distributed implementations, support for a variety of modeling
languages. It also has the fundamentals: linear model examples, data
subsampling for scaling to massive data sets, mean-field variational
inference, and so on.

\subsection{Related Software}\label{related-software}

There are several notable themes in Edward.

\textbf{Probabilistic programming.} There has been incredible progress
recently on developing generic languages for specifying probability
models, or probabilistic programs. Among these include
\href{http://probcomp.csail.mit.edu/venture/}{Venture},
\href{http://www.robots.ox.ac.uk/~fwood/anglican/literature/index.html}{Anglican},
\href{https://www.cra.com/work/case-studies/figaro}{Figaro},
\href{http://mc-stan.org}{Stan}, and
\href{http://dippl.org/chapters/02-webppl.html}{WebPPL}. We do not work
on modeling languages; we work on general-purpose algorithms, which can
be applied to any of these languages. For example, we use Stan's
modeling language as an easy way to specify probability models, and our
library performs inference on them.

\textbf{Computational frameworks.} There are many computational
frameworks, primarily built for deep learning: as of this date, this
includes \href{http://deeplearning.net/software/theano/}{Theano},
\href{https://www.tensorflow.org}{TensorFlow},
\href{http://torch.ch}{Torch},
\href{http://rll.berkeley.edu/cgt/}{Computational Graph Toolkit},
\href{https://github.com/NervanaSystems/neon}{neon}, and
\href{https://github.com/stan-dev/math}{Stan Math Library}. These are
incredible back end tools, which we use rather than compete with. In
terms of abstraction, our library sits at one level higher. For example,
we chose to use TensorFlow among the bunch due to Python as our language
of choice for machine learning research and as an investment in Google's
prodigious engineering.

\textbf{High-level deep learning libraries.} Neural network libraries
such as \href{https://github.com/fchollet/keras}{Keras} and
\href{https://github.com/Lasagne/Lasagne}{Lasagne} are at a similar
abstraction level as us, but they are interested in parameterizing
complicated likelihood functions in order to minimize a loss. We are
more interested in inferring Bayesian hierarchical models, which can
have both complicated likelihood and complicated priors (neural networks
are an option but not a necessity). Therefore our goals are orthogonal,
and in fact mutually benefit each other: for example, we use Keras'
abstraction as a way to easily specify models parameterized by deep
neural networks.

\textbf{Scalable inference.} Implementations of black box inference
algorithms are usually tied to a specific probabilistic programming
language, and the majority of these algorithms focus on non-gradient
based approaches using sequential Monte Carlo. We are agnostic to the
modeling language. Further, we are interested in scalable approaches
using gradient information from the model and optimization of an
objective function. The most related in this line is
\href{http://mc-stan.org}{Stan}'s algorithms such as
\href{http://arxiv.org/abs/1506.03431}{ADVI}. However, we don't aim to
be as user-friendly by automating as many internals as possible. This is
because this library is first and foremost a research tool, where we are
interested in not abstracting away the specific mathematics but
understanding the mechanics behind them. Further, our developed
algorithms will eventually make its way into more mature,
enterprise-level software (such as Stan). As several of us are also core
developers in Stan, we do prototyping of new algorithms here, and deploy
them in Stan when they're ready to be applied industry-wide.
