% Define the subtitle of the page
\title{Tutorials}

% Begin the content of the page
\subsection{Tutorials}

Edward supports a variety of probability models, inference
algorithms, and criticism procedures. Here we show a few key examples from each
category. We recommend \href{getting-started.html}{Getting Started} and
\href{delving-in.html}{Delving In} before continuing to the tutorials.

\subsubsection{Models}

\begin{itemize}
\item \href{tut_model.html}{Probability models}
\item \href{tut_bayesian_linear_regression.html}{Bayesian linear regression}
\item \href{#}{Hierarchical logistic regression}
\item \href{tut_mixture_gaussian.html}{Mixture of Gaussians}
\item \href{tut_gp_classification.html}{Gaussian process classification}
\item \href{tut_bayesian_nn.html}{Bayesian neural network}
\end{itemize}

\subsubsection{Inference}

\begin{itemize}
  \item \href{tut_inference.html}{Inference of probability models}
  \item \href{tut_KLqp.html}{$\text{KL}(q\|p)$ minimization (variational
  inference)}
  \begin{itemize}
    \item \href{tut_KLqp_score.html}{Score function method}
    \item \href{tut_KLqp_reparam.html}{Reparameterization method}
  \end{itemize}
  \item \href{tut_KLpq.html}{$\text{KL}(p\|q)$ minimization (variational
  inference)}
  \begin{itemize}
    \item \href{tut_KLpq_ais.html}{Adaptive importance sampling}
  \end{itemize}
  \item \href{tut_MAP.html}{Maximum a posteriori estimation}
  \begin{itemize}
    \item \href{tut_MAP_Laplace.html}{Laplace approximation}
  \end{itemize}
  \item \href{#}{Importance-weighted variational inference}
\end{itemize}

\subsubsection{Criticism}

\begin{itemize}
  \item \href{tut_point_eval.html}{Point-based evaluations}
  \item \href{tut_PPC.html}{Posterior predictive checks}
\end{itemize}

\subsubsection{End-to-end}

\begin{itemize}
  \item \href{#}{Tractable posteriors} (explain this is a useful toy model to play
    around with in a new inference algorithm, focused on normal
    posterior)
  \item \href{#}{Conjugate models} (explain this is a useful toy model to play
    around with in a new inference algorithm, focused on
    beta-bernoulli)
  \item \href{#}{Supervised learning (regression)} (use the bayesian linear regression demo)
  \item \href{#}{Supervised learning (classification)} (use the gp classification demo)
  \item \href{#}{Unsupervised learning} (use the mixture gaussian's demo)
  \item \href{#}{Mixture density network}
  \item \href{#}{Variational auto-encoder}
\end{itemize}
