% Define the subtitle of the page
\title{Adaptive importance sampling}

% Begin the content of the page
\subsection{Adaptive importance sampling}

(This tutorial follows the
\href{tut_KLpq.html}{$\text{KL}(p\|q)$ minimization} tutorial.)

We seek to minimize $\text{KL}(p\|q)$ by following its gradient
\begin{align*}
  \nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &=
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big].
\end{align*}
using a ``black box'' algorithm. This means generically inferring the
posterior while making few assumptions about the model.

\subsubsection{Importance sampling}

The gradient is an expectation with respect to the unknown posterior. Combining
Bayes' rule with importance sampling can circumvent this obstacle.

First rewrite the expectation to be with respect to the variational
distribution,
\begin{align*}
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big]
  &=
  -
  \mathbb{E}_{q(z\;;\;\lambda)}
  \Bigg[
  \frac{p(z \mid x)}{q(z\;;\;\lambda)}
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \Bigg].
\end{align*}

We then use importance sampling to obtain a noisy estimate of this gradient.
The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{z_s\}_1^S \sim q(z\;;\;\lambda)$,
  \item evaluate $\nabla_\lambda\; \log q(z_s\;;\;\lambda)$,
  \item compute the normalized importance weights
  \begin{align*}
    w_s
    &=
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
    \Bigg/
    \sum_{s=1}^{S}
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
  \end{align*}
  \item compute the weighted mean.
\end{enumerate}
The key insight is that we can use the joint $p(x,z)$ instead of the posterior
when estimating the normalized importance weights
\begin{align*}
  w_s
  &=
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} \\
  &=
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}.
\end{align*}
This follows from Bayes' rule
\begin{align*}
  p(z \mid x)
  &=
  p(x, z) / p(x)\\
  &=
  p(x, z) / \text{a constant function of }z.
\end{align*}

Importance sampling thus gives the following noisy yet unbiased gradient
estimate
\begin{align*}
\nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &=
  -
  \frac{1}{S}
  \sum_{s=1}^S
  w_s
  \nabla_\lambda\; \log q(z_s\;;\;\lambda).
\end{align*}
The objective $\text{KL}(p\|q)$ can be calculated in a similar fashion.
The only new ingredient for its gradient is the score function
$\nabla_\lambda \log q(z\;;\;\lambda)$.  Edward uses automatic
differentiation, specifically with TensorFlow's computational graphs,
making this gradient computation both simple and efficient to
distribute.

Adaptive importance sampling follows this gradient to a local optimum using
stochastic optimization. It is adaptive because the variational distribution
$q(z\;;\;\lambda)$ iteratively gets closer to the posterior $p(z \mid x)$.

\subsubsection{Implementation}

We implement $\text{KL}(p\|q)$ with its importance sampling gradient
in the method \texttt{build_loss} of the \texttt{KLpq} class.
\begin{lstlisting}[language=Python]
class KLpq(VariationalInference):
    ...
    def build_loss(self):
        x = self.data
        z = self.variational.sample(self.n_samples)

        # normalized importance weights
        q_log_prob = self.variational.log_prob(stop_gradient(z))
        log_w = self.model.log_prob(x, z) - q_log_prob
        log_w_norm = log_w - log_sum_exp(log_w)
        w_norm = tf.exp(log_w_norm)

        self.loss = tf.reduce_mean(w_norm * log_w)
        return -tf.reduce_mean(q_log_prob * tf.stop_gradient(w_norm))
\end{lstlisting}

This method draws $S$ (\texttt{self.n_samples}) samples from the variational
model. The TensorFlow function \texttt{reduce_mean} takes the
mean over the samples.

The method stores computation of $\text{KL}(p\|q)$ in
\texttt{self.loss}, which can be used to track progress of the
inference for diagnostics.  The method returns an object whose
automatic differentiation is a stochastic gradient of $\text{KL}(p\|q)$.
The TensorFlow function \texttt{stop_gradient} tells the computational
graph to stop traversing nodes to propagate gradients. In this case,
the only gradients taken are $\nabla_\lambda \log q(z_s\;;\;\lambda)$,
because it is not wrapped in a \texttt{stop_gradient}.

Computing the normalized importance weights is a numerically challenging task.
Underflow is a common issue. Here we use the \texttt{log_sum_exp} trick to
compute weights in the log space. However, the normalized weights are still
exponentiated to compute the gradient. As with importance sampling in
general, this inference method does not scale to high dimensions.

\subsubsection{Stochastic gradient optimization}

Stochastic gradient descent is an extension of gradient descent using
noisy estimates of the gradient. Under mild conditions, stochastic
gradient descent finds a (local) optimum of the original (noiseless)
function.

Edward uses TensorFlow's optimizers.
This is specified in the \texttt{initialize} method of the
\texttt{VariationalInference} base class.

\begin{lstlisting}[language=Python]
class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use Adam with a decaying scale factor
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)
\end{lstlisting}

This sets up TensorFlow to optimize $\text{KL}(p\|q)$ using ADAM's
learning rate combined with an exponentially decaying scale factor.

See the \href{api/index.html}{API} for more details.

\subsubsection{References}\label{references}

\begin{itemize}
\item
  Oh, M. S., & Berger, J. O. (1992). Adaptive importance sampling in
  Monte Carlo integration. Journal of Statistical Computation and
  Simulation, 41(3-4), 143-168.
\end{itemize}
