% Define the subtitle of the page
\title{Adaptive importance sampling}

% Begin the content of the page
\subsection{Adaptive importance sampling}

(This tutorial follows the
\href{tut_KLpq.html}{$\text{KL}(p\|q)$ minimization} tutorial.)

We seek to minimize $\text{KL}(p\|q)$ by following its gradient
\begin{align*}
  \nabla_\lambda\;
  \text{KL}( 
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &=
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big].
\end{align*}
to a local minimum using a ``black box'' algorithm. This means avoiding
additional mathematical derivations, such as calculating integrals 
(expectations) or gradients (for maximization) by hand.

\subsubsection{Importance sampling}

The gradient is an expectation with respect to the unknown posterior. Combining
Bayes rule with importance sampling can circumvent this obstacle. 

Begin by construing the expectation with respect to the variational
approximation,
\begin{align*}
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big]
  &=
  -
  \mathbb{E}_{q(z\;;\;\lambda)}
  \Bigg[
  \frac{p(z \mid x)}{q(z\;;\;\lambda)}
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \Bigg].
\end{align*}

We then use importance sampling to obtain a noisy estimate of this gradient.
The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{z_s\}_1^S \sim q(z\;;\;\lambda)$,
  \item evaluate $\nabla_\lambda\; \log q(z_s\;;\;\lambda)$,
  \item compute the normalized importance weights
  \begin{align*}
    w_s
    &=
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} 
    \Bigg/ 
    \sum_{s=1}^{S}
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} 
  \end{align*}
  \item compute the weighted mean.
\end{enumerate}
The key insight is that we can use the joint $p(x,z)$ instead of the posterior
when estimating the normalized importance weights
\begin{align*}
  w_s
  &=
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} 
  \Bigg/ 
  \sum_{s=1}^{S}
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} \\
  &=
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)} 
  \Bigg/ 
  \sum_{s=1}^{S}
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}.
\end{align*}
This follows from Bayes' rule
\begin{align*}
  p(z \mid x)
  &=
  p(x, z) / p(x)\\
  &=
  p(x, z) / \text{a constant function of }z.
\end{align*}

Importance sampling thus gives the following noisy yet unbiased gradient
estimate
\begin{align*}
\nabla_\lambda\;
  \text{KL}( 
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &=
  -
  \frac{1}{S}
  \sum_{s=1}^S
  w_s
  \nabla_\lambda\; \log q(z_s\;;\;\lambda).
\end{align*}
The only additional computation here is the gradient, which is easy to automate
using automatic differentation.

Adaptive importance sampling follows this gradient to a local optimum using
stochastic optimization. It is adaptive because the variational approximation
$q(z\;;\;\lambda)$ iteratively gets closer to the posterior $p(z \mid x)$.

\subsubsection{Implementation}

We implement
We implement $\text{KL}(p\|q)$ and its importance sampling gradient
estimator in a single method \texttt{build_loss} of the \texttt{KLqp}class.
\begin{lstlisting}[language=Python]
class KLpq(VariationalInference):
    ...
    def build_loss(self):
        ...
        x = self.data
        z = self.variational.sample(self.n_minibatch)

        # normalized importance weights
        q_log_prob = self.variational.log_prob(stop_gradient(z))
        log_w = self.model.log_prob(x, z) - q_log_prob
        log_w_norm = log_w - log_sum_exp(log_w)
        w_norm = tf.exp(log_w_norm)

        self.loss = tf.reduce_mean(w_norm * log_w)
        return -tf.reduce_mean(q_log_prob * tf.stop_gradient(w_norm))
\end{lstlisting}

This method draws $S$ (\texttt{self.n_minibatch}) samples from the variational
distribution; the TensorFlow method \texttt{reduce_mean} computes the Monte
Carlo summation. 

The TensorFlow method \texttt{stop_gradient} implicitly specifies the gradient; 
it tells TensorFlow where to stop automatic differentiation. In
this case, it will only compute $\nabla_\lambda \log q(z_s\;;\;\lambda)$ because
it is not wrapped in a \texttt{stop_gradient}.

Computing the normalized importance weights is a numerically challenging task.
Underflow is a common issue. Here we use the \texttt{log_sum_exp} trick to
compute weights in the log space. However, the normalized weights are still
exponentiated to compute the gradient. Thus, this method is limited to
relatively low-dimensional models.

\subsubsection{Stochastic gradient optimization}

Stochastic gradient descent is an extension of gradient descent that permits
following noisy estimates of the gradient. Under mild conditions, stochastic
gradient descent finds a (local) optimum of the original (noiseless) function.

Edward uses TensorFlow's built-in stochastic gradient minimization routines.
This is specified in the \texttt{initialize} method of the 
\texttt{VariationalInference} base class.

\begin{lstlisting}[language=Python]
class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use Adam with a decaying scale factor
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)
\end{lstlisting}

This sets up TensorFlow to minimize $\text{KL}(p\|q)$ using the Adam adaptive
step-size schedule with an exponentially decaying scaling factor.

See the \href{api/index.html}{API} for further implementation details.
