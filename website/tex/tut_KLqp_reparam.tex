% Define the subtitle of the page
\title{Reparameterization gradient}

% Begin the content of the page
\subsection{Reparameterization gradient}

(This tutorial follows the
\href{tut_KLqp.html}{$\text{KL}(q\|p)$ minimization} tutorial.)

We seek to maximize the ELBO,
\begin{align*}
  \lambda^*
  &=
  \arg \max_\lambda \text{ELBO}(\lambda)\\
  &=\;
  \arg \max_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
using a ``black box'' algorithm. This means generically inferring the
posterior while making few assumptions about the model.

If the model has differentiable latent variables, then it is generally
advantageous to leverage gradient information from the model in order to
better traverse the optimization space. One approach to doing this is
the reparameterization gradient.

\subsubsection{The reparameterization identity}

Gradient descent is a standard approach for optimizing complicated
objectives like the ELBO. The idea is to calculate its gradient
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=
  \nabla_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
and update the current set of parameters proportional to the gradient.

Some variational distributions $q(z\;;\;\lambda)$ admit useful
reparameterizations. For example, we can reparameterize a normal distribution
$z \sim \mathcal{N}(\mu, \Sigma)$ as
$z \sim \mu + L \mathcal{N}(0, I)$ where $\Sigma = LL^\top$. In general, write
this as
\begin{align*}
  \epsilon &\sim q(\epsilon)\\
  z &= z(\epsilon \;;\; \lambda),
\end{align*}
where $\epsilon$ is a random variable that does \textbf{not} depend on the
variational parameters $\lambda$. The deterministic function
$z(\cdot;\lambda)$ encapsulates the variational parameters instead,
and following the process is equivalent to directly drawing $z$ from
the original distribution.

The reparameterization gradient estimator leverages this property of the
variational distribution to write the gradient as
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=\;
  \mathbb{E}_{q(\epsilon)}
  \big[
  \nabla_\lambda
  \big(
  \log p(x, z(\epsilon \;;\; \lambda))
  -
  \log q(z(\epsilon \;;\; \lambda) \;;\;\lambda)
  \big)
  \big].
\end{align*}
The gradient of the ELBO is an expectation over the base
distribution $q(\epsilon)$, and the gradient can be applied directly
to the inner expression.
Edward uses automatic differentiation, specifically with TensorFlow's
computational graphs, making this gradient computation both simple and
efficient to distribute.

\subsubsection{Noisy estimates using Monte Carlo integration}

We can use Monte Carlo integration to obtain noisy estimates of both the ELBO
and its gradient. The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{\epsilon_s\}_1^S \sim q(\epsilon)$,
  \item evaluate the argument of the expectation using $\{\epsilon_s\}_1^S$, and
  \item compute the empirical mean of the evaluated quantities.
\end{enumerate}

A Monte Carlo estimate of the gradient is then
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &\approx\;
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \nabla_\lambda
  \big(
  \log p(x, z(\epsilon \;;\; \lambda))
  -
  \log q(z(\epsilon \;;\; \lambda) \;;\;\lambda)
  \big)
  \big].
\end{align*}
This is an unbiased estimate of the actual gradient of the ELBO. Empirically, it
exhibits lower variance than the
\href{tut_KLqp_score.html}{score function gradient}, leading to
faster convergence in a large set of problems.

\subsubsection{Implementation}

We implement the ELBO with the reparameterization gradient in the method
\texttt{build_reparam_loss} of the \texttt{MFVI} (mean-field variational
inference) class.
\begin{lstlisting}[language=Python]
class MFVI(VariationalInference):
    ...
    def build_reparam_loss(self):
        x = self.data
        z = self.variational.sample(self.n_samples)

        self.loss = tf.reduce_mean(self.model.log_prob(x, z) -
                                   self.variational.log_prob(z))
        return -self.loss
\end{lstlisting}

This method draws $S$ (\texttt{self.n_samples}) samples from the variational
distribution. The \texttt{sample} method follows the reparameterization
procedure. The TensorFlow function \texttt{reduce_mean} then
computes the Monte Carlo summation.

The method stores computation of the ELBO in \texttt{self.loss}, which
can be used to track progress of the inference for diagnostics.  The
method returns an object whose automatic differentiation is the
reparameterization gradient of the ELBO. The computational graph will
simply traverse the nodes during backpropagation; unlike the
\href{tut_KLqp_score.html}{score function gradient}, the
reparameterization gradient does not need TensorFlow's
\texttt{stop_gradient} function.

There is a nuance here. TensorFlow's optimizers are configured to
\emph{minimize} an objective function, so the gradient is set to be
the negative of the ELBO's gradient.

\subsubsection{Stochastic gradient optimization}

Stochastic gradient descent is an extension of gradient descent using
noisy estimates of the gradient. Under mild conditions, stochastic
gradient descent finds a (local) optimum of the original (noiseless)
function.

Edward uses TensorFlow's optimizers.
This is specified in the \texttt{initialize} method of the
\texttt{VariationalInference} base class.

\begin{lstlisting}[language=Python]
class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use Adam with a decaying scale factor
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)
\end{lstlisting}

This sets up TensorFlow to optimize the ELBO using ADAM's learning rate
combined with an exponentially decaying scale factor.

See the \href{api/index.html}{API} for more details.
