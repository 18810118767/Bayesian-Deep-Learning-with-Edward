% Define the subtitle of the page
\title{Reparameterization method}

% Begin the content of the page
\subsection{Reparameterization method}

(This tutorial follows the
\href{tut_KLqp.html}{$\text{KL}(q\|p)$ minimization} tutorial.)

We seek to maximize the ELBO,
\begin{align*}
  \lambda^*
  &=
  \arg \max_\lambda \text{ELBO}(\lambda)\\
  &=\;
  \arg \max_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
using a ``black box'' algorithm. This means avoiding additional
mathematical derivations, such as calculating integrals (expectations) or
gradients (for maximization) by hand.

\subsubsection{The reparameterization gradient estimator}

Gradient ascent is one way to maximize complicated functions like the ELBO. The
idea is to compute the function's gradient
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=
  \nabla_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
and to follow it to a (local) optimum.

Some variational distributions $q(z\;;\;\lambda)$ admit useful
reparameterizations. For example, we can reparameterize the normal distribution 
$z \sim \mathcal{N}(\mu, \Sigma)$ as 
$z \sim \mu + L \mathcal{N}(0, I)$ where $\Sigma = LL^\top$. In general, write
this as
\begin{align*}
  \epsilon &\sim q(\epsilon)\\
  z &= g(\epsilon \;;\; \lambda),
\end{align*}
where $\epsilon$ is a random variable that does \textbf{not} depend on the
variational parameters $\lambda$. The deterministic function $g$ encapsulates
the variational parameters instead.

The reparameterization gradient estimator leverages this property of the
variational approximation to write the gradient as
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=\; 
  \mathbb{E}_{q(\epsilon)}
  \big[
  \nabla_\lambda
  \big(
  \log p(x, g(\epsilon \;;\; \lambda))
  -
  \log q(g(\epsilon \;;\; \lambda) \;;\;\lambda)
  \big)
  \big].
\end{align*}
This construes the gradient of the ELBO as an expectation over the base
distribution $q(\epsilon)$. This allows the gradient to move inside the
expectati on; however, now the gradient applies to both the model and the
variational approximation. We need to run automatic differentation on both.

\subsubsection{Noisy estimates using Monte Carlo integration}

We can use Monte Carlo integration to obtain noisy estimates of both the ELBO
and its gradient. The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{\epsilon_s\}_1^S \sim q(\epsilon)$,
  \item evaluate the argument of the expectation using $\{\epsilon_s\}_1^S$, and
  \item compute the empirical mean of the evaluated quantities.
\end{enumerate}

A Monte Carlo estimate of the gradient is then
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &\approx\; 
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \nabla_\lambda
  \big(
  \log p(x, g(\epsilon \;;\; \lambda))
  -
  \log q(g(\epsilon \;;\; \lambda) \;;\;\lambda)
  \big)
  \big].
\end{align*}
This is an unbiased estimate of the actual gradient of the ELBO. Empirically, it
exhibits lower variance than the 
\href{tut_KLqp_score.html}{score function method}.

\subsubsection{Implementation}

We implement the ELBO and its reparameterization gradient estimator in a single
method \texttt{build_reparam_loss} of the \texttt{MFVI} (mean-field variational
inference) class.
\begin{lstlisting}[language=Python]
class MFVI(VariationalInference):
    ...
    def build_reparam_loss(self):
        x = self.data
        z = self.variational.sample(self.n_minibatch)

        self.loss = tf.reduce_mean(self.model.log_prob(x, z) -
                                   self.variational.log_prob(z))
        return -self.loss
\end{lstlisting}

This method draws $S$ (\texttt{self.n_minibatch}) samples from the variational
distribution. The \texttt{sample} function implements the reparameterization
$g(\epsilon\;;\;\lambda)$. The TensorFlow method \texttt{reduce_mean} then
computes the Monte Carlo summation. 

This method implicitly specifies the gradient of the ELBO as well. TensorFlow's
automatic differentation will apply to \texttt{z}, thus applying the chain rule
of differentation to compute the gradient of the ELBO. Unlike the
\href{tut_KLqp_score.html}{score function method},
we do not have to use TensorFlow's \texttt{stop_gradient} method.

There is a nuance here. This method actually returns the \textbf{negative} ELBO
because TensorFlow's stochastic optimization routines are all configured to
\emph {minimize} an objective function.

\subsubsection{Stochastic gradient optimization}

Stochastic gradient ascent is an extension of gradient ascent that permits
following noisy estimates of the gradient. Under mild conditions, stochastic
gradient ascent finds a (local) optimum of the original (noiseless) function.

Edward uses TensorFlow's built-in stochastic gradient minimization routines.
This is specified in the \texttt{initialize} method of the 
\texttt{VariationalInference} base class.

\begin{lstlisting}[language=Python]
class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use Adam with a decaying scale factor
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)
\end{lstlisting}

This sets up TensorFlow to minimize the ELBO using the Adam adaptive step-size
schedule with an exponentially decaying scaling factor.

See the \href{api/index.html}{API} for further implementation details.
