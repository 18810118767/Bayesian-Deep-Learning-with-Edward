% Define the subtitle of the page
\title{Score function method}

% Begin the content of the page
\subsection{Score function method}

(This tutorial follows the
\href{tut_KLqp.html}{$\text{KL}(q\|p)$ minimization} tutorial.)

We seek to maximize the ELBO,
\begin{align*}
  \lambda^*
  &=
  \arg \max_\lambda \text{ELBO}(\lambda)\\
  &=\;
  \arg \max_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
using a ``black box'' algorithm. This means avoiding additional
mathematical derivations, such as calculating integrals (expectations) or
gradients (for maximization) by hand.

\subsubsection{The score function gradient estimator}

Gradient ascent is one way to maximize complicated functions like the ELBO. The
idea is to compute the function's gradient
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=
  \nabla_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
and to follow it to a (local) optimum.

The score function gradient estimator leverages a nice property of logarithm to
write the gradient as
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=\; 
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \big(
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(z\;;\;\lambda)
  \big].
\end{align*}
This construes the gradient of the ELBO as an expectation over the variational
approximation $q(z\;;\;\lambda)$; the only new ingredient it requires is the 
\emph{score function} $\nabla_\lambda \log q(z\;;\;\lambda)$, which is easy
using automatic differentiation.

\subsubsection{Noisy estimates using Monte Carlo integration}

We can use Monte Carlo integration to obtain noisy estimates of both the ELBO
and its gradient. The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{z_s\}_1^S \sim q(z\;;\;\lambda)$,
  \item evaluate the argument of the expectation using $\{z_s\}_1^S$, and
  \item compute the empirical mean of the evaluated quantities.
\end{enumerate}

A Monte Carlo estimate of the gradient is then
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &\approx\; 
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \big(
  \log p(x, z_s)
  -
  \log q(z_s\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(z_s\;;\;\lambda)
  \big].
\end{align*}
This is an unbiased estimate of the actual gradient of the ELBO. 

\subsubsection{Stochastic gradient optimization}

Stochastic gradient ascent is an extension of gradient ascent that permits
following noisy estimates of the gradient. Under mild conditions, stochastic
gradient ascent finds a (local) optimum of the original (noiseless) function.
