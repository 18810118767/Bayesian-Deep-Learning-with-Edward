% Define the subtitle of the page
\title{Score function method}

% Begin the content of the page
\subsection{Score function method}

(This tutorial follows the
\href{tut_KLqp.html}{$\text{KL}(q\|p)$ minimization} tutorial.)

We seek to maximize the ELBO,
\begin{align*}
  \lambda^*
  &=
  \arg \max_\lambda \text{ELBO}(\lambda)\\
  &=\;
  \arg \max_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
using a ``black box'' algorithm. This means avoiding additional
mathematical derivations, such as calculating integrals (expectations) or
gradients (for maximization) by hand.

\subsubsection{The score function gradient estimator}

Gradient ascent is one way to maximize complicated functions like the ELBO. The
idea is to compute the function's gradient
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=
  \nabla_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
and to follow it to a (local) optimum.

The score function gradient estimator leverages a nice property of logarithm to
write the gradient as
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=\; 
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \big(
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(z\;;\;\lambda)
  \big].
\end{align*}
This construes the gradient of the ELBO as an expectation over the variational
approximation $q(z\;;\;\lambda)$; the only new ingredient it requires is the 
\emph{score function} $\nabla_\lambda \log q(z\;;\;\lambda)$, which is easy
using automatic differentiation.

\subsubsection{Noisy estimates using Monte Carlo integration}

We can use Monte Carlo integration to obtain noisy estimates of both the ELBO
and its gradient. The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{z_s\}_1^S \sim q(z\;;\;\lambda)$,
  \item evaluate the argument of the expectation using $\{z_s\}_1^S$, and
  \item compute the empirical mean of the evaluated quantities.
\end{enumerate}

A Monte Carlo estimate of the gradient is then
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &\approx\; 
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \big(
  \log p(x, z_s)
  -
  \log q(z_s\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(z_s\;;\;\lambda)
  \big].
\end{align*}
This is an unbiased estimate of the actual gradient of the ELBO. 

\subsubsection{Implementation}

We implement the ELBO and its score function gradient estimator in a single
method \texttt{build_score_loss} of the \texttt{MFVI} (mean-field variational
inference) class.
\begin{lstlisting}[language=Python]
class MFVI(VariationalInference):
    ...
    def build_score_loss(self):
        x = self.data
        z = self.variational.sample(self.n_minibatch)

        q_log_prob = self.variational.log_prob(stop_gradient(z))
        losses = self.model.log_prob(x, z) - q_log_prob
        return -tf.reduce_mean(q_log_prob * stop_gradient(losses))
\end{lstlisting}

This method draws $S$ (\texttt{self.n_minibatch}) samples from the variational
distribution; the TensorFlow method \texttt{reduce_mean} computes the Monte
Carlo summation. 

The TensorFlow method \texttt{stop_gradient} implicitly specifies the gradient
of the ELBO; it tells TensorFlow where to stop automatic differentiation. In
this case, it will only compute $\nabla_\lambda \log q(z_s\;;\;\lambda)$ because
it is not wrapped in a \texttt{stop_gradient}.

There is a nuance here. This method actually returns the \textbf{negative} ELBO
because TensorFlow's stochastic optimization routines are all configured to
\emph {minimize} an objective function.

\subsubsection{Stochastic gradient optimization}

Stochastic gradient ascent is an extension of gradient ascent that permits
following noisy estimates of the gradient. Under mild conditions, stochastic
gradient ascent finds a (local) optimum of the original (noiseless) function.

Edward uses TensorFlow's built-in stochastic gradient minimization routines.
This is specified in the \texttt{initialize} method of the 
\texttt{VariationalInference} base class.

\begin{lstlisting}[language=Python]
class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use Adam with a decaying scale factor
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)
\end{lstlisting}

This sets up TensorFlow to minimize the ELBO using the Adam adaptive step-size
schedule with an exponentially decaying scaling factor.

See the \href{api/index.html}{API} for further implementation details.
