% Define the subtitle of the page
\title{Score function gradient}

% Begin the content of the page
\subsection{Score function gradient}

(This tutorial follows the
\href{tut_KLqp.html}{$\text{KL}(q\|p)$ minimization} tutorial.)

We seek to maximize the ELBO,
\begin{align*}
  \lambda^*
  &=
  \arg \max_\lambda \text{ELBO}(\lambda)\\
  &=\;
  \arg \max_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
using a ``black box'' algorithm. This means generically inferring the
posterior while making few assumptions about the model.

\subsubsection{The score function identity}

Gradient descent is a standard approach for optimizing complicated
objectives like the ELBO. The idea is to calculate its gradient
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=
  \nabla_\lambda\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big],
\end{align*}
and update the current set of parameters proportional to the gradient.

The score function gradient estimator leverages a property of
logarithms to write the gradient as
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &=\;
  \mathbb{E}_{q(z\;;\;\lambda)}
  \big[
  \nabla_\lambda \log q(z\;;\;\lambda)
  \:
  \big(
  \log p(x, z)
  -
  \log q(z\;;\;\lambda)
  \big)
  \big].
\end{align*}
The gradient of the ELBO is an expectation over the variational
model $q(z\;;\;\lambda)$; the only new ingredient it requires is the
\emph{score function} $\nabla_\lambda \log q(z\;;\;\lambda)$.
Edward uses automatic differentiation, specifically with TensorFlow's
computational graphs, making this gradient computation both simple and
efficient to distribute.

\subsubsection{Noisy estimates using Monte Carlo integration}

We can use Monte Carlo integration to obtain noisy estimates of both the ELBO
and its gradient. The basic procedure follows these steps:
\begin{enumerate}
  \item draw $S$ samples $\{z_s\}_1^S \sim q(z\;;\;\lambda)$,
  \item evaluate the argument of the expectation using $\{z_s\}_1^S$, and
  \item compute the empirical mean of the evaluated quantities.
\end{enumerate}

A Monte Carlo estimate of the gradient is then
\begin{align*}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &\approx\;
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \big(
  \log p(x, z_s)
  -
  \log q(z_s\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(z_s\;;\;\lambda)
  \big].
\end{align*}
This is an unbiased estimate of the actual gradient of the ELBO.

\subsubsection{Implementation}

We implement the ELBO with the score function gradient in the method
\texttt{build_score_loss} of the \texttt{MFVI} (mean-field variational
inference) class.
\begin{lstlisting}[language=Python]
class MFVI(VariationalInference):
    ...
    def build_score_loss(self):
        x = self.data
        z = self.variational.sample(self.n_samples)

        q_log_prob = self.variational.log_prob(stop_gradient(z))
        losses = self.model.log_prob(x, z) - q_log_prob
        self.loss = tf.reduce_mean(losses)
        return -tf.reduce_mean(q_log_prob * stop_gradient(losses))
\end{lstlisting}

This method draws $S$ (\texttt{self.n_samples}) samples from the variational
model. The TensorFlow function \texttt{reduce_mean} takes the
mean over the samples.

The method stores computation of the ELBO in \texttt{self.loss}, which
can be used to track progress of the inference for diagnostics.
The method returns an object whose automatic differentiation is the
score function gradient of the ELBO.  The TensorFlow function
\texttt{stop_gradient} tells the computational graph to stop
traversing nodes to propagate gradients. In this case,
the only gradients taken are $\nabla_\lambda \log q(z_s\;;\;\lambda)$,
because it is not wrapped in a \texttt{stop_gradient}.

There is a nuance here. TensorFlow's optimizers are configured to
\emph{minimize} an objective function, so the gradient is set to be
the negative of the ELBO's gradient.

\subsubsection{Stochastic optimization}

Stochastic gradient descent is an extension of gradient descent using
noisy estimates of the gradient. Under mild conditions, stochastic
gradient descent finds a (local) optimum of the original (noiseless)
function.

Edward uses TensorFlow's optimizers.
This is specified in the \texttt{initialize} method of the
\texttt{VariationalInference} base class.

\begin{lstlisting}[language=Python]
class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use ADAM with a decaying scale factor.
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)
\end{lstlisting}

This sets up TensorFlow to optimize the ELBO using ADAM's learning
rate combined with an exponentially decaying scale factor.

See the \href{api/index.html}{API} for more details.
