<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Adaptive importance sampling</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
    
  <!-- CSS -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="icons/manifest.json">
  <link rel="mask-icon" href="icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="icons/mstile-144x144.png">
  <meta name="msapplication-config" content="icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="index.html">Edward</a></h1> 
    <a href="index.html">
    <img src="images/edward.png" class="u-full-width" style="margin-bottom:15%" alt="Edward" />
    </a>
    <a class="button u-full-width" href="index.html">Home</a>
    <a class="button u-full-width" href="getting-started.html">Getting Started</a> 
    <a class="button u-full-width" href="delving-in.html">Delving In</a> 
    <a class="button u-full-width" href="tutorials.html">Tutorials</a> 
    <a class="button u-full-width" href="api/index.html">API</a> 
    <a class="button u-full-width" href="#">Advanced</a> 
    <a class="button2 u-full-width" href="design-philosophy.html">Design Philosophy</a> 
    <a class="button2 u-full-width" href="developer-process.html">Developer Process</a> 
    <a class="button2 u-full-width" href="troubleshooting.html">Troubleshooting</a> 
    <a class="button2 u-full-width" href="license.html">License</a> 
    <div class="row" style="padding-bottom: 5%"> </div>
    <a href="https://github.com/blei-lab/edward">
    <!-- <object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="images/github-mark.svg" class="u-pull-right" style="padding-right:10%"
    alt="Edward on Github" />
    <!-- </object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">
<h2 id="adaptive-importance-sampling">Adaptive importance sampling</h2>
<p>(This tutorial follows the <a href="tut_KLpq.html"><span class="math inline">\(\text{KL}(p\|q)\)</span> minimization</a> tutorial.)</p>
<p>We seek to minimize <span class="math inline">\(\text{KL}(p\|q)\)</span> by following its gradient <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &amp;=
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big].\end{aligned}\]</span> using a “black box” algorithm. This means generically inferring the posterior while making few assumptions about the model.</p>
<h3 id="importance-sampling">Importance sampling</h3>
<p>The gradient is an expectation with respect to the unknown posterior. Combining Bayes’ rule with importance sampling can circumvent this obstacle.</p>
<p>First rewrite the expectation to be with respect to the variational distribution, <span class="math display">\[\begin{aligned}
  -
  \mathbb{E}_{p(z \mid x)}
  \big[
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \big]
  &amp;=
  -
  \mathbb{E}_{q(z\;;\;\lambda)}
  \Bigg[
  \frac{p(z \mid x)}{q(z\;;\;\lambda)}
  \nabla_\lambda\;
  \log q(z\;;\;\lambda)
  \Bigg].\end{aligned}\]</span></p>
<p>We then use importance sampling to obtain a noisy estimate of this gradient. The basic procedure follows these steps:</p>
<ol>
<li><p>draw <span class="math inline">\(S\)</span> samples <span class="math inline">\(\{z_s\}_1^S \sim q(z\;;\;\lambda)\)</span>,</p></li>
<li><p>evaluate <span class="math inline">\(\nabla_\lambda\; \log q(z_s\;;\;\lambda)\)</span>,</p></li>
<li><p>compute the normalized importance weights <span class="math display">\[\begin{aligned}
    w_s
    &amp;=
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
    \Bigg/
    \sum_{s=1}^{S}
    \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
  \end{aligned}\]</span></p></li>
<li><p>compute the weighted mean.</p></li>
</ol>
<p>The key insight is that we can use the joint <span class="math inline">\(p(x,z)\)</span> instead of the posterior when estimating the normalized importance weights <span class="math display">\[\begin{aligned}
  w_s
  &amp;=
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(z_s \mid x)}{q(z_s\;;\;\lambda)} \\
  &amp;=
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(x, z_s)}{q(z_s\;;\;\lambda)}.\end{aligned}\]</span> This follows from Bayes’ rule <span class="math display">\[\begin{aligned}
  p(z \mid x)
  &amp;=
  p(x, z) / p(x)\\
  &amp;=
  p(x, z) / \text{a constant function of }z.\end{aligned}\]</span></p>
<p>Importance sampling thus gives the following noisy yet unbiased gradient estimate <span class="math display">\[\begin{aligned}
\nabla_\lambda\;
  \text{KL}(
  p(z \mid x)
  \;\|\;
  q(z\;;\;\lambda)
  )
  &amp;=
  -
  \frac{1}{S}
  \sum_{s=1}^S
  w_s
  \nabla_\lambda\; \log q(z_s\;;\;\lambda).\end{aligned}\]</span> The objective <span class="math inline">\(\text{KL}(p\|q)\)</span> can be calculated in a similar fashion. The only new ingredient for its gradient is the score function <span class="math inline">\(\nabla_\lambda \log q(z\;;\;\lambda)\)</span>. Edward uses automatic differentiation, specifically with TensorFlow’s computational graphs, making this gradient computation both simple and efficient to distribute.</p>
<p>Adaptive importance sampling follows this gradient to a local optimum using stochastic optimization. It is adaptive because the variational distribution <span class="math inline">\(q(z\;;\;\lambda)\)</span> iteratively gets closer to the posterior <span class="math inline">\(p(z \mid x)\)</span>.</p>
<h3 id="implementation">Implementation</h3>
<p>We implement <span class="math inline">\(\text{KL}(p\|q)\)</span> with its importance sampling gradient in the method <code>build_loss</code> of the <code>KLpq</code> class.</p>
<pre class="python" language="Python"><code>class KLpq(VariationalInference):
    ...
    def build_loss(self):
        x = self.data
        z = self.variational.sample(self.n_samples)

        # normalized importance weights
        q_log_prob = self.variational.log_prob(stop_gradient(z))
        log_w = self.model.log_prob(x, z) - q_log_prob
        log_w_norm = log_w - log_sum_exp(log_w)
        w_norm = tf.exp(log_w_norm)

        self.loss = tf.reduce_mean(w_norm * log_w)
        return -tf.reduce_mean(q_log_prob * tf.stop_gradient(w_norm))</code></pre>
<p>This method draws <span class="math inline">\(S\)</span> (<code>self.n_samples</code>) samples from the variational model. The TensorFlow function <code>reduce_mean</code> takes the mean over the samples.</p>
<p>The method stores computation of <span class="math inline">\(\text{KL}(p\|q)\)</span> in <code>self.loss</code>, which can be used to track progress of the inference for diagnostics. The method returns an object whose automatic differentiation is a stochastic gradient of <span class="math inline">\(\text{KL}(p\|q)\)</span>. The TensorFlow function <code>stop_gradient</code> tells the computational graph to stop traversing nodes to propagate gradients. In this case, the only gradients taken are <span class="math inline">\(\nabla_\lambda \log q(z_s\;;\;\lambda)\)</span>, because it is not wrapped in a <code>stop_gradient</code>.</p>
<p>Computing the normalized importance weights is a numerically challenging task. Underflow is a common issue. Here we use the <code>log_sum_exp</code> trick to compute weights in the log space. However, the normalized weights are still exponentiated to compute the gradient. As with importance sampling in general, this inference method does not scale to high dimensions.</p>
<h3 id="stochastic-gradient-optimization">Stochastic gradient optimization</h3>
<p>Stochastic gradient descent is an extension of gradient descent using noisy estimates of the gradient. Under mild conditions, stochastic gradient descent finds a (local) optimum of the original (noiseless) function.</p>
<p>Edward uses TensorFlow’s optimizers. This is specified in the <code>initialize</code> method of the <code>VariationalInference</code> base class.</p>
<pre class="python" language="Python"><code>class VariationalInference(Inference):
    ...
    def initialize(self, ...):
        ...
        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                     scope=scope)

        # Use Adam with a decaying scale factor
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                            global_step,
                                            100, 0.9, staircase=True)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        self.train = optimizer.minimize(loss, global_step=global_step,
                                        var_list=var_list)</code></pre>
<p>This sets up TensorFlow to optimize <span class="math inline">\(\text{KL}(p\|q)\)</span> using ADAM’s learning rate combined with an exponentially decaying scale factor.</p>
<p>See the <a href="api/index.html">API</a> for more details.</p>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script> 
</body>
</html>
