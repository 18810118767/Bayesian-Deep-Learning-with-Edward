
<!DOCTYPE html>
<html>
<head>
  
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>Getting Started &mdash; Edward</title>

    
      <link rel="shortcut icon" href="../img/favicon.ico">
    

    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <link rel="stylesheet" href="../css/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../css/alabaster-overrides.css" type="text/css" />

    

    

    <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

    
  
</head>
<body role="document">

  <div class="document">
    <div class="documentwrapper">
      <div class="bodywrapper">
        <div class="body" role="main">
          
            <h1 id="getting-started">Getting Started</h1>
<p><strong>Edward</strong> is named after the innovative statistician
<a href="https://en.wikipedia.org/wiki/George_E._P._Box">George Edward Pelham Box</a>.
Edward follows Box's philosophy of statistics and machine learning.</p>
<p>First gather data from a real-world process. Then cycle through Box's loop:</p>
<ol>
<li>Build a probabilistic model of the process</li>
<li>Reason about the process given model and data</li>
<li>Criticize the model, revise and repeat</li>
</ol>
<p>Here's a toy example. A child flips a coin ten times, with the data of outcomes being <code>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]</code>. We are interested in the probability that the coin lands heads. First, build a model: assume the coin flips are independent and land heads with the same probability. Second, reason about the process: use an algorithm to infer the model given data. Third, criticize the model: analyze whether the model captures the real-world process of coin flips. If it doesn't, then revise the model and repeat.</p>
<p>This process defines the design of <strong>Edward</strong>. Here are the four primary objects that enable the above analysis. (More <code>edward</code> syntax follows in a complete example.)</p>
<h3 id="data">Data</h3>
<p><code>Data</code> objects are containers that contain measurements. The structure of these objects must match the inputs of the probabilistic model.</p>
<pre><code class="Python">data = ed.Data(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))
</code></pre>

<h3 id="models">Models</h3>
<p>There are two types of model objects in Edward:</p>
<ol>
<li>Probability models of data</li>
<li>Variational models of latent variables</li>
</ol>
<p>We can specify probability models of data using NumPy/SciPy, TensorFlow, PyMC3, or Stan. Here is a model of coin flips using a <a href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">Beta-Bernoulli distribution</a> in NumPy/Scipy.</p>
<pre><code class="Python">class BetaBernoulli(PythonModel):
    &quot;&quot;&quot;p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)
    &quot;&quot;&quot;
    def _py_log_prob(self, xs, zs):
        n_samples = zs.shape[0]
        lp = np.zeros(n_samples, dtype=np.float32)
        for s in range(n_samples):
            lp[s] = beta.logpdf(zs[s, :], a=1.0, b=1.0)
            for n in range(len(xs)):
                lp[s] += bernoulli.logpmf(xs[n], p=zs[s, :])
        return lp
</code></pre>

<p>This describes a Bayesian model, which is a joint distribution of data and latent variables <code>z</code>. With this model and data of coin flips, we aim to reason about <code>z</code>, the probability that the coin lands heads. The posterior distribution of <code>z</code> captures our reasoning: its mean describes our best guess of the probability, and its variance describes our uncertainty around our best guess. In this toy model, we know that the posterior is a Beta distribution. Let us assume we do not know its parameters in closed form.</p>
<p>Edward can employ variational inference to infer this posterior, which finds the closest distribution within a specified family. Initialize an empty  <code>Variational()</code> model. Then add a Beta distribution to the variational model.</p>
<pre><code class="Python">variational = Variational()
variational.add(Beta())
</code></pre>

<p>With this syntax, we can build rich variational models to describe the latent variables in our data models. (More documentation on this coming soon.)</p>
<h3 id="inference">Inference</h3>
<p><code>Inference</code> objects infer latent variables of models given data. Edward currently supports a variety of variational inference algorithms. These take as input a probability model, a variational model, and data.</p>
<p>Here we use mean-field variational inference.</p>
<pre><code>inference = ed.MFVI(model, variational, data)
</code></pre>

<p>(For the technical audience, the mean-field assumption of a fully factorized approximation is moot here. We're dealing with a one-dimensional latent variable.)</p>
<p>Calling <code>inference.run</code> runs the inference algorithm until convergence. It recovers a Beta distribution with mean 0.25 and variance 0.12. 0.25 is our best guess of the probability that the coin lands heads.</p>
<h3 id="criticism">Criticism</h3>
<p>[In Progress]</p>
<h2 id="a-complete-example-the-beta-bernoulli-model">A complete example: the Beta-Bernoulli model</h2>
<p>Here is a complete script, defining the data, model, and the variational model. We run mean-field variational inference for <code>10000</code> iterations at the end. The same example is also available in cases where the model is written in <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_stan.py">Stan</a>, <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_pymc3.py">PyMC3</a> and <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_tf.py">TensorFlow</a> respectively.</p>
<pre><code class="Python">&quot;&quot;&quot;A simple coin flipping example. The model is written in NumPy/SciPy.

Probability model
    Prior: Beta
    Likelihood: Bernoulli
Variational model
    Likelihood: Mean-field Beta
&quot;&quot;&quot;
import edward as ed
import numpy as np

from edward.models import PythonModel, Variational, Beta
from scipy.stats import beta, bernoulli

class BetaBernoulli(PythonModel):
    &quot;&quot;&quot;p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)
    &quot;&quot;&quot;
    def _py_log_prob(self, xs, zs):
        # This example is pedagogical.
        # We recommend vectorizing operations in practice.
        n_minibatch = zs.shape[0]
        lp = np.zeros(n_minibatch, dtype=np.float32)
        for s in range(n_minibatch):
            lp[s] = beta.logpdf(zs[s, :], a=1.0, b=1.0)
            for n in range(len(xs)):
                lp[s] += bernoulli.logpmf(xs[n], p=zs[s, :])
        return lp

data = ed.Data(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))
model = BetaBernoulli()
variational = Variational()
variational.add(Beta())
inference = ed.MFVI(model, variational, data)

inference.run(n_iter=10000)
</code></pre>

<h2 id="more-links">More Links</h2>
<p>You can find more complicated examples in the <a href="https://github.com/blei-lab/edward/tree/master/examples"><code>examples/</code></a> directory. We highlight several here:</p>
<ul>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/bayesian_linear_regression.py">Bayesian linear regression</a></li>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/hierarchical_logistic_regression.py">Hierarchical logistic regression</a></li>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian.py">Mixture model of Gaussians</a></li>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/gp_classification.py">Gaussian process classification</a></li>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/bayesian_nn.py">Bayesian neural network</a></li>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/mixture_density_network.py">Mixture density network</a></li>
<li><a href="https://github.com/blei-lab/edward/blob/master/examples/convolutional_vae.py">Variational auto-encoder</a></li>
</ul>
<p>We think the library will make it significantly easier to do research in machine learning and statistics. You can find more about this <a href="../guide-research/">here</a>.</p>
<p>You can find more about Edward's design and philosophy, and how it relates to other software <a href="../design/">here</a>.</p>
<h2 id="references">References</h2>
<ul>
<li>David M Blei. Build, compute, critique, repeat: data analysis with latent variable models. Annual Review of Statistics and Its Application, 1:203-232, 2014.</li>
</ul>
          
        </div>
      </div>
    </div>
    <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
      <div class="sphinxsidebarwrapper">
        
  <p class="logo">
    <a href="..">
      <img class="logo" src="../edward.png" title="Edward">
    </a>
  </p>
  
    <h1 class="logo">Edward</h1>
  



  <p class="blurb">A library for probabilistic modeling, inference, and criticism.
      <a href="http://github.com/blei-lab/edward"><i class="fa fa-github fa-lg"
      aria-hidden="true"></i></a>
  </p>

        <hr />


<ul>
  
      
        <li class="toctree-l1">
          <a href="..">Home</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="./">Getting Started</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../guide-research/">Guide for Research</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../design/">Design Philosophy</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../developer/">Developer Process</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../tensorflow/">TensorFlow</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../wishlist/">Feature Wishlist</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../misc/">Miscellaneous</a>
        </li>
      
    
  </ul>
        <!--Commented out since we haven't autogenerated anything yet.-->
        <!--<div id="searchbox" style="display: none;" role="search">
  <h3>Quick search</h3>
  <form class="search" action="../search.html" method="get">
    <input name="q" type="text">
    <input value="Go" type="submit">
  </form>
  <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
  </p>
</div>
<script type="text/javascript">
  document.getElementById("searchbox").style.display = "block";
</script>-->
      </div>
    </div>
    <div class="clearer"></div>
  </div>

  
    <div class="footer">
      
      
    </div>
  

  <script src="../js/jquery-1.10.2.min.js"></script>
  <script src="../js/highlight.pack.js"></script>
  <script src="../js/base.js"></script>

  <!--
  MkDocs version      : 0.15.3
  Docs Build Date UTC : 2016-07-16 01:15:15.488625
  -->
</body>
</html>