\title{Data Subsampling}

{{navbar}}

\subsubsection{Data Subsampling}

Running algorithms which require the full data set for each update
can be expensive when the data is large. In order to scale inferences,
we perform batch training: update inference using
only a subsample of data at a time.
(Note that only certain algorithms can support data subsampling, such as
\texttt{KLqp} and \texttt{SGLD}.)

% In the typical setting for stochastic optimization, we
% described how to feed batches of data at a time.  In Bayesian
% inference we have to deal with the additional problem of local versus
% global latent variables.
% In addition, with our modeling API, the modeler never explicitly
% writes down plates, so we can't tell which subset of the local latent
% variables are relevant to any batch (TODO although we could prbably do
% it internally, with batch\_shape/event\_shape; this also comes up in
% the bijector class).

% To do this, we use manual control of latent variable updates
% during inference.

There are two settings of data subsampling: working with the full model
(when the data and model fit in memory) and working with a subgraph of
the model (when the data and model do not fit in memory).
We illustrate these two settings in Edward.

\subsubsection{Full graphs}

In the full graph setting, we do data subsampling while working with the
full model. This setting is recommended when the data and model fit in
memory. It is scalable in that the algorithm's computational complexity is
independent of the data set size.

For example, suppose we are in the following setting.
\begin{lstlisting}[language=Python]
N = 10000000 # data set size
M = 5 # mini-batch size
\end{lstlisting}
We define a hierarchical model,
% TODO tikz pictures for all of these
\begin{equation*}
p(\mathbf{y}, \mathbf{z}, \beta)
= p(\beta) \prod_{n=1}^N p(z_n \mid \beta) p(y_n \mid z_n, \beta),
\end{equation*}
where there are latent variables $z_n$ for
each data point $y_n$ (local variables) and latent variables $\beta$
which are shared across data points (global variables).
\begin{lstlisting}[language=Python]
beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for n in range(N)]))
y = RandomVariable(par_1=z, par_2=tf.pack([beta for n in range(N)]))
\end{lstlisting}
For inference, we define the variational model,
\begin{equation*}
q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{n=1}^N q(z_n \mid \beta; \gamma_n),
\end{equation*}
parameterized by $\{\lambda, \{\gamma_n\}\}$.
\begin{lstlisting}[language=Python]
qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
qz = RandomVariable(par=f(tf.pack([qbeta for n in range(N)]),
                          tf.Variable(tf.random_normal(N)))
\end{lstlisting}
We instantiate the inference algorithm to perform inference over
$\beta$ and a subset of $\mathbf{z}$. The subset is determined by
a TensorFlow placeholder \texttt{idx_ph}, which we will change at each
step of the algorithm.
We also pass in a TensorFlow placeholder \texttt{y_ph} for the data,
so we can change the data at each step. (Alternatively,
\href{/api/data}{batch tensors} can be used.)
\begin{lstlisting}[language=Python]
y_ph = tf.placeholder(tf.float32, [M])
idx_ph = tf.placeholder(tf.int32, [M])
inference = ed.KLqp({beta: qbeta, tf.gather(z, idx_ph): tf.gather(qz, idx_ph)},
                  data={y: y_ph})
\end{lstlisting}
We initialize the algorithm with the \texttt{scale} argument, so that
computation on \texttt{z} will be scaled appropriately.
This enables unbiased estimates for stochastic gradients.
\begin{lstlisting}[language=Python]
inference.initialize(scale={z: float(N) / M})
\end{lstlisting}
We now run the algorithm, assuming there is a \texttt{next_batch}
function which provides the next batch of data.
\begin{lstlisting}[language=Python]
for t in range(10000):
  y_batch = next_batch(size=M)
  local_idx = np.arange(t * M, (t + 1) * M) % N
  inference.update(feed_dict={y_ph: y_batch, idx_ph: local_idx})
\end{lstlisting}

\subsubsection{Subgraphs}

In the subgraph setting, we do data subsampling while working with a
subgraph of the full model. This setting is recommended when the data
and model do not fit in memory. It is scalable in that both the
algorithm's computational complexity and memory complexity are
independent of the data set size.

For example, suppose we are in the following setting.
\begin{lstlisting}[language=Python]
N = 10000000 # data set size
M = 5 # mini-batch size
\end{lstlisting}
The model is the same as before,
\begin{equation*}
p(\mathbf{y}, \mathbf{z}, \beta)
= p(\beta) \prod_{n=1}^N p(z_n \mid \beta) p(y_n \mid z_n, \beta).
\end{equation*}
To avoid memory issues, we work on only a subgraph of the model,
\begin{equation*}
p(\mathbf{y}, \mathbf{z}, \beta)
= p(\beta) \prod_{m=1}^M p(z_m \mid \beta) p(y_m \mid z_m, \beta)
\end{equation*}
\begin{lstlisting}[language=Python]
beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for m in range(M)]))
y = RandomVariable(par_1=z par_2=tf.pack([beta for m in range(M)]))

\end{lstlisting}
For inference, the variational model is the same as before,
\begin{equation*}
q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{n=1}^N q(z_n \mid \beta; \gamma_n),
\end{equation*}
parameterized by $\{\lambda, \{\gamma_n\}\}$.
To avoid memory issues, we work on only a subgraph of the model,
\begin{equation*}
q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{m=1}^M q(z_m \mid \beta; \gamma_m).
\end{equation*}
parameterized by $\{\lambda, \{\gamma_m\}\}$. Importantly, only $M$
parameters are stored in memory for $\{\gamma_m\}$ rather than $N$.
\begin{lstlisting}[language=Python]
qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
qz_variables = tf.Variable(tf.random_normal(M)
qz = RandomVariable(par=f(tf.pack([qbeta for m in range(M)]),
                          qz_variables))
\end{lstlisting}
We instantiate the inference algorithm to perform inference over
$\beta$ and the subset of $\mathbf{z}$.
Unlike the full model setting, we do not do any local indexing because
the subgraph is already local.
We also pass in a TensorFlow placeholder \texttt{y_ph} for the data,
so we can change the data at each step. (Alternatively,
\href{/api/data}{batch tensors} can be used.)
\begin{lstlisting}[language=Python]
y_ph = tf.placeholder(tf.float32, [M])
inference = ed.KLqp({beta: qbeta, z: qz}, data={y: y_ph})
\end{lstlisting}
We initialize the algorithm with the \texttt{scale} argument, so that
computation on \texttt{z} will be scaled appropriately.
This enables unbiased estimates for stochastic gradients.
\begin{lstlisting}[language=Python]
inference.initialize(scale={z: float(N) / M})
\end{lstlisting}
We now run the algorithm, assuming there is a \texttt{next_batch}
function which provides the next batch of data.
\begin{lstlisting}[language=Python]
for t in range(10000):
  y_batch = next_batch(size=M)
  inference.update(feed_dict={y_ph: y_batch})
  tf.initialize_variables(qz_variables)
\end{lstlisting}
After each iteration, we also reinitialize the parameters for
$q(\mathbf{z}\mid\beta)$; this is because we do inference on a new
set of local variational factors for each batch.
% If we care about the local parameters, we can save them. For
% example, if we want to do computation over multiple epochs, and we
% don't have enough memory to store all parameters, then we can write
% the parameters to disk (at the end of the loop) and re-read them.

\subsubsection{Advanced settings}

Another approach to reduce memory complexity is to use an inference
network. This can be applied using a global parameterization of
$q(\mathbf{z}, \beta)$. For more details, see the
\href{/tutorials/inference-networks}{inference networks tutorial}.

In streaming data, or online inference, the size of the data $N$
may be unknown, or conceptually the size of the data may be
infinite and at any time in which we query parameters from the online
algorithm, the outputted parameters are from having processed as many
data points up to that time.
The approach of Bayesian filtering
\citep{doucet2000on,broderick2013streaming} can be applied in Edward using
recursive posterior inferences; the approach of population posteriors
\citep{mcinerney2015population} is readily applicable from the subgraph
setting.

In other settings, working on a subgraph of the model does not
apply, such as in time series models when we want to
preserve dependencies across time steps in our variational model.
Approaches in the literature can be applied in Edward
\citep{binder1997space,johnson2014stochastic,foti2014stochastic}.

% Sometimes resetting parameters is not sufficient, such as when \texttt{qz} may
% have a different distributional form for each data point. In this
% case, we can create random variables on the fly using
% local and global inference instantiations.
% \begin{lstlisting}[language=Python]
% global_vi = ed.KLqp({beta: qbeta}, data={y: y_ph, z: qz})
% global_vi.initialize(scale={z: float(N) / M})
% for _ in range(10000):
%   y_batch = next_batch(size=M)
%   qz = RandomVariable(M, par=qbeta)
%   local_vi = ed.KLqp({z: qz}, data={y_ph: y_batch, beta: qbeta})
%   local_vi.initialize()
%   for _ in range(10):
%     local_vi.update()

%   global_vi.update(feed_dict={y_ph: y_batch})
% \end{lstlisting}

% \begin{lstlisting}[language=Python]
% global_vi = ed.KLqp({beta: qbeta}, data={y: y_ph, z: qz})
% global_vi.initialize(scale={z: float(N) / M})
% for _ in range(10000):
%   y_batch = next_batch(size=M)
%   qz = RandomVariable(M, par=qbeta)
%   local_vi = ed.KLqp({z: qz}, data={y: y_batch, beta: qbeta})
%   local_vi.initialize()
%   for _ in range(10):
%     local_vi.update()

%   global_vi.update(feed_dict={y_ph: y_batch})
% \end{lstlisting}
% \begin{itemize}
% \item
%   We define a global VI object and a local VI object. Each abstractly
%   defines inference over different distributions (one does $q(\beta)
%   \approx p(\beta \mid x)$; the other does $\prod q(z_n) \approx \prod
%   p(z_n \mid \beta, x))$. We create the global VI object outside the loop
%   and proceed to update it per iteration. We create the local VI
%   object inside the loop and update it only once.
% \item
%   This construct is very general. We can arbitrarily compose inference
%   algorithms to infer different conditional distributions within the
%   full posterior. For example, instead of using both VI for both
%   inferring $p(\beta \mid x)$ and $\prod p(z_n \mid \beta, x)$, we can do
%   MCMC to exactly infer only one of these.
% \item
%   The TensorFlow graph is ``unrolled'', in the sense that a new node is
%   being added at each step of inference.
% \end{itemize}

\subsubsection{References}\label{references}
