\title{Composing Inference}

{{navbar}}

\subsubsection{Composing Inference}

Core to this language around inference is composability.

For example, we can compose inference to develop hybrid algorithms
such as variational EM.
\begin{lstlisting}[language=Python]
qbeta = PointMass()
qz = RandomVariable()
inference_e = VI({z: qz}, data={x: np.array(), beta: qbeta})
inference_m = MAP({beta: qbeta}, data={x: np.array(), z: qz})

for _ in range(10000):
  inference_e.update()
  inference_m.update()
\end{lstlisting}
Other examples of hybrid algorithms are VI with Gibbs updates (e.g.,
Wang and Blei (2011)), pseudo-marginal MCMC methods, automatic
variational ABC, etc.

One can also do this without passing in the random variables
explicitly as part of the data, restricting the inner integral to be
approximated with one sample.
\begin{lstlisting}[language=Python]
qbeta = PointMass()
qz = RandomVariable()

beta_ph = tf.placeholder(tf.float32)
z_ph = tf.placeholder(tf.float32)
inference_e = VI({z: qz}, data={x: np.array(), beta: beta_ph})
inference_m = MAP({beta: qbeta}, data={x: np.array(), z: z_ph})

for _ in range(10000):
  inference_e.update(feed_dict={beta_ph: qbeta.sample()})
  inference_m.update(feed_dict={z_ph: qz.sample()})
\end{lstlisting}
Note this is equivalent for the E-step, as the $q(\beta)$ is a point mass
anyways. In general, passing the whole random varriable vs a sample
are different different: for example, in BBVI, the latter will only
add the prior and likelihood to the ELBO; the former will additionally
include the variational log-density to the ELBO;.

In general, composability enables fine-grained control of latent
variable updates. This enables us to do efficient (distributed!)
message passing.

Let's consider Gibbs sampling. Here, we define the complete
conditional where the latent variables are conditioned on the last
sample from the Gibbs sample.
\begin{lstlisting}[language=Python]
t = tf.Variable(0, trainable=False)
increment_t = t.assign_add(1)

inference_z1 = ed.ConjugateInference(
    {z1: qz1}, {x: x_train, z2: tf.gather(qz2, t)})
inference_z2 = ed.ConjugateInference(
    {z2: qz2}, {x: x_train, z1: tf.gather(qz1, t)})

for _ in range(10000):
  inference_z1.update()
  inference_z2.update()
  increment_t.eval()
\end{lstlisting}

{{autogenerated}}
