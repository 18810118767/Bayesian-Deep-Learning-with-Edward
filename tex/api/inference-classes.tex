\title{Classes of Inference}

{{navbar}}

\subsubsection{Classes of Inference}

...

\subsubsection{Variational Inference, Monte Carlo, MAP}

We define inference algorithms using classes and class inheritance,
where the base class is \texttt{Inference} above.

We take the extreme case of inference where there is always an
approximating family, and that inference is about matching two
distributions (specifically, two TensorFlow graphs).
We must explicitly pass in the random variables in the second
graph that infer the random variables in the original graph.
\begin{lstlisting}[language=Python]
qbeta = RandomVariable()
qz = RandomVariable()
inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: np.array()})
\end{lstlisting}
Inference trains the factors binded to each latent
variable. Concretely this means optimizing over TensorFlow variables
in both the variational family and the model.

This is also true for methods not typically thought of with an
approximating family such as Monte Carlo and MAP. For Monte Carlo, we
define an empirical distribution whose parameters are a specified
number of samples. For MAP, we define a point mass distribution whose
parameters is a single point.
\begin{lstlisting}[language=Python]
qbeta = Empirical()
qz = Empirical()
inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: np.array()})

qbeta = PointMass()
qz = PointMass()
inference = ed.MAP({beta: qbeta, z: qz}, data={x: np.array()})
\end{lstlisting}

\subsubsection{Exact Inference}

In our general setup, we are simply matching two graphs subject to any
constraints. The constraints enable us to perform posterior inference.
Matching graphs without the constraints is a form of marginal
inference.

Let's consider inference to get marginal distributions.

Collapsing discrete random variables.
\begin{lstlisting}[language=Python]
z = Categorical(logits=tf.random_normal([K]))
x = Normal(mu=tf.gather(mus, z), sigma=tf.gather(sigmas, z))

cat = Categorical(logits=tf.Variable(tf.random_normal([K])))
components = [Normal(mu=tf.Variable(tf.random_normal([D])),
                     sigma=tf.Variable(tf.random_normal([D])))
              for _ in K]
qx = Mixture(cat=cat, components=components)

inference = CollapsedInference({x: qx})
inference.run()
\end{lstlisting}
TODO has to explicitly define distribution form of each component

Conjugate inference.
\begin{lstlisting}[language=Python]
z = Gamma(alpha=tf.constant([1]), beta=tf.constant([1./2]))
x = Exponential(lam=z)

qx = Gamma(alpha=tf.Variable(tf.random_normal([1])),
           beta=tf.Variable(tf.random_normal([1])))

inference = ConjugateInference({x: qx})
inference.run()
\end{lstlisting}

Symbolic inference.
\begin{lstlisting}[language=Python]
eps = Normal(mu=tf.constant([0.0]), sigma=tf.constant([1.0]))
x = mu + sigma * eps

qx = Normal(mu=tf.Variable(tf.random_normal([1])),
            sigma=tf.Variable(tf.random_normal([1])))

# using, e.g., Maple
inference = SymbolicInference({x: qx})
inference.run()
\end{lstlisting}
TODO has to explicitly define distribution form of posterior
functions of rvs

(we can also do deterministic approximations; not sure what's a better subsection to this)

Numerical integration. This includes for example Riemannian sums, Gaussian quadrature,
adaptive quadrature, and quasi-Monte Carlo.
\begin{lstlisting}[language=Python]
x = Normal(0, 1)
fx = f(x)
# TODO how to represent E_{p(x)} [ f(x) ]?

output = tf.Variable(tf.random_normal(1))

# We want E_{p(x)} [ f(x) ].
inference = GaussianQuadrature({ : output}))
inference.run()
\end{lstlisting}

{{autogenerated}}
