\title{Inference}

{{navbar}}

\subsubsection{Inference}

We describe the general scope of inference.

Suppose we have a model $p(\mathbf{x}, \mathbf{z}, \beta)$ of data $\mathbf{x}_{\text{train}}$ with latent variables $(\mathbf{z}, \beta)$.
Consider the posterior inference problem,
\begin{equation*}
q(\mathbf{z}, \beta)\approx p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}}),
\end{equation*}
in which the task is to approximate the posterior using another
distribution, $q(\mathbf{z},\beta)$. (For more details, see the
\href{/tutorials/inference} {Inference of Probability Models
tutorial}.)

We represent this by binding random variables in the model to other
random variables in \texttt{latent_vars}; the latter random variables
aim to match the former. For \texttt{data}, we bind random variables
in the model to their observations.

\begin{lstlisting}[language=Python]
inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
\end{lstlisting}

Running inference is as simple as running one method.
\begin{lstlisting}[language=Python]
inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.run()
\end{lstlisting}
%
Inference also supports fine-grained control of the training procedure.
%
\begin{lstlisting}[language=Python]
inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.initialize()

init = tf.initialize_all_variables()
init.run()

for _ in range(inference.n_iter):
  info_dict = inference.update()
  inference.print_progress(info_dict)

inference.finalize()
\end{lstlisting}
The \texttt{run()} method is a simple wrapper for this procedure.

\subsubsection{Remarks}

We highlight other typical settings during inference. (Note the settings are not exclusive and can be used in combination with others.)

\textbf{Model parameters}.
Model parameters are defined as \texttt{tf.Variable}s. They are
nodes in the computational graph that the probability model
depends on.
\begin{equation*}
\hat{\theta} \leftarrow^{\text{optimize}}
p(x; \theta)
\end{equation*}

\begin{lstlisting}[language=Python]
theta = tf.Variable(0.0)
x = Normal(mu=tf.ones(10) * theta, sigma=1.0)

inference = ed.Inference({}, {x: x_train})
\end{lstlisting}
(Here, this inference example does not have any latent variables, and
is only about estimating \texttt{theta} given that we observe
$\mathbf{x} = \mathbf{x}_{\text{train}}$. We can add them so that
inference is both posterior inference and parameter estimation.)

For example, model parameters are useful when applying neural networks
from high-level libraries such as TensorFlow Slim and Keras. The library
implements \texttt{tf.Variable}s  for each neural network layer under
the hood, and thus the inference problem requires estimating these
parameters.

\textbf{Implicit prior samples}.
Latent variables can be defined in the model without any posterior
inference over them.
Tehse are implicitly marginalized out with a single sample.
\begin{equation*}
q(\beta)\approx
p(\beta\mid\mathbf{x}_{\text{train}}, \mathbf{z}^*)
\end{equation*}
where $\mathbf{z}^*\sim p(\mathbf{z}\mid\beta)$ is a prior sample.

\begin{lstlisting}[language=Python]
inference = ed.Inference({beta: qbeta}, {x: x_train})
\end{lstlisting}

For example, implicit prior samples are useful for generative adversarial
networks. Their inference problem does not require any inference over
the latent variables; it uses samples from the prior.
