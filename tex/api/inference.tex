\title{Inference}

{{navbar}}

\subsubsection{Inference}

Given a model $p(x, z)$ and data set $x$,
an inference algorithm infers the posterior distribution $p(z\mid x)$
of the latent variables given data. For more details, see the
\href{/tutorials/inference}
{Inference of Probability Models tutorial}.

The API to \texttt{Inference} is very general. We demonstrate its typical use cases below.

\subsubsection{Posterior Inference}

Consider the posterior inference problem,
\begin{equation*}
q(\mathbf{z}, \beta)\approx p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}}).
\end{equation*}

We represent this by binding random variables in the model to other
random variables in \texttt{latent_vars}; the latter random variables
aim to match the former. For \texttt{data}, we bind random variables
in the model to their observations.

\begin{lstlisting}[language=Python]
inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
\end{lstlisting}

Running inference is as simple as running one method.
\begin{lstlisting}[language=Python]
inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.run()
\end{lstlisting}
%
Inference also supports fine-grained control of the training procedure.
%
\begin{lstlisting}[language=Python]
inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.initialize()

init = tf.initialize_all_variables()
init.run()

for _ in range(inference.n_iter):
  info_dict = inference.update()
  inference.print_progress(info_dict)

inference.finalize()
\end{lstlisting}
The \texttt{run()} method is a simple wrapper for this procedure.

\subsubsection{Remarks}

We highlight other typical settings during inference. (Note the settings are not exclusive and can be used in combination with others.)

\textbf{Model parameters}.
Model parameters are defined as \texttt{tf.Variable}s. They are
nodes in the computational graph that the probability model
depends on.
\begin{equation*}
\hat{\theta} \leftarrow^{\text{optimize}}
p(x; \theta)
\end{equation*}

\begin{lstlisting}[language=Python]
theta = tf.Variable()
x = RandomVariable(params=theta)

inference = ed.Inference({}, {x: x_train})
\end{lstlisting}
(Here, this inference example does not concern any latent variables, and is only about estimating \texttt{theta} given that we observe $\mathbf{x} = \mathbf{x}_{\text{train}}$. We can add them, so that inference is both posterior inference and parameter optimization.)

For example, model parameters are useful when applying neural networks
from libraries such as TensorFlow Slim and Keras. The library
implements \texttt{tf.Variable}s  for each neural network layer under
the hood, and thus the inference problem requires estimating these
parameters.

\textbf{Implicit prior samples}.
Any random variables that the model depends on, but are not specified
in \texttt{Inference}, are implicitly marginalized out with a single
sample.
\begin{equation*}
q(\beta)\approx
p(\beta\mid\mathbf{x}_{\text{train}}, \mathbf{z}^*)
\end{equation*}

\begin{lstlisting}[language=Python]
inference = ed.Inference({beta: qbeta}, {x: x_train})
\end{lstlisting}

For example, implicit prior samples are useful for generative adversarial
networks. Their inference problem does not require any inference over
the latent variables; instead, it uses samples from the prior.
