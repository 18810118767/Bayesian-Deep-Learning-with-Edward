% Define the subtitle of the page
\title{Supervised learning (Regression)}

% Begin the content of the page
\subsection{Supervised learning (Regression)}

In supervised learning, the task is to infer hidden structure from
labeled data, comprised of training examples $\{(x_n, y_n)\}$.
Regression (typically) means the output $y$ takes continuous values.

We demonstrate how to do this in Edward with an example.
The script is available
\href{https://github.com/blei-lab/edward/blob/master/examples/bayesian_linear_regression_test.py}
{here}.


\subsubsection{Data}

Simulate a data set of $40$ data points, which comprises of pairs of inputs $\mathbf{x}_n\in\mathbb{R}^{10}$ and outputs
$y_n\in\mathbb{R}$. They have a linear dependence with normally distributed noise.

\begin{lstlisting}[language=Python]
def build_toy_dataset(N, coeff=np.random.randn(10), noise_std=0.1):
  n_dim = len(coeff)
  x = np.random.randn(N, n_dim).astype(np.float32)
  y = np.dot(x, coeff) + norm.rvs(0, noise_std, size=N)
  return x, y

coeff = np.random.randn(10)
x_train, y_train = build_toy_dataset(N=40, coeff=coeff)
data = {'x': x_train, 'y': y_train}
\end{lstlisting}


\subsubsection{Model}

Posit the model as Bayesian linear regression. For more details on the
model, see the
\href{tut_bayesian_linear_regression}
{Bayesian linear regression tutorial}.

Here we build the model in Edward using TensorFlow.
\begin{lstlisting}[language=Python]
class LinearModel:
  """
  Bayesian linear regression for outputs y on inputs x.

  p((x,y), z) = Normal(y | x*z, lik_std) *
                Normal(z | 0, prior_std),

  where z are weights, and with known lik_std and prior_std.

  Parameters
  ----------
  lik_std : float, optional
    Standard deviation of the normal likelihood; aka noise parameter,
    homoscedastic noise, scale parameter.
  prior_std : float, optional
    Standard deviation of the normal prior on weights; aka L2
    regularization parameter, ridge penalty, scale parameter.
  """
  def __init__(self, lik_std=0.1, prior_std=0.1):
    self.lik_std = lik_std
    self.prior_std = prior_std

  def log_prob(self, xs, zs):
    """Return scalar, the log joint density log p(xs, zs)."""
    x, y = xs['x'], xs['y']
    w, b = zs['w'], zs['b']
    log_prior = tf.reduce_sum(norm.logpdf(w, 0.0, self.prior_std))
    log_prior += tf.reduce_sum(norm.logpdf(b, 0.0, self.prior_std))
    log_lik = tf.reduce_sum(norm.logpdf(y, ed.dot(x, w) + b, self.lik_std))
    return log_lik + log_prior


model = LinearModel()
\end{lstlisting}


\subsubsection{Inference}

Perform variational inference.
Define the variational model to be a fully factorized normal
\begin{lstlisting}[language=Python]
qz_mu = tf.Variable(tf.random_normal([model.n_vars]))
qz_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([model.n_vars])))
qz = Normal(mu=qz_mu, sigma=qz_sigma)
\end{lstlisting}

Run mean-field variational inference for 250 iterations, using 5
latent variable samples per iteration and printing
every 10 iterations.
\begin{lstlisting}[language=Python]
inference = ed.MFVI({'z': qz}, data, model)
inference.run(n_iter=250, n_samples=5, n_print=10)
\end{lstlisting}
In this case \texttt{MFVI} defaults to minimizing the
$\text{KL}(q\|p)$ divergence measure using the reparameterization
gradient.
For more details on inference, see the \href{tut_KLqp}{$\text{KL}(q\|p)$ tutorial}.


\subsubsection{Criticism}

Use point-based evaluation, and calculate the mean squared
error for predictions on test data.

We do this by adding the \texttt{predict()} method in the
probability model.
\begin{lstlisting}[language=Python]
class LinearModel:
  ...
  def predict(self, xs, zs):
    """Return a prediction for each data point, via the likelihood's
    mean."""
    x = xs['x']
    w, b = zs['w'], zs['b']
    return ed.dot(x, w) + b
\end{lstlisting}

Here the test data is simulated from the same process.
\begin{lstlisting}[language=Python]
x_test, y_test = build_toy_dataset(coeff=coeff)
print(ed.evaluate('mse', data={'x': x_test, 'y': y_test},
                  latent_vars={'z': qz}, model_wrapper=model))
## 0.0236971
\end{lstlisting}

The trained model makes predictions with low mean squared error
(relative to the magnitude of the output).

In addition to the model's \texttt{log_prob()} method typically required for
inference, it has a \texttt{predict()} method which makes
predictions. This method is required for point-based evaluation. For
more details on criticism, see the \href{tut_point_eval}{point-based
evaluation tutorial}.
