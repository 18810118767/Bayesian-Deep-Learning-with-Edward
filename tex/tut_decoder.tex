% Define the subtitle of the page
\title{Probabilistic decoder}

% Begin the content of the page
\subsection{Probabilistic decoder}

A probabilistic decoder is a reinterpretation of model likelihoods
based on coding theory. It is a distribution $p(\mathbf{x}_n\mid \mathbf{z}_n)$  over each value
$\mathbf{x}_n\in\mathbb{R}^D$ given a code $\mathbf{z}_n$. The latent
variables $\mathbf{z}_n$ are interpreted as the hidden representation, or code, of the value
$\mathbf{x}_n$. The decoder is probabilistic because its generated
values (decoding) for any given code is random.

For real-valued data,
the randomness in the decoder is given by a multivariate Gaussian
\begin{align*}
  p(\mathbf{x}_n\mid\mathbf{z}_n)
  &=
  \mathcal{N}(\mathbf{x}_n\mid [\mu,\sigma^2]=\mathrm{NN}(\mathbf{z}_n; \mathbf{\theta})),
\end{align*}
where the probabilistic decoder is parameterized by a neural network
$\mathrm{NN}$ taking the code $\mathbf{z}_n$ as input.

For binary data,
the randomness in the decoder is given by a Bernoulli
\begin{align*}
  p(\mathbf{x}_n\mid\mathbf{z}_n)
  &=
  \text{Bernoulli}(\mathbf{x}_n\mid p=\mathrm{NN}(\mathbf{z}_n; \mathbf{\theta})).
\end{align*}
Probabilistic decoders are typically used alongside a standard normal
prior over the code
\begin{align*}
  p(\mathbf{z})
  &=
  \mathcal{N}(\mathbf{z} \;;\; \mathbf{0}, I).
\end{align*}

Let's build the model in Edward using
Keras as an easy way to build neural networks. Here we use a
probabilistic decoder to model binarized 28 x 28
pixel images from MNIST.
\begin{lstlisting}[language=Python]
z = Normal(mu=tf.zeros([N, d]), sigma=tf.ones([N, d]))
hidden = Dense(256, activation=K.relu)(z.value())
x = Bernoulli(logits=Dense(28 * 28)(hidden))
\end{lstlisting}
It starts with a $d$-dimensional standard normal prior, one for each
data point. Then it applies a layer of 256 hidden units with
\texttt{relu} nonlinearity. Then it applies a layer of 256 hidden
units without a nonlinearity. The output of this neural net
parameterizes the Bernoulli's logits, which is more numerically stable
than parameterizing a Bernoulli according to a probability constrained
from 0 and 1.

An example script using this model can found
\href{https://github.com/blei-lab/edward/blob/master/examples/tf_convolutional_vae.py}
{here}.
%We experiment with this model in the
%\href{variational_autoencoder}{variational auto-encoder} tutorial.

\subsubsection{Footnotes}

The neural network which parameterizes the probabilistic decoder is
also known as a generative network. It is in analogy to an
\href{tut_inference_networks}{inference network}, which
can parameterize a variational model used for inference,
interpreted as a probabilistic encoder.

Traditionally, a probabilistic encoder is the most common
choice of inference. This lead to the coinage of the model-inference
combination known as the variational auto-encoder
(Kingma and Welling, 2014), which is a probabilistic extension of
auto-encoders.
We recommend against this terminology,
in favor of making explicit the separation of model and inference.
That is, probabilistic decoders are a general class of
models that can be used without an encoder.
Variational
inference is not necessary to infer probabilistic decoders, and
variational inference can also be done without an inference network.

\subsubsection{References}\label{references}

\begin{itemize}
\item
  Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889â€“904.
\item
  Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. In International Conference on Learning Representations.
\end{itemize}
