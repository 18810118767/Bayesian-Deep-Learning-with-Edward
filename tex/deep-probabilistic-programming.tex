\title{Deep Probabilistic Programming}

\subsection{Deep Probabilistic Programming}

This webpage is a companion to the article,
\href{https://arxiv.org/abs/1701.03757}{Deep Probabilistic Programming}
\citep{tran2017deep}
.
Here we provide more details for plug-and-play with the code snippets
and also how to reproduce the experiments.

The code snippets assume Edward v1.2.0 and TensorFlow v0.11.0.
\begin{lstlisting}[language=bash]
pip install edward==1.2.0
export TF_BINARY_URL =  # see https://www.tensorflow.org/versions/r0.11/get_started/os_setup#download-and-setup
pip install $TF_BINARY_URL
\end{lstlisting}

\subsection{Experiments}
\textbf{This section is under construction.}
% \textbf{Recent Methods in Variational Inference}.

% \textbf{GPU-accelerated Hamiltonian Monte Carlo}.

\subsection{Figures}

\textbf{Figure 1}. Beta-Bernoulli program.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Bernoulli, Beta

theta = Beta(a=1.0, b=1.0)
x = Bernoulli(p=tf.ones(50) * theta)
\end{lstlisting}
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli.py}{\texttt{examples/beta_bernoulli.py}}
in the Github repository.

\textbf{Figure 2}. Variational auto-encoder for a data set of 28 x 28 pixel images
\citep{kingma2014auto,rezende2014stochastic}.
\begin{lstlisting}[language=python]
import tensorflow as tf
from tensorflow.contrib import slim
from edward.models import Bernoulli, Normal

N = # TODO  # number of data points
d = 50  # latent dimension

# Probabilistic model
z = Normal(mu=tf.zeros([N, d]), sigma=tf.ones([N, d]))
h = slim.fully_connected(z, 256, activation_fn=tf.nn.relu)
x = Bernoulli(logits=slim.fully_connected(h, 28 * 28, activation_fn=None))

# Variational model
qx = tf.placeholder(tf.float32, [N, 28 * 28])
qh = slim.fully_connected(qx, 256, activation_fn=tf.nn.relu)
qz = Normal(mu=slim.fully_connected(qh, d, activation_fn=None),
            sigma=slim.fully_connected(qh, d, activation_fn=tf.nn.softplus))
\end{lstlisting}
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/vae.py}{\texttt{examples/vae.py}}
in the Github repository.

\textbf{Figure 3}. Bayesian recurrent neural network \citep{neal2012bayesian}.
The program has an unspecified number of time steps; it uses a
symbolic for loop (\texttt{tf.scan}).
\begin{lstlisting}[language=python]
# TODO tf.dot
import tensorflow as tf
from edward.models import Normal

H = 50  # number of hidden units
D = 10  # number of features

def rnn_cell(hprev, xt):
  return tf.tanh(tf.dot(hprev, Wh) + tf.dot(xt, Wx) + bh)

Wh = Normal(mu=tf.zeros([H, H]), sigma=tf.ones([H, H]))
Wx = Normal(mu=tf.zeros([D, H]), sigma=tf.ones([D, H]))
Wy = Normal(mu=tf.zeros([H, 1]), sigma=tf.ones([H, 1]))
bh = Normal(mu=tf.zeros(H), sigma=tf.ones(H))
by = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.placeholder(tf.float32, [None, D])
h = tf.scan(rnn_cell, x, initializer=tf.zeros(H))
y = Normal(mu=tf.matmul(h, Wy) + by, sigma=1.0)
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/bayesian_rnn.py}{\texttt{examples/bayesian_rnn.py}}
in the Github repository.

\textbf{Figure 5}. Hierarchical model \citep{gelman2006data}.
  It is a mixture of Gaussians over
  $D$-dimensional data $\{x_n\}\in\mathbb{R}^{N\times D}$. There are
  $K$ latent cluster means $\beta\in\mathbb{R}^{K\times D}$.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Categorical, Normal

N = 10000  # number of data points
D = 2  # data dimension
K = 5  # number of clusters

beta = Normal(mu=tf.zeros([K, D]), sigma=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([N, K]))
x = Normal(mu=tf.gather(beta, z), sigma=tf.ones([N, D]))
\end{lstlisting}
It is used below in Figure * (Inference), Figure 6 (left/right),
Figure * (variational EM), and Figure * (data subsampling).

\textbf{Figure 6} (left). Variational inference
\citep{jordan1999introduction}.
It performs inference on the model defined in Figure 5.
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf
from edward.models import Categorical, Normal

x_train =  # TODO

qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
  sigma=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_vi.py}{\texttt{examples/mixture_gaussian_vi.py}}
in the Github repository.

\textbf{Figure 6} (right). Monte Carlo \citep{robert1999monte}.
It performs inference on the model defined in Figure 5.
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf
from edward.models import Empirical

x_train =  # TODO

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_mc.py}{\texttt{examples/mixture_gaussian_mc.py}}
in the Github repository.

\textbf{Figure 7}. Generative adversarial network
\citep{goodfellow2014generative}.
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf

def generative_network(z):
  h = slim.fully_connected(z, 256, activation_fn=tf.nn.relu)
  return slim.fully_connected(h, 28 * 28, activation_fn=None)

def discriminative_network(x):
  h = slim.fully_connected(z, 28 * 28, activation_fn=tf.nn.relu)
  return slim.fully_connected(h, 1, activation_fn=None)

# Probabilistic model
z = Normal(mu=tf.zeros([M, d]), sigma=tf.ones([M, d]))
x = generative_network(z)

# augmentation for GAN-based inference
y_fake = Bernoulli(logits=discriminative_network(x))
y_real = Bernoulli(logits=discriminative_network(x_train))

data = {y_real: tf.ones(N), y_fake: tf.zeros(M)}
# TODO
inference = ed.GANInference(data=data)
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/gan.py}{\texttt{examples/gan.py}}
in the Github repository.

\textbf{Figure *}. Variational EM \citep{neal1993new}.
It performs inference on the model defined in Figure 5.
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference_e = ed.VariationalInference({z: qz}, data={x: x_data, beta: qbeta})
inference_m = ed.MAP({beta: qbeta}, data={x: x_data, z: qz})

inference_e.initialize()
inference_m.initialize()

tf.initialize_all_variables().run()

for _ in range(10000):
  inference_e.update()
  inference_m.update()
\end{lstlisting}
For more details, see the
\href{/api/inference-compositionality}{inference compositionality} webpage.

\textbf{Figure *}. Data subsampling.
It performs inference on the model defined in Figure 5.
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf

N = 10000  # number of data points
D = 2  # data dimension
K = 5  # number of clusters

beta = Normal(mu=tf.zeros([K, D]), sigma=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([M, K]))
x = Normal(mu=tf.gather(beta, z), sigma=tf.ones([M, D]))

qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
               sigma=tf.nn.softplus(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[M, D]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_batch})
inference.initialize(scale={x: float(N) / M, z: float(N) / M})
\end{lstlisting}
For more details, see the
\href{/api/data-subsampling}{data subsampling} webpage.

\textbf{Figure 9}. Bayesian logistic regression with Hamiltonian Monte Carlo.
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf
from edward.models import Bernoulli, Empirical, Normal

x_data =  # TODO
y_data =  # TODO
T =  # TODO
N =  # TODO
D =  # TODO

# Model
x = tf.Variable(x_data, trainable=False)
beta = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
y = Bernoulli(logits=tf.dot(x, beta))

# Inference
qbeta = Empirical(params=tf.Variable(tf.zeros([T, D])))
inference = ed.HMC({beta: qbeta}, data={y: y_data})
inference.run(step_size=0.5 / N, n_steps=10)
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/hmc.py}{\texttt{examples/hmc.py}}
in the Github repository.

\textbf{Figure 10}. Bayesian neural network for classification \citep{denker1987large}.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Bernoulli, Normal

W_0 = Normal(mu=tf.zeros([D, H]), sigma=tf.ones([D, H]))
W_1 = Normal(mu=tf.zeros([H, 1]), sigma=tf.ones([H, 1]))
b_0 = Normal(mu=tf.zeros(H), sigma=tf.ones(L))
b_1 = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.placeholder(tf.float32, [N, D])
y = Bernoulli(logits=tf.matmul(tf.nn.tanh(tf.matmul(x, W_0) + b_0), W_1) + b_1)
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/bayesian_nn_classification.py}{\texttt{examples/bayesian_nn_classification.py}}
in the Github repository.

\textbf{Figure 11}. Restricted Boltzmann machine \citep{smolensky1986information}.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Bernoulli

# Model parameters
W = tf.Variable(tf.float32, [N, M])  # N x M weight matrix
b_v = tf.Variable(tf.float32, [N])  # bias vector for v
b_h = tf.Variable(tf.float32, [M])  # bias vector for h

# Model conditionals
v_ph = tf.placeholder(tf.float32, [N])  # mutable state
h = Bernoulli(logits=b_h + tf.dot(v, W))
v = Bernoulli(logits=b_v + tf.dot(W, h))  # v is tied to v_ph during inference
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/rbm.py}{\texttt{examples/rbm.py}}
in the Github repository.

\textbf{Figure 12}. Dirichlet process mixture model \citep{antoniak1974mixtures}.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Normal, DirichletProcess

H = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
mu = tf.pack([DirichletProcess(alpha=1.0, base=H) for _ in range(N)])
x = Normal(mu=mu, sigma=tf.ones(N))
\end{lstlisting}
The essential component defining the \texttt{DirichletProcess} random
variable is a stochastic while loop. We define it below.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Bernoulli, Beta

def dirichlet_process(alpha):
  def cond(k, beta_k):
    flip = Bernoulli(p=beta_k)
    return tf.equal(flip, tf.constant(1))

  def body(k, beta_k):
    beta_k = beta_k * Beta(a=1.0, b=alpha)
    return k + 1, beta_k

  k = tf.constant(0)
  beta_k = Beta(a=1.0, b=alpha)
  stick_num, stick_beta = tf.while_loop(cond, body, loop_vars=[k, beta_k])
  return stick_num
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/dp_mixture_model.py}{\texttt{examples/dp_mixture_model.py}}
in the Github repository.

\textbf{Figure 13}. Latent Dirichlet allocation \citep{blei2003latent}.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Categorical, Dirichlet

D = 50  # number of documents
N = [11502, 213, 1523, 1351, ...]  # words per doc
K = 10  # number of topics
V = 100000  # vocabulary size

theta = Dirichlet(alpha=tf.zeros([D, K]) + 0.1)
phi = Dirichlet(alpha=tf.zeros([K, V]) + 0.05)
z = [[0] * N] * D
w = [[0] * N] * D
for d in range(D):
  for n in range(N[d]):
    z[d][n] = Categorical(pi=theta[d, :])
    w[d][n] = Categorical(pi=phi[z[d][n], :])
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/lda.py}{\texttt{examples/lda.py}}
in the Github repository.

\textbf{Figure 14}. Gaussian matrix factorization
\citep{salakhutdinov2011probabilistic}.
\begin{lstlisting}[language=python]
import tensorflow as tf
from edward.models import Normal

N = 10
M = 10
K = 5  # latent dimension

U = Normal(mu=tf.zeros([M, K]), sigma=tf.ones([M, K]))
V = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
Y = Normal(mu=tf.matmul(U, V, transpose_b=True), sigma=tf.ones([N, M]))
\end{lstlisting}
TODO
For an example of it in use, see
\href{https://github.com/blei-lab/edward/blob/master/examples/gaussian_matrix_factorization.py}{\texttt{examples/gaussian_matrix_factorization.py}}
in the Github repository.

\textbf{Figure *}. Stochastic variational inference \citep{hoffman2013stochastic}.
For more details, see the
\href{/api/data-subsampling}{data subsampling} webpage.

\textbf{Figure 15}. Variational auto-encoder
\citep{kingma2014auto,rezende2014stochastic}.
See the script
\href{https://github.com/blei-lab/edward/blob/master/examples/vae.py}{\texttt{examples/vae.py}}
in the Github repository.

TODO
\textbf{Figure 16}. Exponential family embedding \citep{rudolph2016exponential}.
See the script
\href{https://github.com/blei-lab/edward/blob/master/examples/ef_emb.py}{\texttt{examples/ef_emb.py}}
in the Github repository.
A Github repo with more features is available at
\href{https://github.com/mariru/exponential_family_embeddings}{mariru/exponential_family_embeddings}.

\subsubsection{References}\label{references}
