\title{Criticism}

{{navbar}}

\subsubsection{Criticism}

We can never validate whether a model is true. In practice, ``all
models are wrong''~\citep{box1976science}. However, we can try to
uncover where the model goes wrong. Model criticism helps justify the
model as an approximation or point to good directions for revising the
model.

Criticism is defined simply with utility functions. They take random
variables as input and output NumPy arrays.

Edward explores model and inference criticism using
\begin{itemize}
  \item point-based evaluations, such as mean squared error or
  classification accuracy
\end{itemize}
\begin{lstlisting}[language=Python]
ed.evaluate('mean_squared_error', data={y: y_data, x: x_data})
\end{lstlisting}
\begin{itemize}
  \item posterior predictive checks, for making probabilistic
  assessments of the model fit using discrepancy functions
\end{itemize}
\begin{lstlisting}[language=Python]
T = lambda xs, zs: tf.reduce_mean(xs[x])
ed.ppc(T, data={x: x_data})
\end{lstlisting}

Here's an example. First, let's build a model and perform
inference.
\begin{lstlisting}[language=Python]
beta = RandomVariable()
z = RandomVariable(par=z)
x = RandomVariable(par_1=z, par_2=beta)

qbeta = RandomVariable()
qz = RandomVariable(par=qbeta)

x_train = np.array()

inference = Inference({z: qz, beta: qbeta}, data={x: x_train})
inference.run()
\end{lstlisting}
We first build the posterior predictive distribution.
\begin{lstlisting}[language=Python]
# Build the posterior predictive distribution by copying the
# likelihood, swapping out the prior `z` and `beta` with the inferred
# posterior `qz` and `qbeta`.
x_post = copy(x, {z: qz, beta: qbeta})
\end{lstlisting}

Now we run some techniques:
\href{/tutorials/point-evaluation}{\texttt{evaluate}}
and
\href{/tutorials/ppc}{\texttt{ppc}}.

\begin{lstlisting}[language=Python]
# log-likelihood performance
evaluate('log_likelihood', data={x_post: x_train})

# classification accuracy
# here, `x_ph` is any features the model is defined with respect to,
# and `y_post` is the posterior predictive distribution
evaluate('binary_accuracy', data={y_post: y_train, x_ph: x_train})

# posterior predictive check
# T is a user-defined function of data, T(data)
ppc(T, data={x_post: x_train})

# in general T is a discrepancy function of the data (both response and
# covariates) and latent variables, T(data, latent_vars)
ppc(T, data={y_post: y_train, x_ph: x_train}, latent_vars={'z': qz, 'beta': qbeta})

# prior predictive check
# running ppc on original x
ppc(T, data={x: x_train})
\end{lstlisting}

Suppose you want to perform criticism on data held-out from training.
We first perform inference over the local latent variables of the
held-out data. Then we make the predictions.

\begin{lstlisting}[language=Python]
# create local variational factors for test data
qz_test = RandomVariable()

x_test = np.array()

inference_test = Inference({z: qz_test, beta: qbeta}, data={x: x_test})
inference_test.run()

# build posterior predictive on test data
x_post = copy(x, {z: qz_test, beta: qbeta}})
evaluate('log_likelihood', data={x_post: x_test})
\end{lstlisting}

Criticism techniques are simply functions which take as input data,
the probability model and variational model (binded through a latent
variable dictionary), and any additional inputs.

\begin{lstlisting}[language=Python]
def criticize(data, latent_vars, ...)
  ...
\end{lstlisting}

Developing new criticism techniques is easy.  They can be derived from
the current techniques or built as a standalone function.

For examples of criticism techniques built in Edward, see the
criticism
\href{/tutorials/}{tutorials}.

\subsubsection{References}\label{references}
