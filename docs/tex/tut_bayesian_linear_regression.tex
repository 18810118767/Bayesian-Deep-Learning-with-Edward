% Define the subtitle of the page
\title{Bayesian Linear Regression}

% Begin the content of the page
\subsection{Bayesian Linear Regression}


Bayesian linear regression is a model $p(\mathbf{y}\mid \mathbf{X})$ of
outputs $y\in\mathbb{R}$, also known as the response, given
a vector of inputs
$\mathbf{x}\in\mathbb{R}^D$, also known as the features or covariates.
The model assumes a
linear relationship between these two random variables.

For a set of $N$ data points $(\mathbf{X},\mathbf{y})=\{(\mathbf{x}_n, y_n)\}$,
the model is
\begin{align*}
  p(\mathbf{z})
  &=
  \mathcal{N}(\mathbf{z} \;;\; \mathbf{0}, \sigma_z^2\mathbf{I}),
  \\
  p(\mathbf{y} \mid \mathbf{z}, \mathbf{X})
  &=
  \prod_{n=1}^N
  \mathcal{N}(y_n \;;\; \mathbf{x}_n^\top\mathbf{w} + b, \sigma_y^2).
\end{align*}
The latent variables $\mathbf{z}=[\mathbf{w},b]$ form the weights $\mathbf{w}$
and intercept $b$, also known as the bias.
Assume $\sigma_z^2$ is a known prior variance and $\sigma_y^2$ is a
known likelihood variance. The mean of the likelihood is given by a
linear transformation of the inputs $\mathbf{x}_n$ and shared weights
$\mathbf{w}$, along with the intercept $b$.

Let's build the model in Edward using TensorFlow, fixing $D=10$.
\begin{lstlisting}[language=Python]
class LinearModel:
  """
  Bayesian linear regression for outputs y on inputs x.

  p((x,y), (w,b)) = Normal(y | x*w + b, lik_std) *
                    Normal(w | 0, prior_std) *
                    Normal(b | 0, prior_std),

  where w and b are weights and intercepts, and with known lik_std and
  prior_std.

  Parameters
  ----------
  lik_std : float, optional
    Standard deviation of the normal likelihood; aka noise parameter,
    homoscedastic noise, scale parameter.
  prior_std : float, optional
    Standard deviation of the normal prior on weights; aka L2
    regularization parameter, ridge penalty, scale parameter.
  """
  def __init__(self, lik_std=0.1, prior_std=0.1):
    self.lik_std = lik_std
    self.prior_std = prior_std

  def log_prob(self, xs, zs):
    x, y = xs['x'], xs['y']
    w, b = zs['w'], zs['b']
    log_prior = tf.reduce_sum(norm.logpdf(w, 0.0, self.prior_std))
    log_prior += tf.reduce_sum(norm.logpdf(b, 0.0, self.prior_std))
    log_lik = tf.reduce_sum(norm.logpdf(y, ed.dot(x, w) + b, self.lik_std))
    return log_lik + log_prior


model = LinearModel()
\end{lstlisting}

We experiment with this model in the \href{tut_supervised_regression.html}{supervised
learning (regression)} tutorial.
An example script with visualization is available
\href{https://github.com/blei-lab/edward/blob/master/examples/bayesian_linear_regression_plot.py}
{here}.

\subsubsection{References}\label{references}

\begin{itemize}
\item
  Murphy, K. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
\end{itemize}
