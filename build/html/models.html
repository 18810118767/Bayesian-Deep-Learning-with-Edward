

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Models &mdash; Edward 1.0.9 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Edward 1.0.9 documentation" href="index.html"/>
        <link rel="next" title="Inference" href="inferences.html"/>
        <link rel="prev" title="Data" href="data.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Edward
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#random-variables">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="#variational-models">Variational Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-wrappers">Model Wrappers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-wrapper-api">Model Wrapper API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="inferences.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="criticisms.html">Criticism</a></li>
<li class="toctree-l1"><a class="reference internal" href="edward.html">edward package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Edward</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Models</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/models.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<p>A probabilistic model specifies a joint distribution <code class="docutils literal"><span class="pre">p(x,</span> <span class="pre">z)</span></code>
of data <code class="docutils literal"><span class="pre">x</span></code> and latent variables <code class="docutils literal"><span class="pre">z</span></code>.
For more details, see the
<a class="reference external" href="../tutorials/model">Probability Models tutorial</a>.</p>
<div class="section" id="random-variables">
<h2>Random Variables<a class="headerlink" href="#random-variables" title="Permalink to this headline">¶</a></h2>
<p>The building block of probabilistic modeling is a random variable. In
Edward, random variables are built on top of
<a class="reference external" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distributions">tf.contrib.distributions</a>.
It has the same arguments to instantiate the object, and is equipped
with the same methods such as <code class="docutils literal"><span class="pre">log_prob()</span></code> and <code class="docutils literal"><span class="pre">sample()</span></code>.
See their docstrings for more details.</p>
<p>Here are examples of univariate distributions.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># univariate normal</span>
<span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
<span class="c1"># vector of 5 univariate normals</span>
<span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># 2 x 3 matrix of Exponentials</span>
<span class="n">Exponential</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
</pre></div>
</div>
<p>The dimension of the distributions are deduced from the dimension of
its parameters.</p>
<p>Here are examples of multivariate distributions.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># 1 K-dimensional Dirichlet</span>
<span class="n">Dirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">]</span><span class="o">*</span><span class="n">K</span><span class="p">)</span>
<span class="c1"># vector of 5 K-dimensional multivariate normals</span>
<span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="n">K</span><span class="p">]))</span>
<span class="c1"># 2 x 5 matrix of K-dimensional multivariate normals</span>
<span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">K</span><span class="p">]))</span>
</pre></div>
</div>
<p>Multivariate distributions store their multivariate dimension in the
inner dimension (right-most dimension) of the parameters.</p>
<p>Formally, <code class="docutils literal"><span class="pre">RandomVariable</span></code> objects define a wrapper around tensors. That is,
we register a <code class="docutils literal"><span class="pre">tensor_conversion_function()</span></code> which converts the object to
samples. This enables tensor operations, placing <code class="docutils literal"><span class="pre">RandomVariable</span></code>
objects on the TensorFlow graph.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
<span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 3rd normal rv in the vector</span>
</pre></div>
</div>
<p>Importantly, the graphical model is defined explicitly by its
TensorFlow graph. This allows random variables to be used in
conjuction with other TensorFlow ops.</p>
<p>Parameters work with random variables in the same way they
work with <code class="docutils literal"><span class="pre">tf.contrib.distributions</span></code>.</p>
<ul>
<li><p class="first">Constant parameters.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># 5-dimensional normal with constant params</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p class="first">Trainable parameters.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># 5-dimensional normal</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="c1"># 5-dimensional full rank normal</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">)</span>

<span class="c1"># Normal parameterized by a neural network</span>
<span class="n">W_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
<span class="n">W_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">h</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">])</span>
<span class="n">b_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
<span class="n">b_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_1</span><span class="p">),</span> <span class="n">W_2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p class="first">Random parameters.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># 2-dimensional prior</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
</pre></div>
</div>
</li>
</ul>
<p>For examples of models built in Edward, see the model
<a class="reference external" href="../tutorials/">tutorials</a>.</p>
</div>
<div class="section" id="variational-models">
<h2>Variational Models<a class="headerlink" href="#variational-models" title="Permalink to this headline">¶</a></h2>
<p>A variational model defines a distribution over latent variables. It
is a model of the posterior distribution, specifying another
distribution to approximate it.
Edward implements variational models using the same language of random
variables.</p>
<p>We parameterize them with TensorFlow variables so that their
parameters may be trained during inference.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">Dirichlet</span><span class="p">,</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">InverseGamma</span>

<span class="n">qpi_alpha</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">K</span><span class="p">])))</span>
<span class="n">qmu_mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">K</span> <span class="o">*</span> <span class="n">D</span><span class="p">]))</span>
<span class="n">qmu_sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">K</span> <span class="o">*</span> <span class="n">D</span><span class="p">])))</span>
<span class="n">qsigma_alpha</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">K</span> <span class="o">*</span> <span class="n">D</span><span class="p">])))</span>
<span class="n">qsigma_beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">K</span> <span class="o">*</span> <span class="n">D</span><span class="p">])))</span>

<span class="n">qpi</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">qpi_alpha</span><span class="p">)</span>
<span class="n">qmu</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">qmu_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">qmu_sigma</span><span class="p">)</span>
<span class="n">qsigma</span> <span class="o">=</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">qsigma_alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">qsigma_beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="model-wrappers">
<h2>Model Wrappers<a class="headerlink" href="#model-wrappers" title="Permalink to this headline">¶</a></h2>
<p>Edward also supports specifying models using external languages. These
model wrappers are written as a class.</p>
<p>In general, a model wrapper is a class with the structure</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vars</span> <span class="o">=</span> <span class="o">...</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
        <span class="n">log_prior</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="o">...</span>
        <span class="k">return</span> <span class="n">log_prior</span> <span class="o">+</span> <span class="n">log_likelihood</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>The field <code class="docutils literal"><span class="pre">n_vars</span></code> denotes the number of latent variables in the
probability model. For example, a model with a Gaussian likelihood with latent
mean and variance would have <code class="docutils literal"><span class="pre">n_vars=2*N</span></code> latent variables for
<code class="docutils literal"><span class="pre">N</span></code> observations.</p>
<p>The method <code class="docutils literal"><span class="pre">log_prob(xs,</span> <span class="pre">zs)</span></code> calculates the logarithm of
the joint density $log p(x,z)$. Here <code class="docutils literal"><span class="pre">xs</span></code> can be a single data
point or a batch of data points. Analogously, <code class="docutils literal"><span class="pre">zs</span></code> can be a
single set of latent variables, or a batch thereof.</p>
<p><strong>TensorFlow.</strong>
Write a class with the method <code class="docutils literal"><span class="pre">log_prob(xs,</span> <span class="pre">zs)</span></code>. The method defines
the logarithm of a joint density, where <code class="docutils literal"><span class="pre">xs</span></code> and <code class="docutils literal"><span class="pre">zs</span></code> are Python
dictionaries binding the name of a random variable to
a realization.
Here <code class="docutils literal"><span class="pre">xs</span></code> can be a single data
point or a batch of data points, and analogously, <code class="docutils literal"><span class="pre">zs</span></code> can be a
single set or multiple sets of latent variables.
Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">edward.stats</span> <span class="k">import</span> <span class="n">bernoulli</span><span class="p">,</span> <span class="n">beta</span>

<span class="k">class</span> <span class="nc">BetaBernoulli</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;p(x, p) = Bernoulli(x | p) * Beta(p | 1, 1)&quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
    <span class="n">log_prior</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">],</span> <span class="n">a</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">log_lik</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">zs</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">log_lik</span> <span class="o">+</span> <span class="n">log_prior</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BetaBernoulli</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">BetaBernoulli</span></code> defines a log joint density with a Bernoulli
likelihood (for an unspecified number of data points) and a Beta prior
on the Bernoulli&#8217;s success probability.
<code class="docutils literal"><span class="pre">xs</span></code> is a dictionary with string <code class="docutils literal"><span class="pre">x</span></code> binded to a vector of
observations. <code class="docutils literal"><span class="pre">zs</span></code> is a dictionary with string <code class="docutils literal"><span class="pre">z</span></code> binded to a
sample from the one-dimensional Beta latent variable.</p>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/tf_beta_bernoulli.py">toy script</a>
that uses this model. The model class can be more complicated,
containing fields or other methods required for other functionality in
Edward. See the section below for more details.</p>
<p><strong>Python.</strong>
Write a class that inherits from <code class="docutils literal"><span class="pre">PythonModel</span></code> and with the method
<code class="docutils literal"><span class="pre">_py_log_prob(xs,</span> <span class="pre">zs)</span></code>. The method defines the logarithm of a joint
density with the same concept as in a TensorFlow model, but where
<code class="docutils literal"><span class="pre">xs</span></code> and <code class="docutils literal"><span class="pre">zs</span></code> now use NumPy arrays rather than TensorFlow tensors.
Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">PythonModel</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="k">import</span> <span class="n">bernoulli</span><span class="p">,</span> <span class="n">beta</span>

<span class="k">class</span> <span class="nc">BetaBernoulli</span><span class="p">(</span><span class="n">PythonModel</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;p(x, p) = Bernoulli(x | p) * Beta(p | 1, 1)&quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">_py_log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
    <span class="n">log_prior</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">],</span> <span class="n">a</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">log_lik</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">zs</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">log_lik</span> <span class="o">+</span> <span class="n">log_prior</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">BetaBernoulli</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/np_beta_bernoulli.py">toy script</a>
that uses this model.</p>
<p><strong>Stan.</strong>
Write a Stan program in the form of a file or string. Then
call it with <code class="docutils literal"><span class="pre">StanModel(file=file)</span></code> or
<code class="docutils literal"><span class="pre">StanModel(model_code=model_code)</span></code>. Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">StanModel</span>

<span class="n">model_code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">  data {</span>
<span class="s2">    int&lt;lower=0&gt; N;</span>
<span class="s2">    int&lt;lower=0,upper=1&gt; x[N];</span>
<span class="s2">  }</span>
<span class="s2">  parameters {</span>
<span class="s2">    real&lt;lower=0,upper=1&gt; p;</span>
<span class="s2">  }</span>
<span class="s2">  model {</span>
<span class="s2">    p ~ beta(1.0, 1.0);</span>
<span class="s2">    for (n in 1:N)</span>
<span class="s2">    x[n] ~ bernoulli(p);</span>
<span class="s2">  }</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">StanModel</span><span class="p">(</span><span class="n">model_code</span><span class="o">=</span><span class="n">model_code</span><span class="p">)</span>
</pre></div>
</div>
<p>During inference the latent variable string matches the name of the
parameters from the parameter block. Analogously, the data&#8217;s string
matches the name of the data from the data block.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">qp</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;N&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}</span>
<span class="n">inference</span> <span class="o">=</span> <span class="n">Inference</span><span class="p">({</span><span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="n">qp</span><span class="p">},</span> <span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/stan_beta_bernoulli.py">toy
script</a>
that uses this model. Stan programs are convenient as <a class="reference external" href="https://github.com/stan-dev/example-models/wiki">there are many
online examples</a>,
although they are limited to probability models with differentiable
latent variables. <code class="docutils literal"><span class="pre">StanModel</span></code> objects also contain no structure about
the model besides how to calculate its joint density.</p>
<p><strong>PyMC3.</strong>
Write a PyMC3 model whose observed values are Theano shared variables,
and whose latent variables use <code class="docutils literal"><span class="pre">transform=None</span></code> to keep them on their
original (constrained) domain.
The values in the Theano shared variables can be plugged at a later
time. Here is an example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">from</span> <span class="nn">edward.models</span> <span class="k">import</span> <span class="n">PyMC3Model</span>

<span class="n">x_obs</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pm_model</span><span class="p">:</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">x_obs</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">PyMC3Model</span><span class="p">(</span><span class="n">pm_model</span><span class="p">)</span>
</pre></div>
</div>
<p>During inference the latent variable string matches the name of the
model&#8217;s latent variables; the data&#8217;s string matches the Theano shared
variables.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">qp</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="n">x_obs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])}</span>
<span class="n">inference</span> <span class="o">=</span> <span class="n">Inference</span><span class="p">({</span><span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="n">qp</span><span class="p">},</span> <span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a <a class="reference external" href="https://github.com/blei-lab/edward/blob/master/examples/pymc3_beta_bernoulli.py">toy
script</a>
that uses this model. PyMC3 can be used to define models with both
differentiable latent variables and non-differentiable (e.g., discrete)
latent variables. <code class="docutils literal"><span class="pre">PyMC3Model</span></code> objects contain no structure about the
model besides how to calculate its joint density.</p>
<p>For modeling convenience, we recommend using the modeling language that
you are most familiar with. For efficiency, we recommend using
TensorFlow, as Edward uses TensorFlow as the computational backend.
Internally, other languages are wrapped in TensorFlow so their
computation represents a single node in the graph (making it difficult
to tease apart and thus distribute their computation).</p>
<div class="section" id="model-wrapper-api">
<h3>Model Wrapper API<a class="headerlink" href="#model-wrapper-api" title="Permalink to this headline">¶</a></h3>
<p>This outlines the current spec for all methods in the model object.
It includes all modeling languages, where certain methods are
implemented by wrapping around other methods. For example, by a Python
model builds a <code class="docutils literal"><span class="pre">_py_log_prob()</span></code> method and inherits from
<code class="docutils literal"><span class="pre">PythonModel</span></code>; <code class="docutils literal"><span class="pre">PythonModel</span></code> implements <code class="docutils literal"><span class="pre">log_prob()</span></code> by wrapping
around <code class="docutils literal"><span class="pre">_py_log_prob()</span></code> as a TensorFlow operation.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Used in: (most) inference.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    xs : dict of str to tf.Tensor</span>
<span class="sd">      Data dictionary. Each key names a data structure used in the</span>
<span class="sd">      model (str), and its value is the corresponding corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>
<span class="sd">    zs : dict of str to tf.Tensor</span>
<span class="sd">      Latent variable dictionary. Each key names a latent variable</span>
<span class="sd">      used in the model (str), and its value is the corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tf.Tensor</span>
<span class="sd">      Scalar, the log joint density log p(xs, zs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">log_lik</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Used in: inference with analytic KL.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    xs : dict of str to tf.Tensor</span>
<span class="sd">      Data dictionary. Each key names a data structure used in the</span>
<span class="sd">      model (str), and its value is the corresponding corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>
<span class="sd">    zs : dict of str to tf.Tensor</span>
<span class="sd">      Latent variable dictionary. Each key names a latent variable</span>
<span class="sd">      used in the model (str), and its value is the corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tf.Tensor</span>
<span class="sd">      Scalar, the log-likelihood log p(xs | zs).</span>
<span class="sd">    &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Used in: ed.evaluate().</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    xs : dict of str to tf.Tensor</span>
<span class="sd">      Data dictionary. Each key names a data structure used in the</span>
<span class="sd">      model (str), and its value is the corresponding corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>
<span class="sd">    zs : dict of str to tf.Tensor</span>
<span class="sd">      Latent variable dictionary. Each key names a latent variable</span>
<span class="sd">      used in the model (str), and its value is the corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tf.Tensor</span>
<span class="sd">      Tensor of predictions, one for each data point. The prediction</span>
<span class="sd">      is the likelihood&#39;s mean. For example, in supervised learning</span>
<span class="sd">      of i.i.d. categorical data, it is a vector of labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">sample_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Used in: ed.ppc().</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict of str to tf.Tensor</span>
<span class="sd">      Latent variable dictionary. Each key names a latent variable</span>
<span class="sd">      used in the model (str), and its value is the corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">sample_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Used in: ed.ppc().</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    zs : dict of str to tf.Tensor</span>
<span class="sd">      Latent variable dictionary. Each key names a latent variable</span>
<span class="sd">      used in the model (str), and its value is the corresponding</span>
<span class="sd">      realization (tf.Tensor).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict of str to tf.Tensor</span>
<span class="sd">      Data dictionary. It is a replicated data set, where each key</span>
<span class="sd">      and value matches the same type as any observed data set that</span>
<span class="sd">      the model aims to capture.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="inferences.html" class="btn btn-neutral float-right" title="Inference" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="data.html" class="btn btn-neutral" title="Data" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Edward Development Team.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0.9',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>