<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Probabilistic PCA</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/">Home</a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="#">Advanced</a>
    <a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
    <a class="button2 u-full-width" href="/contributing">Contributing</a>
    <a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
    <a class="button2 u-full-width" href="/license">License</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
      <!--</object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="probabilistic-pca">Probabilistic PCA</h2>
<p>Probabilistic PCA is useful when dealing with data that is dependent on both principal axes and latent variables <span class="citation">(Tipping &amp; Bishop, 1999)</span>. It is often used when there are missing values in the data.</p>
<p>We’ll run through a quick tutorial to implement it in Edward below. The full script can be accessed <a href="https://github.com/blei-lab/edward/blob/master/examples/probabilistic_pca.py">here</a>.</p>
<h3 id="data">Data</h3>
<p>We simulate our data points below. We’ll talk about the individual variables and what they stand for in the next section. For this example, each point is 2-dimensional, <span class="math inline">\(\mathbf{x}_n\in\mathbb{R}^2\)</span>.</p>
<pre class="python" language="Python"><code>def build_toy_dataset(N, D, K, sigma=1):
  x_train = np.zeros((D, N))
  w = np.random.normal(0.0, 2.0, size=(D, K))
  z = np.random.normal(0.0, 1.0, size=(K, N))
  mean = np.dot(w, z)
  for d in range(D):
    for n in range(N):
      x_train[d, n] = np.random.normal(mean[d, n], sigma)

  return x_train

N = 5000  # number of data points
D = 2  # data dimensionality
K = 1  # latent dimensionality

x_train = build_toy_dataset(N, D, K)</code></pre>
<p>This generates a dataset that looks like this:</p>
<p><img src="/images/ppca-fig0.png" alt="image" width="450" /></p>
<h3 id="model">Model</h3>
<p>Consider a set of <span class="math inline">\(N\)</span> data points, <span class="math inline">\(X = \{\mathbf{x}_n\}\)</span> where <span class="math inline">\(\mathbf{x}_n \in \Bbb{R}^D\)</span>.</p>
<p>While our data operates in dimension <span class="math inline">\(D\)</span>, we will have latent variables <span class="math inline">\(\mathbf{z}_n \in \Bbb{R}^K\)</span> for every <span class="math inline">\(\mathbf{x}_n\)</span> such that <span class="math inline">\(K &lt; D\)</span>. The set of principal axes <span class="math inline">\(\mathbf{W}\)</span> relates our latent variables to our actual data.</p>
<p>Thus, we have that <span class="math display">\[\mathbf{z} \sim N(0, \mathbf{I})\]</span> and that <span class="math display">\[\mathbf{x} \vert \mathbf{z} \sim N(\mathbf{Wz} + \mu, \sigma^2\mathbf{I})\]</span> where <span class="math inline">\(\mathbf{\epsilon} \sim N(0, \sigma^2\mathbf{I})\)</span>.</p>
<p>Because of the dependence on <span class="math inline">\(\mathbf{Wz}\)</span>, we don’t want to solely want to infer the original principal axes (as would be done in the vanilla PCA model). Rather, we aim to model the principal axes with respect to the latent variables.</p>
<p>For our marginal distribution, we get that <span class="math display">\[x \sim N(0, \mathbf{W}\mathbf{W}^Y + \sigma^2\mathbf{I})\]</span></p>
<p>Note here that regular PCA is simply the specific case of Probabilistic PCA, as <span class="math inline">\(\sigma^2 \to 0\)</span>.</p>
<p>We set up our model below by creating a prior distribution over our variables of interest so that we can infer the posterior.</p>
<pre class="python" language="Python"><code>w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))
z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))</code></pre>
<h3 id="inference">Inference</h3>
<p>Since <span class="math inline">\(\mathbf{W}\)</span> cannot be analytically determined, we must use some inference method. Below, we set up our inference variables and then run an approximation algorithm. For this example, our method is to minimize the <span class="math inline">\(\text{KL}(q\|p)\)</span> divergence measure.</p>
<pre class="python" language="Python"><code>qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))
qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))

inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})

init = tf.global_variables_initializer()
inference.run(n_iter=500, n_print=100, n_samples=10)</code></pre>
<h3 id="criticism">Criticism</h3>
<p>One way to criticize the model is to visually compare our actual data to data produced by our inferred values. The blue dots represent the original data, while the red is the inferred.</p>
<p><img src="/images/ppca-fig0.png" alt="image" width="450" /></p>
<p><img src="/images/ppca-fig1.png" alt="image" width="450" /></p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-tipping1999probabilistic">
<p>Tipping, M. E., &amp; Bishop, C. M. (1999). Probabilistic principal component analysis. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <em>61</em>(3), 611–622.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
