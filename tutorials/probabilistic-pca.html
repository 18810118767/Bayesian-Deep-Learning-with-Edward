<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Probabilistic PCA</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/">Home</a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="#">Advanced</a>
    <a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
    <a class="button2 u-full-width" href="/contributing">Contributing</a>
    <a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
    <a class="button2 u-full-width" href="/license">License</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
      <!--</object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="probabilistic-pca">Probabilistic PCA</h2>
<p>Probabilistic principal components analysis (PCA) is useful for analyzing data via a lower dimensional latent space <span class="citation">(Tipping &amp; Bishop, 1999)</span>. It is often used when there are missing values in the data or for multidimensional scaling.</p>
<p>We demonstrate with an example in Edward. An interactive version with Jupyter notebook is available <a href="http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/docs/notebooks/probabilistic_pca.ipynb">here</a>.</p>
<h3 id="data">Data</h3>
<p>We use simulated data. We’ll talk about the individual variables and what they stand for in the next section. For this example, each data point is 2-dimensional, <span class="math inline">\(\mathbf{x}_n\in\mathbb{R}^2\)</span>.</p>
<pre class="python" language="Python"><code>def build_toy_dataset(N, D, K, sigma=1):
  x_train = np.zeros((D, N))
  w = np.random.normal(0.0, 2.0, size=(D, K))
  z = np.random.normal(0.0, 1.0, size=(K, N))
  mean = np.dot(w, z)
  for d in range(D):
    for n in range(N):
      x_train[d, n] = np.random.normal(mean[d, n], sigma)

  print(&quot;True principal axes:&quot;)
  print(w)
  return x_train

N = 5000  # number of data points
D = 2  # data dimensionality
K = 1  # latent dimensionality

x_train = build_toy_dataset(N, D, K)</code></pre>
<pre><code>## True principal axes:
## [[ 0.25947927]
##  [ 1.80472372]]</code></pre>
<p>We visualize the data set.</p>
<pre class="python" language="Python"><code>plt.scatter(x_train[0, :], x_train[1, :], color=&#39;blue&#39;, alpha=0.1)
plt.axis([-10, 10, -10, 10])
plt.title(&quot;Simulated data set&quot;)
plt.show()</code></pre>
<p><img src="/images/probabilistic-pca-fig0.png" alt="image" width="450" /></p>
<h3 id="model">Model</h3>
<p>Consider a set of <span class="math inline">\(N\)</span> data points, <span class="math inline">\(X = \{\mathbf{x}_n\}\)</span>, where each data point is <span class="math inline">\(d\)</span>-dimensional, <span class="math inline">\(\mathbf{x}_n \in \mathbb{R}^D\)</span>.</p>
<p>While our data operates in <span class="math inline">\(D\)</span> dimensions, we will use latent variables <span class="math inline">\(\mathbf{z}_n \in \mathbb{R}^K\)</span> for each data point <span class="math inline">\(\mathbf{x}_n\)</span> with lower dimension, <span class="math inline">\(K &lt; D\)</span>. The set of principal axes <span class="math inline">\(\mathbf{W}\)</span> relates our latent variables to our actual data.</p>
<p>Specifically, we have that <span class="math display">\[\mathbf{z} \sim N(0, \mathbf{I}),\]</span> and that <span class="math display">\[\mathbf{x} \mid \mathbf{z} \sim N(\mathbf{Wz} + \mu, \sigma^2\mathbf{I}),\]</span> where <span class="math inline">\(\mathbf{\epsilon} \sim N(0, \sigma^2\mathbf{I})\)</span>.</p>
<p>Because of the dependence on <span class="math inline">\(\mathbf{Wz}\)</span>, we don’t want to solely want to infer the original principal axes (as would be done in the vanilla PCA model). Rather, we aim to model the principal axes with respect to the latent variables.</p>
<p>For our marginal distribution, we get that <span class="math display">\[x \sim N(0, \mathbf{W}\mathbf{W}^Y + \sigma^2\mathbf{I}).\]</span></p>
<p>Note here that regular PCA is simply the specific case of Probabilistic PCA, as <span class="math inline">\(\sigma^2 \to 0\)</span>.</p>
<p>We set up our model below, forming creating a prior distribution over our variables of interest.</p>
<pre class="python" language="Python"><code>from edward.models import Normal

w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))
z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))</code></pre>
<h3 id="inference">Inference</h3>
<p>Since the principal axes <span class="math inline">\(\mathbf{W}\)</span> cannot be analytically determined, we must use some inference method. Below, we set up our inference variables and then run a chosen algorithm. For this example, we minimize the <span class="math inline">\(\text{KL}(q\|p)\)</span> divergence measure.</p>
<pre class="python" language="Python"><code>qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))
qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))

inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})
inference.run(n_iter=500, n_print=100, n_samples=10)</code></pre>
<h3 id="criticism">Criticism</h3>
<p>We first inspect the model’s inferred principal axes.</p>
<pre class="python" language="Python"><code>sess = ed.get_session()
print(&quot;Inferred principal axes:&quot;)
print(sess.run(qw.mean()))</code></pre>
<pre><code>## Inferred principal axes:
## [[-0.24093632]
##  [-1.76468039]]</code></pre>
<p>The model has recovered the true principal axes up to finite data and also up to identifiability (there’s a symmetry in the parameterization).</p>
<p>Another way to criticize the model is to visualize the observed data against data generated from our fitted model.</p>
<pre class="python" language="Python"><code>x_post = ed.copy(x, {w: qw, z: qz})
x_gen = sess.run(x_post)

plt.scatter(x_gen[0, :], x_gen[1, :], color=&#39;red&#39;, alpha=0.1)
plt.axis([-10, 10, -10, 10])
plt.title(&quot;Data generated from model&quot;)
plt.show()</code></pre>
<p><img src="/images/probabilistic-pca-fig1.png" alt="image" width="450" /></p>
<p>The generated data looks close to the true data.</p>
<h3 id="acknowledgments">Acknowledgments</h3>
<p>We thank Mayank Agrawal for writing the initial version of this tutorial.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-tipping1999probabilistic">
<p>Tipping, M. E., &amp; Bishop, C. M. (1999). Probabilistic principal component analysis. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <em>61</em>(3), 611–622.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
