<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Supervised Learning (Classification)</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Docstrings. */
  dl.class, dl.function { /* Add border to each docstring. */
    border-bottom: 1px solid #E1E1E1;
  }
  dd { /* Remove overall indenting in an entry. */
    margin-left: 0.0rem;
  }
  dl th, dl td { /* Remove extraneous padding and decorations. */
    padding: 0 15px 0 0;
    border: none;
  }
  dt em { /* Keep 'class' consistent in style with rest of def. */
    font-style: normal;
    font-size: 14px !important;
  }
  /* Attribute contents within a docstring. */
  dd blockquote, dl blockquote, dt blockquote { /* Reduce margins. */
    margin-left: 0.0rem;
    margin-top: 0.0rem;
    margin-bottom: 0.0rem;
  }
  dl td p { /* Reduce spacing. */
    margin-bottom: 0.75rem;
  }
  dl td.field-body { /* Add indenting. */
    padding-top: 0.75rem;
    padding-left: 2.0rem;
    display: block;
  }
  dl code { /* Keep code font size consistent with rest of contents. */
    font-size: 90%;
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/">Home</a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="#">Advanced</a>
    <a class="button2 u-full-width" href="/community">Community</a>
    <a class="button2 u-full-width" href="/contributing">Contributing</a>
    <a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
      <!--</object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="supervised-learning-classification">Supervised Learning (Classification)</h2>
<p>In supervised learning, the task is to infer hidden structure from labeled data, comprised of training examples <span class="math inline">\(\{(x_n, y_n)\}\)</span>. Classification means the output <span class="math inline">\(y\)</span> takes discrete values.</p>
<p>We demonstrate with an example in Edward. An interactive version with Jupyter notebook is available <a href="http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/docs/notebooks/supervised_classification.ipynb">here</a>.</p>
<h3 id="data">Data</h3>
<p>Use 25 data points from the <a href="https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/crabs.html">crabs data set</a>.</p>
<pre class="python" language="Python"><code>df = np.loadtxt(&#39;data/crabs_train.txt&#39;, dtype=&#39;float32&#39;, delimiter=&#39;,&#39;)
df[df[:, 0] == -1, 0] = 0  # replace -1 label with 0 label

N = 25  # number of data points
D = df.shape[1] - 1  # number of features

subset = np.random.choice(df.shape[0], N, replace=False)
X_train = df[subset, 1:]
y_train = df[subset, 0]</code></pre>
<h3 id="model">Model</h3>
<p>A Gaussian process is a powerful object for modeling nonlinear relationships between pairs of random variables. It defines a distribution over (possibly nonlinear) functions, which can be applied for representing our uncertainty around the true functional relationship. Here we define a Gaussian process model for classification <span class="citation">(Rasmussen &amp; Williams, 2006)</span>.</p>
<p>Formally, a distribution over functions <span class="math inline">\(f:\mathbb{R}^D\to\mathbb{R}\)</span> can be specified by a Gaussian process <span class="math display">\[\begin{aligned}
  p(f)
  &amp;=
  \mathcal{GP}(f\mid \mathbf{0}, k(\mathbf{x}, \mathbf{x}^\prime)),\end{aligned}\]</span> whose mean function is the zero function, and whose covariance function is some kernel which describes dependence between any set of inputs to the function.</p>
<p>Given a set of input-output pairs <span class="math inline">\(\{\mathbf{x}_n\in\mathbb{R}^D,y_n\in\mathbb{R}\}\)</span>, the likelihood can be written as a multivariate normal <span class="math display">\[\begin{aligned}
  p(\mathbf{y})
  &amp;=
  \text{Normal}(\mathbf{y} \mid \mathbf{0}, \mathbf{K})\end{aligned}\]</span> where <span class="math inline">\(\mathbf{K}\)</span> is a covariance matrix given by evaluating <span class="math inline">\(k(\mathbf{x}_n, \mathbf{x}_m)\)</span> for each pair of inputs in the data set.</p>
<p>The above applies directly for regression where <span class="math inline">\(\mathbb{y}\)</span> is a real-valued response, but not for (binary) classification, where <span class="math inline">\(\mathbb{y}\)</span> is a label in <span class="math inline">\(\{0,1\}\)</span>. To deal with classification, we interpret the response as latent variables which is squashed into <span class="math inline">\([0,1]\)</span>. We then draw from a Bernoulli to determine the label, with probability given by the squashed value.</p>
<p>Define the likelihood of an observation <span class="math inline">\((\mathbf{x}_n, y_n)\)</span> as <span class="math display">\[\begin{aligned}
  p(y_n \mid \mathbf{z}, x_n)
  &amp;=
  \text{Bernoulli}(y_n \mid \text{logit}^{-1}(\mathbf{x}_n^\top \mathbf{z})).\end{aligned}\]</span></p>
<p>Define the prior to be a multivariate normal <span class="math display">\[\begin{aligned}
  p(\mathbf{z})
  &amp;=
  \text{Normal}(\mathbf{z} \mid \mathbf{0}, \mathbf{K}),\end{aligned}\]</span> with covariance matrix given as previously stated.</p>
<p>Let’s build the model in Edward. We use a radial basis function (RBF) kernel, also known as the squared exponential or exponentiated quadratic.</p>
<pre class="python" language="Python"><code>from edward.models import Bernoulli, Normal
from edward.util import multivariate_rbf

def kernel(x):
  mat = []
  for i in range(N):
    mat += [[]]
    xi = x[i, :]
    for j in range(N):
      if j == i:
        mat[i] += [multivariate_rbf(xi, xi)]
      else:
        xj = x[j, :]
        mat[i] += [multivariate_rbf(xi, xj)]

    mat[i] = tf.stack(mat[i])

  return tf.stack(mat)

X = tf.placeholder(tf.float32, [N, D])
f = MultivariateNormalFull(mu=tf.zeros(N), sigma=kernel(X))
y = Bernoulli(logits=f)</code></pre>
<p>Here, we define a placeholder <code>X</code>. During inference, we pass in the value for this placeholder according to data.</p>
<h3 id="inference">Inference</h3>
<p>Perform variational inference. Define the variational model to be a fully factorized normal.</p>
<pre class="python" language="Python"><code>qf = Normal(mu=tf.Variable(tf.random_normal([N])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))</code></pre>
<p>Run variational inference for <code>500</code> iterations.</p>
<pre class="python" language="Python"><code>inference = ed.KLqp({f: qf}, data={X: X_train, y: y_train})
inference.run(n_iter=500)</code></pre>
<p>In this case <code>KLqp</code> defaults to minimizing the <span class="math inline">\(\text{KL}(q\|p)\)</span> divergence measure using the reparameterization gradient. For more details on inference, see the <a href="/tutorials/klqp"><span class="math inline">\(\text{KL}(q\|p)\)</span> tutorial</a>. (This example happens to be slow because evaluating and inverting full covariances in Gaussian processes happens to be slow.)</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-rasmussen2006gaussian">
<p>Rasmussen, C. E., &amp; Williams, C. (2006). <em>Gaussian processes for machine learning</em>. MIT Press.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
