
<!DOCTYPE html>
<html>
<head>
  
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>Guide for Research &mdash; Edward</title>

    
      <link rel="shortcut icon" href="../img/favicon.ico">
    

    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <link rel="stylesheet" href="../css/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../css/alabaster-overrides.css" type="text/css" />

    

    

    <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

    
  
</head>
<body role="document">

  <div class="document">
    <div class="documentwrapper">
      <div class="bodywrapper">
        <div class="body" role="main">
          
            <h1 id="getting-started-for-research">Getting Started for Research</h1>
<p>This is a guide to how to use Edward for research. Following Box's loop, we divide research into three components: model, inference, and criticism.</p>
<p>As the library uses TensorFlow as a backend, here is a quick guide on <a href="../tensorflow/">how to get started with it</a>. You will most likely need to work directly in TensorFlow as you manipulate different objects and understand how certain behaviors of the new research works. Here is an <a href="https://github.com/blei-lab/edward/blob/master/examples/normal_idiomatic_tf.py">example</a> with access to the TensorFlow session rather than hiding the TensorFlow internals with <code>inference.run</code>.</p>
<h2 id="developing-new-probabilistic-models">Developing new probabilistic models</h2>
<p>A probabilistic model is specified by a joint distribution <code>p(x,z)</code> of data <code>x</code> and latent variables <code>z</code>. All models in Edward are written as a class; to implement a new model, it can be written in any of the currently supported modeling languages: Stan, TensorFlow, and NumPy/SciPy.</p>
<p>To use Stan, simply write a Stan program in the form of a file or string. Then call it with <code>StanModel(file)</code> or <code>StanModel(model_code)</code>. Here is an example:</p>
<pre><code class="Python">model_code = &quot;&quot;&quot;
    data {
      int&lt;lower=0&gt; N;
      int&lt;lower=0,upper=1&gt; y[N];
    }
    parameters {
      real&lt;lower=0,upper=1&gt; theta;
    }
    model {
      theta ~ beta(1.0, 1.0);
      for (n in 1:N)
        y[n] ~ bernoulli(theta);
    }
&quot;&quot;&quot;
model = ed.StanModel(model_code=model_code)
</code></pre>

<p>Here is a <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_stan.py">toy script</a> that uses this model. Stan programs are convenient as <a href="https://github.com/stan-dev/example-models/wiki">there are many online examples</a>, although they are limited to probability models with differentiable latent variables and they can be quite slow to call in practice over TensorFlow.</p>
<p>To use TensorFlow, PyMC3, or NumPy/SciPy, write a class with the method <code>log_prob(xs, zs)</code>. This method takes as input a mini-batch of data <code>xs</code> and a mini-batch of the latent variables <code>zs</code>; the method outputs a vector of the joint density evaluations <code>[log p(xs, zs[0,:]), log p(xs, zs[1,:]), ...]</code> with size being the size of the latent variables' mini-batch. Here is an example:</p>
<pre><code class="Python">class BetaBernoulli:
    &quot;&quot;&quot;
    p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)
    &quot;&quot;&quot;
    def __init__(self):
        self.num_vars = 1

    def log_prob(self, xs, zs):
        log_prior = beta.logpdf(zs, a=1.0, b=1.0)
        log_lik = tf.pack([tf.reduce_sum(bernoulli.logpmf(xs, z))
                           for z in tf.unpack(zs)])
        return log_lik + log_prior

model = BetaBernoulli()
</code></pre>

<p>Here is a <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_tf.py">toy script</a> that uses this model which is written in TensorFlow. Here is another <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_np.py">toy script</a> that uses the same model written in NumPy/SciPy and <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli_pymc3.py">another</a> written in PyMC3.</p>
<p>For efficiency during future inferences or criticisms, we recommend using the modeling language which contains the most structure about the model; this enables the inference algorithms to automatically take advantage of any available structure if they are implemented to do so. TensorFlow will be most efficient as Edward uses it as the backend for computation.</p>
<h2 id="developing-new-inference-algorithms">Developing new inference algorithms</h2>
<p>An inference algorithm calculates the posterior for a particular model and data set; it is the distribution of the latent variables given data, <code>p(z | x)</code>, and is used in all downstream analyses such as prediction. With Edward, you can develop new black box inference algorithms and also develop custom inference algorithms which are tailored to a particular model or restricted class of models.</p>
<p>There is a base <code>Inference</code> class, from which all inference methods are based on. We categorize inference under two paradigms:</p>
<ul>
<li><code>VariationalInference</code></li>
<li><code>MonteCarlo</code></li>
</ul>
<p>(or more plainly, optimization and sampling). These inherit from <code>Inference</code> and each have their own default methods. See the file <a href="https://github.com/blei-lab/edward/blob/master/edward/inferences.py"><code>inferences.py</code></a>.</p>
<p>Consider developing a variational inference algorithm.
The main method in <code>VariationalInference</code> is <code>run()</code>, which is a simple wrapper that first runs <code>initialize()</code> and then in a loop runs <code>update()</code> and <code>print_progress()</code>. To develop a new variational inference algorithm, inherit from <code>VariationalInference</code> and write a new method for <code>build_loss()</code>: this returns an object that TensorFlow will automatically differentiate during optimization. The other methods have defaults which you can update as necessary. The <a href="https://github.com/blei-lab/edward/blob/master/edward/inferences.py">inclusive KL divergence algorithm in <code>inferences.py</code></a> is a useful example. It writes <code>build_loss()</code> so that automatic diferentiation of its return object is a tractable gradient that minimizes KL(p||q). It also modifies <code>initialize()</code> and <code>update()</code>.</p>
<p>Consider developing a Monte Carlo algorithm. Inherit from <code>MonteCarlo</code>.[Documentation is in progress.]</p>
<p>Note that you can build model-specific inference algorithms and inference algorithms that are tailored to a smaller class than the general class available here. There's nothing preventing you to do so, and the general organizational paradigm and low-level functions are still useful in such a case. You can write a class that for example inherits from <code>Inference</code> directly or inherits to carry both optimization and sampling methods.</p>
<h2 id="developing-new-criticism-techniques">Developing new criticism techniques</h2>
<p>[Documentation is in progress.]</p>
          
        </div>
      </div>
    </div>
    <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
      <div class="sphinxsidebarwrapper">
        
  <p class="logo">
    <a href="..">
      <img class="logo" src="../edward.png" title="Edward">
    </a>
  </p>
  
    <h1 class="logo">Edward</h1>
  



  <p class="blurb">A library for probabilistic modeling, inference, and criticism.
      <a href="http://github.com/blei-lab/edward"><i class="fa fa-github fa-lg"
      aria-hidden="true"></i></a>
  </p>

        <hr />


<ul>
  
      
        <li class="toctree-l1">
          <a href="..">Home</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../getting-started/">Getting Started</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="./">Guide for Research</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../design/">Design and Philosophy</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../developer/">Developer Process</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../tensorflow/">TensorFlow</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../wishlist/">Feature Wishlist</a>
        </li>
      
    
      
        <li class="toctree-l1">
          <a href="../misc/">Miscellaneous</a>
        </li>
      
    
  </ul>
        <!--Commented out since we haven't autogenerated anything yet.-->
        <!--<div id="searchbox" style="display: none;" role="search">
  <h3>Quick search</h3>
  <form class="search" action="../search.html" method="get">
    <input name="q" type="text">
    <input value="Go" type="submit">
  </form>
  <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
  </p>
</div>
<script type="text/javascript">
  document.getElementById("searchbox").style.display = "block";
</script>-->
      </div>
    </div>
    <div class="clearer"></div>
  </div>

  
    <div class="footer">
      
      
    </div>
  

  <script src="../js/jquery-1.10.2.min.js"></script>
  <script src="../js/highlight.pack.js"></script>
  <script src="../js/base.js"></script>

  <!--
  MkDocs version      : 0.15.3
  Docs Build Date UTC : 2016-06-04 09:59:45.151772
  -->
</body>
</html>