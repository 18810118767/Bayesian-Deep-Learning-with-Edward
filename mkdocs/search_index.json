{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Edward\n\n\nEdward\n is a Python library for probabilistic modeling, inference,\nand criticism. It enables black box inference for models with discrete\nand continuous latent variables, neural network parameterizations, and\ninfinite dimensional parameter spaces. Edward serves as a fusion of\nthree fields: Bayesian statistics and machine learning, deep learning,\nand probabilistic programming.\n\n\nIt supports \nmodeling languages\n including\n\n\n\n\nTensorFlow\n (with neural network compositionality via \nKeras\n, \nPretty Tensor\n, or \nTensorFlow-Slim\n)\n\n\nStan\n\n\nPyMC3\n\n\noriginal Python using \nNumPy/SciPy\n\n\n\n\nIt supports \ninference\n via\n\n\n\n\nVariational inference\n\n\nBlack box variational inference\n\n\nStochastic variational inference\n\n\nVariational auto-encoders\n\n\nInclusive KL divergence (KL(p || q))\n\n\n\n\n\n\nMarginal posterior optimization (empirical Bayes, marginal maximum likelihood)\n\n\nMaximum a posteriori (penalized maximum likelihood, maximum likelihood)\n\n\n\n\nIt supports \ncriticism\n of the model and inference via\n\n\n\n\nEvaluations on both supervised and unsupervised tasks\n\n\nPosterior predictive checks\n\n\n\n\nIt also has \nfeatures\n including\n\n\n\n\nTensorFlow\n for backend computation, which includes automatic differentiation, GPU support, computational graphs, optimization, and TensorBoard\n\n\nA library for probability distributions in TensorFlow\n\n\nDocumentation and tutorials\n\n\nExamples demonstrating state-of-the-art generative models and inference\n\n\n\n\nGetting Started\n\n\nYou can find a tutorial here\n\nfor getting started with Edward. We highlight a few examples, more of\nwhich can be found in \nexamples/\n:\n\n\n\n\nBayesian linear regression\n\n\nHierarchical logistic regression\n\n\nMixture model of Gaussians\n\n\nGaussian process classification\n\n\nBayesian neural network\n\n\nMixture density network\n\n\nVariational auto-encoder\n\n\n\n\nInstallation\n\n\nTo install the latest stable version, run\n\n\npip install edward\n\n\n\n\nTo install the latest development version, run\n\n\npip install -e \ngit+https://github.com/blei-lab/edward.git#egg=edward\n\n\n\n\n\nAuthors\n\n\nEdward is led by \nDustin Tran\n with guidance by \nDavid Blei\n. It is under active development (by order of joining) by \nDustin Tran\n, \nDavid Blei\n, \nAlp Kucukelbir\n, \nAdji Dieng\n, \nMaja Rudolph\n, and \nDawen Liang\n. We welcome contributions by submitting issues, feature requests, or by solving any current issues!\n\n\nWe thank Rajesh Ranganath, Allison Chaney, Jaan Altosaar, and other members of the Blei Lab for their helpful feedback and advice.\n\n\nCitation\n\n\nWe appreciate citations for Edward because it lets us find out how\npeople have been using the library and it motivates further work.\n\n\nDustin Tran, David M. Blei, Alp Kucukelbir, Adji Dieng, Maja Rudolph, and Dawen Liang. 2016. Edward: A library for probabilistic modeling, inference, and criticism. https://github.com/blei-lab/edward\n\n\n@misc{tran2016edward,\n  author = {Dustin Tran and David M. Blei and Alp Kucukelbir and Adji Dieng and Maja Rudolph and Dawen Liang},\n  title = {{Edward: A library for probabilistic modeling, inference, and criticism}},\n  year = {2016},\n  url = {https://github.com/blei-lab/edward}\n}", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-edward", 
            "text": "Edward  is a Python library for probabilistic modeling, inference,\nand criticism. It enables black box inference for models with discrete\nand continuous latent variables, neural network parameterizations, and\ninfinite dimensional parameter spaces. Edward serves as a fusion of\nthree fields: Bayesian statistics and machine learning, deep learning,\nand probabilistic programming.  It supports  modeling languages  including   TensorFlow  (with neural network compositionality via  Keras ,  Pretty Tensor , or  TensorFlow-Slim )  Stan  PyMC3  original Python using  NumPy/SciPy   It supports  inference  via   Variational inference  Black box variational inference  Stochastic variational inference  Variational auto-encoders  Inclusive KL divergence (KL(p || q))    Marginal posterior optimization (empirical Bayes, marginal maximum likelihood)  Maximum a posteriori (penalized maximum likelihood, maximum likelihood)   It supports  criticism  of the model and inference via   Evaluations on both supervised and unsupervised tasks  Posterior predictive checks   It also has  features  including   TensorFlow  for backend computation, which includes automatic differentiation, GPU support, computational graphs, optimization, and TensorBoard  A library for probability distributions in TensorFlow  Documentation and tutorials  Examples demonstrating state-of-the-art generative models and inference", 
            "title": "Welcome to Edward"
        }, 
        {
            "location": "/#getting-started", 
            "text": "You can find a tutorial here \nfor getting started with Edward. We highlight a few examples, more of\nwhich can be found in  examples/ :   Bayesian linear regression  Hierarchical logistic regression  Mixture model of Gaussians  Gaussian process classification  Bayesian neural network  Mixture density network  Variational auto-encoder", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#installation", 
            "text": "To install the latest stable version, run  pip install edward  To install the latest development version, run  pip install -e  git+https://github.com/blei-lab/edward.git#egg=edward", 
            "title": "Installation"
        }, 
        {
            "location": "/#authors", 
            "text": "Edward is led by  Dustin Tran  with guidance by  David Blei . It is under active development (by order of joining) by  Dustin Tran ,  David Blei ,  Alp Kucukelbir ,  Adji Dieng ,  Maja Rudolph , and  Dawen Liang . We welcome contributions by submitting issues, feature requests, or by solving any current issues!  We thank Rajesh Ranganath, Allison Chaney, Jaan Altosaar, and other members of the Blei Lab for their helpful feedback and advice.", 
            "title": "Authors"
        }, 
        {
            "location": "/#citation", 
            "text": "We appreciate citations for Edward because it lets us find out how\npeople have been using the library and it motivates further work.  Dustin Tran, David M. Blei, Alp Kucukelbir, Adji Dieng, Maja Rudolph, and Dawen Liang. 2016. Edward: A library for probabilistic modeling, inference, and criticism. https://github.com/blei-lab/edward  @misc{tran2016edward,\n  author = {Dustin Tran and David M. Blei and Alp Kucukelbir and Adji Dieng and Maja Rudolph and Dawen Liang},\n  title = {{Edward: A library for probabilistic modeling, inference, and criticism}},\n  year = {2016},\n  url = {https://github.com/blei-lab/edward}\n}", 
            "title": "Citation"
        }, 
        {
            "location": "/getting-started/", 
            "text": "Getting Started\n\n\nEdward\n is a Python library for probabilistic modeling, inference, and criticism. It is named after the innovative statistician \nGeorge Edward Pelham Box\n.\n\n\nEdward\n follows Box's philosophy of statistics and machine learning.\n\n\nFirst gather data from a real-world process. Then cycle through Box's loop:\n\n\n\n\nBuild a probabilistic model of the process\n\n\nReason about the process given model and data\n\n\nCriticize the model, revise and repeat\n\n\n\n\nHere's a toy example. A child flips a coin ten times, with the data of outcomes being \n[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n. We are interested in the probability that the coin lands heads. First, build a model: assume the coin flips are independent and land heads with the same probability. Second, reason about the process: use an algorithm to infer the model given data. Third, criticize the model: analyze whether the model captures the real-world process of coin flips. If it doesn't, then revise the model and repeat.\n\n\nThis process defines the design of \nEdward\n. Here are the four primary objects that enable the above analysis. (More \nedward\n syntax follows in a complete example.)\n\n\nData\n\n\nData\n objects are containers that contain measurements. The structure of these objects must match the inputs of the probabilistic model.\n\n\ndata = ed.Data(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))\n\n\n\n\nModels\n\n\nThere are two types of model objects in Edward:\n\n\n\n\nProbability models of data\n\n\nVariational models of latent variables\n\n\n\n\nWe can specify probability models of data using NumPy/SciPy, TensorFlow, PyMC3, or Stan. Here is a model of coin flips using a \nBeta-Bernoulli distribution\n in NumPy/Scipy.\n\n\nclass BetaBernoulli(PythonModel):\n    \np(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)\n    \n\n    def _py_log_prob(self, xs, zs):\n        n_samples = zs.shape[0]\n        lp = np.zeros(n_samples, dtype=np.float32)\n        for s in range(n_samples):\n            lp[s] = beta.logpdf(zs[s, :], a=1.0, b=1.0)\n            for n in range(len(xs)):\n                lp[s] += bernoulli.logpmf(xs[n], p=zs[s, :])\n        return lp\n\n\n\n\nThis describes a Bayesian model, which is a joint distribution of data and latent variables \nz\n. With this model and data of coin flips, we aim to reason about \nz\n, the probability that the coin lands heads. The posterior distribution of \nz\n captures our reasoning: its mean describes our best guess of the probability, and its variance describes our uncertainty around our best guess. In this toy model, we know that the posterior is a Beta distribution. Let us assume we do not know its parameters in closed form.\n\n\nEdward can employ variational inference to infer this posterior, which finds the closest distribution within a specified family. Initialize an empty  \nVariational()\n model. Then add a Beta distribution to the variational model.\n\n\nvariational = Variational()\nvariational.add(Beta())\n\n\n\n\nWith this syntax, we can build rich variational models to describe the latent variables in our data models. (More documentation on this coming soon.)\n\n\nInference\n\n\nInference\n objects infer latent variables of models given data. Edward currently supports a variety of variational inference algorithms. These take as input a probability model, a variational model, and data.\n\n\nHere we use mean-field variational inference.\n\n\ninference = ed.MFVI(model, variational, data)\n\n\n\n\n(For the technical audience, the mean-field assumption of a fully factorized approximation is moot here. We're dealing with a one-dimensional latent variable.)\n\n\nCalling \ninference.run\n runs the inference algorithm until convergence. It recovers a Beta distribution with mean 0.25 and variance 0.12. 0.25 is our best guess of the probability that the coin lands heads.\n\n\nCriticism\n\n\n[In Progress]\n\n\nA complete example: the Beta-Bernoulli model\n\n\nHere is a complete script, defining the data, model, and the variational model. We run mean-field variational inference for \n10000\n iterations at the end. The same example is also available in cases where the model is written in \nStan\n, \nPyMC3\n and \nTensorFlow\n respectively.\n\n\nA simple coin flipping example. The model is written in NumPy/SciPy.\n\nProbability model\n    Prior: Beta\n    Likelihood: Bernoulli\nVariational model\n    Likelihood: Mean-field Beta\n\n\nimport edward as ed\nimport numpy as np\n\nfrom edward.models import PythonModel, Variational, Beta\nfrom scipy.stats import beta, bernoulli\n\nclass BetaBernoulli(PythonModel):\n    \np(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)\n    \n\n    def _py_log_prob(self, xs, zs):\n        # This example is pedagogical.\n        # We recommend vectorizing operations in practice.\n        n_minibatch = zs.shape[0]\n        lp = np.zeros(n_minibatch, dtype=np.float32)\n        for s in range(n_minibatch):\n            lp[s] = beta.logpdf(zs[s, :], a=1.0, b=1.0)\n            for n in range(len(xs)):\n                lp[s] += bernoulli.logpmf(xs[n], p=zs[s, :])\n        return lp\n\ndata = ed.Data(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))\nmodel = BetaBernoulli()\nvariational = Variational()\nvariational.add(Beta())\ninference = ed.MFVI(model, variational, data)\n\ninference.run(n_iter=10000)\n\n\n\n\nMore Links\n\n\nYou can find more complicated examples in the \nexamples/\n directory. We highlight several here:\n\n\n\n\nBayesian linear regression\n\n\nHierarchical logistic regression\n\n\nMixture model of Gaussians\n\n\nGaussian process classification\n\n\nBayesian neural network\n\n\nMixture density network\n\n\nVariational auto-encoder\n\n\n\n\nWe think the library will make it significantly easier to do research in machine learning and statistics. You can find more about this \nhere\n.\n\n\nYou can find more about Edward's design and philosophy, and how it relates to other software \nhere\n.\n\n\nReferences\n\n\n\n\nDavid M Blei. Build, compute, critique, repeat: data analysis with latent variable models. Annual Review of Statistics and Its Application, 1:203-232, 2014.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#getting-started", 
            "text": "Edward  is a Python library for probabilistic modeling, inference, and criticism. It is named after the innovative statistician  George Edward Pelham Box .  Edward  follows Box's philosophy of statistics and machine learning.  First gather data from a real-world process. Then cycle through Box's loop:   Build a probabilistic model of the process  Reason about the process given model and data  Criticize the model, revise and repeat   Here's a toy example. A child flips a coin ten times, with the data of outcomes being  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1] . We are interested in the probability that the coin lands heads. First, build a model: assume the coin flips are independent and land heads with the same probability. Second, reason about the process: use an algorithm to infer the model given data. Third, criticize the model: analyze whether the model captures the real-world process of coin flips. If it doesn't, then revise the model and repeat.  This process defines the design of  Edward . Here are the four primary objects that enable the above analysis. (More  edward  syntax follows in a complete example.)", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#data", 
            "text": "Data  objects are containers that contain measurements. The structure of these objects must match the inputs of the probabilistic model.  data = ed.Data(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))", 
            "title": "Data"
        }, 
        {
            "location": "/getting-started/#models", 
            "text": "There are two types of model objects in Edward:   Probability models of data  Variational models of latent variables   We can specify probability models of data using NumPy/SciPy, TensorFlow, PyMC3, or Stan. Here is a model of coin flips using a  Beta-Bernoulli distribution  in NumPy/Scipy.  class BetaBernoulli(PythonModel):\n     p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)\n     \n    def _py_log_prob(self, xs, zs):\n        n_samples = zs.shape[0]\n        lp = np.zeros(n_samples, dtype=np.float32)\n        for s in range(n_samples):\n            lp[s] = beta.logpdf(zs[s, :], a=1.0, b=1.0)\n            for n in range(len(xs)):\n                lp[s] += bernoulli.logpmf(xs[n], p=zs[s, :])\n        return lp  This describes a Bayesian model, which is a joint distribution of data and latent variables  z . With this model and data of coin flips, we aim to reason about  z , the probability that the coin lands heads. The posterior distribution of  z  captures our reasoning: its mean describes our best guess of the probability, and its variance describes our uncertainty around our best guess. In this toy model, we know that the posterior is a Beta distribution. Let us assume we do not know its parameters in closed form.  Edward can employ variational inference to infer this posterior, which finds the closest distribution within a specified family. Initialize an empty   Variational()  model. Then add a Beta distribution to the variational model.  variational = Variational()\nvariational.add(Beta())  With this syntax, we can build rich variational models to describe the latent variables in our data models. (More documentation on this coming soon.)", 
            "title": "Models"
        }, 
        {
            "location": "/getting-started/#inference", 
            "text": "Inference  objects infer latent variables of models given data. Edward currently supports a variety of variational inference algorithms. These take as input a probability model, a variational model, and data.  Here we use mean-field variational inference.  inference = ed.MFVI(model, variational, data)  (For the technical audience, the mean-field assumption of a fully factorized approximation is moot here. We're dealing with a one-dimensional latent variable.)  Calling  inference.run  runs the inference algorithm until convergence. It recovers a Beta distribution with mean 0.25 and variance 0.12. 0.25 is our best guess of the probability that the coin lands heads.", 
            "title": "Inference"
        }, 
        {
            "location": "/getting-started/#criticism", 
            "text": "[In Progress]", 
            "title": "Criticism"
        }, 
        {
            "location": "/getting-started/#a-complete-example-the-beta-bernoulli-model", 
            "text": "Here is a complete script, defining the data, model, and the variational model. We run mean-field variational inference for  10000  iterations at the end. The same example is also available in cases where the model is written in  Stan ,  PyMC3  and  TensorFlow  respectively.  A simple coin flipping example. The model is written in NumPy/SciPy.\n\nProbability model\n    Prior: Beta\n    Likelihood: Bernoulli\nVariational model\n    Likelihood: Mean-field Beta \nimport edward as ed\nimport numpy as np\n\nfrom edward.models import PythonModel, Variational, Beta\nfrom scipy.stats import beta, bernoulli\n\nclass BetaBernoulli(PythonModel):\n     p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)\n     \n    def _py_log_prob(self, xs, zs):\n        # This example is pedagogical.\n        # We recommend vectorizing operations in practice.\n        n_minibatch = zs.shape[0]\n        lp = np.zeros(n_minibatch, dtype=np.float32)\n        for s in range(n_minibatch):\n            lp[s] = beta.logpdf(zs[s, :], a=1.0, b=1.0)\n            for n in range(len(xs)):\n                lp[s] += bernoulli.logpmf(xs[n], p=zs[s, :])\n        return lp\n\ndata = ed.Data(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))\nmodel = BetaBernoulli()\nvariational = Variational()\nvariational.add(Beta())\ninference = ed.MFVI(model, variational, data)\n\ninference.run(n_iter=10000)", 
            "title": "A complete example: the Beta-Bernoulli model"
        }, 
        {
            "location": "/getting-started/#more-links", 
            "text": "You can find more complicated examples in the  examples/  directory. We highlight several here:   Bayesian linear regression  Hierarchical logistic regression  Mixture model of Gaussians  Gaussian process classification  Bayesian neural network  Mixture density network  Variational auto-encoder   We think the library will make it significantly easier to do research in machine learning and statistics. You can find more about this  here .  You can find more about Edward's design and philosophy, and how it relates to other software  here .", 
            "title": "More Links"
        }, 
        {
            "location": "/getting-started/#references", 
            "text": "David M Blei. Build, compute, critique, repeat: data analysis with latent variable models. Annual Review of Statistics and Its Application, 1:203-232, 2014.", 
            "title": "References"
        }, 
        {
            "location": "/guide-research/", 
            "text": "Getting Started for Research\n\n\nThis is a guide to how to use Edward for research. Following Box's loop, we divide research into three components: model, inference, and criticism.\n\n\nAs the library uses TensorFlow as a backend, here is a quick guide on \nhow to get started with it\n. You will most likely need to work directly in TensorFlow as you manipulate different objects and understand how certain behaviors of the new research works. Here is an \nexample\n with access to the TensorFlow session rather than hiding the TensorFlow internals with \ninference.run\n.\n\n\nDeveloping new probabilistic models\n\n\nA probabilistic model is specified by a joint distribution \np(x,z)\n of data \nx\n and latent variables \nz\n. All models in Edward are written as a class; to implement a new model, it can be written in any of the currently supported modeling languages: Stan, TensorFlow, and NumPy/SciPy.\n\n\nTo use Stan, simply write a Stan program in the form of a file or string. Then call it with \nStanModel(file)\n or \nStanModel(model_code)\n. Here is an example:\n\n\nmodel_code = \n\n    data {\n      int\nlower=0\n N;\n      int\nlower=0,upper=1\n y[N];\n    }\n    parameters {\n      real\nlower=0,upper=1\n theta;\n    }\n    model {\n      theta ~ beta(1.0, 1.0);\n      for (n in 1:N)\n        y[n] ~ bernoulli(theta);\n    }\n\n\nmodel = ed.StanModel(model_code=model_code)\n\n\n\n\nHere is a \ntoy script\n that uses this model. Stan programs are convenient as \nthere are many online examples\n, although they are limited to probability models with differentiable latent variables and they can be quite slow to call in practice over TensorFlow.\n\n\nTo use TensorFlow, PyMC3, or NumPy/SciPy, write a class with the method \nlog_prob(xs, zs)\n. This method takes as input a mini-batch of data \nxs\n and a mini-batch of the latent variables \nzs\n; the method outputs a vector of the joint density evaluations \n[log p(xs, zs[0,:]), log p(xs, zs[1,:]), ...]\n with size being the size of the latent variables' mini-batch. Here is an example:\n\n\nclass BetaBernoulli:\n    \n\n    p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)\n    \n\n    def __init__(self):\n        self.num_vars = 1\n\n    def log_prob(self, xs, zs):\n        log_prior = beta.logpdf(zs, a=1.0, b=1.0)\n        log_lik = tf.pack([tf.reduce_sum(bernoulli.logpmf(xs, z))\n                           for z in tf.unpack(zs)])\n        return log_lik + log_prior\n\nmodel = BetaBernoulli()\n\n\n\n\nHere is a \ntoy script\n that uses this model which is written in TensorFlow. Here is another \ntoy script\n that uses the same model written in NumPy/SciPy and \nanother\n written in PyMC3.\n\n\nFor efficiency during future inferences or criticisms, we recommend using the modeling language which contains the most structure about the model; this enables the inference algorithms to automatically take advantage of any available structure if they are implemented to do so. TensorFlow will be most efficient as Edward uses it as the backend for computation.\n\n\nDeveloping new inference algorithms\n\n\nAn inference algorithm calculates the posterior for a particular model and data set; it is the distribution of the latent variables given data, \np(z | x)\n, and is used in all downstream analyses such as prediction. With Edward, you can develop new black box inference algorithms and also develop custom inference algorithms which are tailored to a particular model or restricted class of models.\n\n\nThere is a base \nInference\n class, from which all inference methods are based on. We categorize inference under two paradigms:\n\n\n\n\nVariationalInference\n\n\nMonteCarlo\n\n\n\n\n(or more plainly, optimization and sampling). These inherit from \nInference\n and each have their own default methods. See the file \ninferences.py\n.\n\n\nConsider developing a variational inference algorithm.\nThe main method in \nVariationalInference\n is \nrun()\n, which is a simple wrapper that first runs \ninitialize()\n and then in a loop runs \nupdate()\n and \nprint_progress()\n. To develop a new variational inference algorithm, inherit from \nVariationalInference\n and write a new method for \nbuild_loss()\n: this returns an object that TensorFlow will automatically differentiate during optimization. The other methods have defaults which you can update as necessary. The \ninclusive KL divergence algorithm in \ninferences.py\n is a useful example. It writes \nbuild_loss()\n so that automatic diferentiation of its return object is a tractable gradient that minimizes KL(p||q). It also modifies \ninitialize()\n and \nupdate()\n.\n\n\nConsider developing a Monte Carlo algorithm. Inherit from \nMonteCarlo\n.[Documentation is in progress.]\n\n\nNote that you can build model-specific inference algorithms and inference algorithms that are tailored to a smaller class than the general class available here. There's nothing preventing you to do so, and the general organizational paradigm and low-level functions are still useful in such a case. You can write a class that for example inherits from \nInference\n directly or inherits to carry both optimization and sampling methods.\n\n\nDeveloping new criticism techniques\n\n\n[Documentation is in progress.]", 
            "title": "Guide for Research"
        }, 
        {
            "location": "/guide-research/#getting-started-for-research", 
            "text": "This is a guide to how to use Edward for research. Following Box's loop, we divide research into three components: model, inference, and criticism.  As the library uses TensorFlow as a backend, here is a quick guide on  how to get started with it . You will most likely need to work directly in TensorFlow as you manipulate different objects and understand how certain behaviors of the new research works. Here is an  example  with access to the TensorFlow session rather than hiding the TensorFlow internals with  inference.run .", 
            "title": "Getting Started for Research"
        }, 
        {
            "location": "/guide-research/#developing-new-probabilistic-models", 
            "text": "A probabilistic model is specified by a joint distribution  p(x,z)  of data  x  and latent variables  z . All models in Edward are written as a class; to implement a new model, it can be written in any of the currently supported modeling languages: Stan, TensorFlow, and NumPy/SciPy.  To use Stan, simply write a Stan program in the form of a file or string. Then call it with  StanModel(file)  or  StanModel(model_code) . Here is an example:  model_code =  \n    data {\n      int lower=0  N;\n      int lower=0,upper=1  y[N];\n    }\n    parameters {\n      real lower=0,upper=1  theta;\n    }\n    model {\n      theta ~ beta(1.0, 1.0);\n      for (n in 1:N)\n        y[n] ~ bernoulli(theta);\n    } \nmodel = ed.StanModel(model_code=model_code)  Here is a  toy script  that uses this model. Stan programs are convenient as  there are many online examples , although they are limited to probability models with differentiable latent variables and they can be quite slow to call in practice over TensorFlow.  To use TensorFlow, PyMC3, or NumPy/SciPy, write a class with the method  log_prob(xs, zs) . This method takes as input a mini-batch of data  xs  and a mini-batch of the latent variables  zs ; the method outputs a vector of the joint density evaluations  [log p(xs, zs[0,:]), log p(xs, zs[1,:]), ...]  with size being the size of the latent variables' mini-batch. Here is an example:  class BetaBernoulli:\n     \n    p(x, z) = Bernoulli(x | z) * Beta(z | 1, 1)\n     \n    def __init__(self):\n        self.num_vars = 1\n\n    def log_prob(self, xs, zs):\n        log_prior = beta.logpdf(zs, a=1.0, b=1.0)\n        log_lik = tf.pack([tf.reduce_sum(bernoulli.logpmf(xs, z))\n                           for z in tf.unpack(zs)])\n        return log_lik + log_prior\n\nmodel = BetaBernoulli()  Here is a  toy script  that uses this model which is written in TensorFlow. Here is another  toy script  that uses the same model written in NumPy/SciPy and  another  written in PyMC3.  For efficiency during future inferences or criticisms, we recommend using the modeling language which contains the most structure about the model; this enables the inference algorithms to automatically take advantage of any available structure if they are implemented to do so. TensorFlow will be most efficient as Edward uses it as the backend for computation.", 
            "title": "Developing new probabilistic models"
        }, 
        {
            "location": "/guide-research/#developing-new-inference-algorithms", 
            "text": "An inference algorithm calculates the posterior for a particular model and data set; it is the distribution of the latent variables given data,  p(z | x) , and is used in all downstream analyses such as prediction. With Edward, you can develop new black box inference algorithms and also develop custom inference algorithms which are tailored to a particular model or restricted class of models.  There is a base  Inference  class, from which all inference methods are based on. We categorize inference under two paradigms:   VariationalInference  MonteCarlo   (or more plainly, optimization and sampling). These inherit from  Inference  and each have their own default methods. See the file  inferences.py .  Consider developing a variational inference algorithm.\nThe main method in  VariationalInference  is  run() , which is a simple wrapper that first runs  initialize()  and then in a loop runs  update()  and  print_progress() . To develop a new variational inference algorithm, inherit from  VariationalInference  and write a new method for  build_loss() : this returns an object that TensorFlow will automatically differentiate during optimization. The other methods have defaults which you can update as necessary. The  inclusive KL divergence algorithm in  inferences.py  is a useful example. It writes  build_loss()  so that automatic diferentiation of its return object is a tractable gradient that minimizes KL(p||q). It also modifies  initialize()  and  update() .  Consider developing a Monte Carlo algorithm. Inherit from  MonteCarlo .[Documentation is in progress.]  Note that you can build model-specific inference algorithms and inference algorithms that are tailored to a smaller class than the general class available here. There's nothing preventing you to do so, and the general organizational paradigm and low-level functions are still useful in such a case. You can write a class that for example inherits from  Inference  directly or inherits to carry both optimization and sampling methods.", 
            "title": "Developing new inference algorithms"
        }, 
        {
            "location": "/guide-research/#developing-new-criticism-techniques", 
            "text": "[Documentation is in progress.]", 
            "title": "Developing new criticism techniques"
        }, 
        {
            "location": "/design/", 
            "text": "Design and Philosophy\n\n\nThe \nEdward\n library serves two purposes:\n\n\n\n\n\n\nto serve as a foundation for research;\n\n\n\n\n\n\nto provide an open-source, unified library for inference and criticism, with modeling tools at our disposal.\n\n\n\n\n\n\nAs a research tool, the code base is easily interchangeable so that we can play with extensions for further avenues of investigation, without having to re-implement everything from scratch. For example, for testing new inference algorithms, the model experiments only need to be written once, and we can simply swap the inference object with the newly proposed algorithm. For certain inference algorithms, this corresponds to inheriting from a current inference class and writing a new function for computing stochastic gradients. The same idea applies at a lower level: we can leverage the optimizers and visualization tools already available.\n\n\nAs a practical tool, Edward has many desirable features such as computational graphs for automatic differentiation, GPU support and distributed implementations, support for a variety of modeling languages. It also has the fundamentals: linear model examples, data subsampling for scaling to massive data sets, mean-field variational inference, and so on.\n\n\nRelated Software\n\n\nThere are several notable themes in Edward.\n\n\nProbabilistic programming.\n\nThere has been incredible progress recently on developing generic languages for specifying probability models, or probabilistic programs. Among these include \nVenture\n, \nAnglican\n, \nFigaro\n, \nStan\n, and \nWebPPL\n. We do not work on modeling languages; we work on general-purpose algorithms, which can be applied to any of these languages. For example, we use Stan's modeling language as an easy way to specify probability models, and our library performs inference on them.\n\n\nComputational frameworks.\n\nThere are many computational frameworks, primarily built for deep learning: as of this date, this includes \nTheano\n, \nTensorFlow\n, \nTorch\n, \nComputational Graph Toolkit\n, \nneon\n, and \nStan Math Library\n. These are incredible back end tools, which we use rather than compete with. In terms of abstraction, our library sits at one level higher. For example, we chose to use TensorFlow among the bunch due to Python as our language of choice for machine learning research and as an investment in Google's prodigious engineering.\n\n\nHigh-level deep learning libraries.\n\nNeural network libraries such as \nKeras\n and \nLasagne\n are at a similar abstraction level as us, but they are interested in parameterizing complicated likelihood functions in order to minimize a loss. We are more interested in inferring Bayesian hierarchical models, which can have both complicated likelihood and complicated priors (neural networks are an option but not a necessity). Therefore our goals are orthogonal, and in fact mutually benefit each other: for example, we use Keras' abstraction as a way to easily specify models parameterized by deep neural networks.\n\n\nScalable inference.\n\nImplementations of black box inference algorithms are usually tied to a specific probabilistic programming language, and the majority of these algorithms focus on non-gradient based approaches using sequential Monte Carlo. We are agnostic to the modeling language. Further, we are interested in scalable approaches using gradient information from the model and optimization of an objective function. The most related in this line is \nStan\n's algorithms such as \nADVI\n. However, we don't aim to be as user-friendly by automating as many internals as possible. This is because this library is first and foremost a research tool, where we are interested in not abstracting away the specific mathematics but understanding the mechanics behind them. Further, our developed algorithms will eventually make its way into more mature, enterprise-level software (such as Stan). As several of us are also core developers in Stan, we do prototyping of new algorithms here, and deploy them in Stan when they're ready to be applied industry-wide.", 
            "title": "Design and Philosophy"
        }, 
        {
            "location": "/design/#design-and-philosophy", 
            "text": "The  Edward  library serves two purposes:    to serve as a foundation for research;    to provide an open-source, unified library for inference and criticism, with modeling tools at our disposal.    As a research tool, the code base is easily interchangeable so that we can play with extensions for further avenues of investigation, without having to re-implement everything from scratch. For example, for testing new inference algorithms, the model experiments only need to be written once, and we can simply swap the inference object with the newly proposed algorithm. For certain inference algorithms, this corresponds to inheriting from a current inference class and writing a new function for computing stochastic gradients. The same idea applies at a lower level: we can leverage the optimizers and visualization tools already available.  As a practical tool, Edward has many desirable features such as computational graphs for automatic differentiation, GPU support and distributed implementations, support for a variety of modeling languages. It also has the fundamentals: linear model examples, data subsampling for scaling to massive data sets, mean-field variational inference, and so on.", 
            "title": "Design and Philosophy"
        }, 
        {
            "location": "/design/#related-software", 
            "text": "There are several notable themes in Edward.  Probabilistic programming. \nThere has been incredible progress recently on developing generic languages for specifying probability models, or probabilistic programs. Among these include  Venture ,  Anglican ,  Figaro ,  Stan , and  WebPPL . We do not work on modeling languages; we work on general-purpose algorithms, which can be applied to any of these languages. For example, we use Stan's modeling language as an easy way to specify probability models, and our library performs inference on them.  Computational frameworks. \nThere are many computational frameworks, primarily built for deep learning: as of this date, this includes  Theano ,  TensorFlow ,  Torch ,  Computational Graph Toolkit ,  neon , and  Stan Math Library . These are incredible back end tools, which we use rather than compete with. In terms of abstraction, our library sits at one level higher. For example, we chose to use TensorFlow among the bunch due to Python as our language of choice for machine learning research and as an investment in Google's prodigious engineering.  High-level deep learning libraries. \nNeural network libraries such as  Keras  and  Lasagne  are at a similar abstraction level as us, but they are interested in parameterizing complicated likelihood functions in order to minimize a loss. We are more interested in inferring Bayesian hierarchical models, which can have both complicated likelihood and complicated priors (neural networks are an option but not a necessity). Therefore our goals are orthogonal, and in fact mutually benefit each other: for example, we use Keras' abstraction as a way to easily specify models parameterized by deep neural networks.  Scalable inference. \nImplementations of black box inference algorithms are usually tied to a specific probabilistic programming language, and the majority of these algorithms focus on non-gradient based approaches using sequential Monte Carlo. We are agnostic to the modeling language. Further, we are interested in scalable approaches using gradient information from the model and optimization of an objective function. The most related in this line is  Stan 's algorithms such as  ADVI . However, we don't aim to be as user-friendly by automating as many internals as possible. This is because this library is first and foremost a research tool, where we are interested in not abstracting away the specific mathematics but understanding the mechanics behind them. Further, our developed algorithms will eventually make its way into more mature, enterprise-level software (such as Stan). As several of us are also core developers in Stan, we do prototyping of new algorithms here, and deploy them in Stan when they're ready to be applied industry-wide.", 
            "title": "Related Software"
        }, 
        {
            "location": "/developer/", 
            "text": "Developer Process\n\n\nStandards\n\n\n\n\nPull request and branches.\n We follow standards used in Stan. Namely, contribute by developing on a clone of the repo and writing the code in a branch. Submit a pull request when ready. See Stan's process for \nhow to describe the pull request\n. For developers with push permission to the repo, see Stan's process for \nhow to name branches\n.\n\n\nUnit testing.\n Unit testing is awesome. It's useful not only for checking code after having written it, but also in checking code as you are developing it. If you're informally writing short scripts that output various things anyways, I suggest saving the file in the \ntests/\n directory. This gets the momentum going as the test becomes formalized. For testing, simply run \nnose2\n in the repo; it will automatically find and run all tests in \ntests/\n. Most if not all pull requests should have unit tests.\n\n\nIssue labeling system.\n We use \nStan's labeling system\n. While several labels obviously don't apply to us, it's better than the default labels and it's not worth the effort to reinvent the wheel and maintain a custom system.\n\n\n\n\nCoding\n\n\n\n\nUse \nPEP 8\n.\n\n\nBe compatible with Python 3. See for example \nhere\n to be cognizant of the main differences between 2.7.x and 3.x.\n\n\nAim for 70 characters per line, with some exceptions.\n\n\ndocstrings.\n We follow [Numpy/SciPy standards]\n(https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt).\n\n\nNaming variables.\n For arguments that are positive integers, use \nn_\n, e.g., \nn_minibatch\n, \nn_print\n, to represent \"number of [...]\".\n\n\n\"Private methods\".\n Python of course doesn't really enforce private methods but as convention prefix all methods with \n_\n that are used internally and are not aimed to be exposed to the user.\n\n\nPackage names are almost always plural, with the exception of \nutil.py\n.\n\n\nUse \nPEP 8's blank line standards\n with the caveat to not use more than one empty line to separate code. Also use a blank line to separate the end of an indented procedure:\n\n\n\n\nfor i in range(5):\n    do_stuff()\n\nmore_code()\n\n\n\n\n\n\nIn general, any remaining tasks to be done are raised as Github issues. In other cases, write a TODO comment for things that need to be done but are so minor you'd rather not raise a Github issue; this can be helpful as you are writing code in a branch, and none of the intermediate commits which have a TODO comment will still have that TODO comment when submitting the pull request.\n\n\nEverything in \nedward.stats\n uses SciPy standards. This includes, for example, the argument specification and the choice of how a distribution is parameterized.\n\n\n\n\nSuggested workflow\n\n\n\n\nUpdate your Python path.\n As you make changes, it can be cumbersome to constantly reinstall the package to test these changes. We recommend adding the path to your local repo to your \nPYTHONPATH\n. That is, run the following on the command-line:\n\n\n\n\nexport PYTHONPATH=\n${PYTHONPATH}:/path/to/repo\n\n\n\n\n\nFor this to work permanently, add this line to your \n~/.bashrc\n if you use Bash or \n~/.zshenv\n if you use zsh. Any time you import the package, it will look for the package locally via this directory path.\n\n\n\n\nLocal installation.\n Sometimes it does make sense to check the installation. To install locally, run the following when at the parent directory of the repo:\n\n\n\n\npip install -e edward\n\n\n\n\n(We recommend not installing with \nsudo\n; rather \nuse virtualenv\n.)\n\n\n\n\nPackaging and submitting to PyPI.\n First, update the version number in \nsetup.py\n. Second, follow \nthese steps\n. For shorthand, the sequence of commands is\n\n\n\n\npython setup.py sdist\npython setup.py bdist_wheel\ntwine upload dist/*\n\n\n\n\nThird, tag the release on Github and note the new additions when tagging this release. You can do this by comparing commits from the previous tagged release to master. A link that compares tagged commits to master is available on the \nreleases page\n.\n\n\nSuggested private workflow\n\n\nTo develop work on a branch privately, we suggest using a private repo that maintains the master branch from the public repo. Development happens on the private repo's branch, and when it is finished, you can push it to the public repo's branch to submit as a pull request. We describe this in detail.\n\n\nClone the private repo so you can work on it (create a repo if it does not exist).\n\n\ngit clone https://github.com/yourname/private-repo.git\n\n\n\n\nPull changes from the public repo. This will let the private repo have the latest code from the public repo on its master branch.\n\n\ncd private-repo\ngit remote add public https://github.com/exampleuser/public-repo.git\ngit pull public master # Creates a merge commit\ngit push origin master\n\n\n\n\nNow create your branch on the private repo, develop stuff, and pull any latest changes from the public repo as you develop (\ngit pull public master\n). Make sure that as you're running Edward, you're using the Edward library pointing to the private repo so it reflects your developer changes and not pointed to the public repo where it won't see any changes.\n\n\nFinally, to create a pull request from a private repo's branch to the public repo, push the private branch to the public repo.\n\n\ngit clone https://github.com/exampleuser/public-repo.git\ncd public-repo\ngit remote add private_repo_yourname https://github.com/yourname/private-repo.git\ngit checkout -b pull_request_yourname\ngit pull private_repo_yourname master\ngit push origin pull_request_yourname\n\n\n\n\nNow simply create a pull request via the Github UI on the public repo. Once project owners review the pull request, they can merge it. \nSource", 
            "title": "Developer Process"
        }, 
        {
            "location": "/developer/#developer-process", 
            "text": "", 
            "title": "Developer Process"
        }, 
        {
            "location": "/developer/#standards", 
            "text": "Pull request and branches.  We follow standards used in Stan. Namely, contribute by developing on a clone of the repo and writing the code in a branch. Submit a pull request when ready. See Stan's process for  how to describe the pull request . For developers with push permission to the repo, see Stan's process for  how to name branches .  Unit testing.  Unit testing is awesome. It's useful not only for checking code after having written it, but also in checking code as you are developing it. If you're informally writing short scripts that output various things anyways, I suggest saving the file in the  tests/  directory. This gets the momentum going as the test becomes formalized. For testing, simply run  nose2  in the repo; it will automatically find and run all tests in  tests/ . Most if not all pull requests should have unit tests.  Issue labeling system.  We use  Stan's labeling system . While several labels obviously don't apply to us, it's better than the default labels and it's not worth the effort to reinvent the wheel and maintain a custom system.   Coding   Use  PEP 8 .  Be compatible with Python 3. See for example  here  to be cognizant of the main differences between 2.7.x and 3.x.  Aim for 70 characters per line, with some exceptions.  docstrings.  We follow [Numpy/SciPy standards]\n(https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt).  Naming variables.  For arguments that are positive integers, use  n_ , e.g.,  n_minibatch ,  n_print , to represent \"number of [...]\".  \"Private methods\".  Python of course doesn't really enforce private methods but as convention prefix all methods with  _  that are used internally and are not aimed to be exposed to the user.  Package names are almost always plural, with the exception of  util.py .  Use  PEP 8's blank line standards  with the caveat to not use more than one empty line to separate code. Also use a blank line to separate the end of an indented procedure:   for i in range(5):\n    do_stuff()\n\nmore_code()   In general, any remaining tasks to be done are raised as Github issues. In other cases, write a TODO comment for things that need to be done but are so minor you'd rather not raise a Github issue; this can be helpful as you are writing code in a branch, and none of the intermediate commits which have a TODO comment will still have that TODO comment when submitting the pull request.  Everything in  edward.stats  uses SciPy standards. This includes, for example, the argument specification and the choice of how a distribution is parameterized.", 
            "title": "Standards"
        }, 
        {
            "location": "/developer/#suggested-workflow", 
            "text": "Update your Python path.  As you make changes, it can be cumbersome to constantly reinstall the package to test these changes. We recommend adding the path to your local repo to your  PYTHONPATH . That is, run the following on the command-line:   export PYTHONPATH= ${PYTHONPATH}:/path/to/repo   For this to work permanently, add this line to your  ~/.bashrc  if you use Bash or  ~/.zshenv  if you use zsh. Any time you import the package, it will look for the package locally via this directory path.   Local installation.  Sometimes it does make sense to check the installation. To install locally, run the following when at the parent directory of the repo:   pip install -e edward  (We recommend not installing with  sudo ; rather  use virtualenv .)   Packaging and submitting to PyPI.  First, update the version number in  setup.py . Second, follow  these steps . For shorthand, the sequence of commands is   python setup.py sdist\npython setup.py bdist_wheel\ntwine upload dist/*  Third, tag the release on Github and note the new additions when tagging this release. You can do this by comparing commits from the previous tagged release to master. A link that compares tagged commits to master is available on the  releases page .", 
            "title": "Suggested workflow"
        }, 
        {
            "location": "/developer/#suggested-private-workflow", 
            "text": "To develop work on a branch privately, we suggest using a private repo that maintains the master branch from the public repo. Development happens on the private repo's branch, and when it is finished, you can push it to the public repo's branch to submit as a pull request. We describe this in detail.  Clone the private repo so you can work on it (create a repo if it does not exist).  git clone https://github.com/yourname/private-repo.git  Pull changes from the public repo. This will let the private repo have the latest code from the public repo on its master branch.  cd private-repo\ngit remote add public https://github.com/exampleuser/public-repo.git\ngit pull public master # Creates a merge commit\ngit push origin master  Now create your branch on the private repo, develop stuff, and pull any latest changes from the public repo as you develop ( git pull public master ). Make sure that as you're running Edward, you're using the Edward library pointing to the private repo so it reflects your developer changes and not pointed to the public repo where it won't see any changes.  Finally, to create a pull request from a private repo's branch to the public repo, push the private branch to the public repo.  git clone https://github.com/exampleuser/public-repo.git\ncd public-repo\ngit remote add private_repo_yourname https://github.com/yourname/private-repo.git\ngit checkout -b pull_request_yourname\ngit pull private_repo_yourname master\ngit push origin pull_request_yourname  Now simply create a pull request via the Github UI on the public repo. Once project owners review the pull request, they can merge it.  Source", 
            "title": "Suggested private workflow"
        }, 
        {
            "location": "/tensorflow/", 
            "text": "Getting Started with TensorFlow\n\n\nIf you'd like to use this library for research, you have to learn TensorFlow. Trust me. It's worth the investment. As a heuristic, it takes roughly a day to have a good grasp of the essential mechanics behind the library.\n\n\nHere's what I recommend for learning TensorFlow.\n\n\n\n\nRead TensorFlow's \nGetting Started\n. It tells you the essential objects that it works with.\n\n\nSkim through the simple examples in these \nTensorFlow tutorials\n. It gives you a big picture of the semantics and how the commands generally work with each other.\n\n\nSkim through the example code in this library! If you\u2019re familiar with the underlying math for variational inference, going through the code base here will also teach you the mapping from math to TensorFlow.", 
            "title": "TensorFlow"
        }, 
        {
            "location": "/tensorflow/#getting-started-with-tensorflow", 
            "text": "If you'd like to use this library for research, you have to learn TensorFlow. Trust me. It's worth the investment. As a heuristic, it takes roughly a day to have a good grasp of the essential mechanics behind the library.  Here's what I recommend for learning TensorFlow.   Read TensorFlow's  Getting Started . It tells you the essential objects that it works with.  Skim through the simple examples in these  TensorFlow tutorials . It gives you a big picture of the semantics and how the commands generally work with each other.  Skim through the example code in this library! If you\u2019re familiar with the underlying math for variational inference, going through the code base here will also teach you the mapping from math to TensorFlow.", 
            "title": "Getting Started with TensorFlow"
        }, 
        {
            "location": "/wishlist/", 
            "text": "Feature Wishlist\n\n\nModeling\n\n\n\n\nA graphical modeling language\n\n\n\n\nInference\n\n\n\n\nVariational inference\n\n\nDivergence minimization\n\n\nCoordinate ascent variational inference\n\n\nExpectation propagation\n\n\nGenerative adversarial networks\n\n\n\n\n\n\nEmpirical risk minimization (mean squared error, classification error)\n\n\n\n\n\n\nMonte Carlo\n\n\nMarkov chain Monte Carlo\n\n\nImportance sampling\n\n\nSequential Monte Carlo\n\n\n\n\n\n\n\n\nCriticism\n\n\n\n\nGraphical checks\n\n\nSensitivity analysis\n\n\nConvergence diagnostics\n\n\n\n\nMiscellaneous\n\n\n\n\nVisualization tools\n\n\nA model zoo\n\n\n\n\nAdvanced features\n\n\n\n\nVariance reduction techniques\n\n\nControl variates\n\n\nMarkov blankets\n\n\nLocal expectation gradients\n\n\n\n\n\n\nMulticanonical variational inference\n\n\nVariational models\n\n\nHierarchical variational models\n\n\nMixture of mean-field\n\n\nStructured factorizations\n\n\nNormalizing flows\n\n\nVariational Gaussian process\n\n\nMarkov chains\n\n\n\n\n\n\nPopulation variational inference\n\n\nAccelerated variational inference", 
            "title": "Feature Wishlist"
        }, 
        {
            "location": "/wishlist/#feature-wishlist", 
            "text": "Modeling   A graphical modeling language   Inference   Variational inference  Divergence minimization  Coordinate ascent variational inference  Expectation propagation  Generative adversarial networks    Empirical risk minimization (mean squared error, classification error)    Monte Carlo  Markov chain Monte Carlo  Importance sampling  Sequential Monte Carlo     Criticism   Graphical checks  Sensitivity analysis  Convergence diagnostics   Miscellaneous   Visualization tools  A model zoo", 
            "title": "Feature Wishlist"
        }, 
        {
            "location": "/wishlist/#advanced-features", 
            "text": "Variance reduction techniques  Control variates  Markov blankets  Local expectation gradients    Multicanonical variational inference  Variational models  Hierarchical variational models  Mixture of mean-field  Structured factorizations  Normalizing flows  Variational Gaussian process  Markov chains    Population variational inference  Accelerated variational inference", 
            "title": "Advanced features"
        }, 
        {
            "location": "/misc/", 
            "text": "Advanced features\n\n\nHere is a list of advanced features currently supported.\n\n\n\n\nScore function gradient\n\n\nReparameterization gradient\n\n\nVariance reduction techniques\n\n\nAnalytic KL's and entropy decompositions\n\n\n\n\n\n\nVariational models\n\n\nMean-field\n\n\nGlobal parameterizations (inference networks, recognition networks, inverse mappings)\n\n\n\n\n\n\n\n\nReferences\n\n\nReferences for the process\n\n\n\n\nBox, G. E. P. (1976). Science and Statistics. Journal of the American Statistical Association, 71(356), 791\u2013799. [\nlink\n]\n\n\nBlei, D. M. (2014). Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application. [\nlink\n]\n\n\nGelman, A., \n Shalizi, C. R. (2012). Philosophy and the practice of Bayesian statistics. British Journal of Mathematical and Statistical Psychology, 66(1), 8\u201338. [\nlink\n]\n\n\n\n\nReferences for modeling languages\n\n\n\n\nCarpenter, B., Gelman A., Hoffman, M., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M. A., Guo, J., Li, P., \n Riddell, A. (2016). Stan: A probabilistic programming language. Journal of Statistical Software. [\nlink\n]\n\n\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., \n Chen, Z. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Technical Report. [\nlink\n]\n\n\nvan der Walt, S., Colbert, S. C., \n Varoquaux, G. (2011). The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science \n Engineering, 13, 22-30. [\nlink\n]\n\n\n\n\nReferences for inference\n\n\n\n\nHoffman, M. D., Blei, D. M., Wang, C., \n Paisley, J. (2013). Stochastic Variational Inference. Journal of Machine Learning Research, 14, 1303\u20131347. [\nlink\n]\n\n\nKingma, D. P., \n Welling, M. (2014). Auto-Encoding Variational Bayes. In International Conference on Learning Representations. [\nlink\n]\n\n\nKucukelbir, A., Tran, D., Ranganath, R., Gelman, A., \n Blei, D. M. (2016). Automatic Differentiation Variational Inference. arXiv preprint arXiv:1603.00788. [\nlink\n]\n\n\nKurihara, K., Welling, M., \n Vlassis, N. (2006). Accelerated Variational Dirichlet Process Mixtures. In Neural Information Processing Systems. [\nlink\n]\n\n\nMandt, S., Mcinerney, J., Abrol, F., Ranganath, R., \n Blei, D. M. (2014). Multicanonical Stochastic Variational Inference. In Artificial Intelligence and Statistics. [\nlink\n]\n\n\nMcinerney, J., Ranganath, R., \n Blei, D. M. (2015). The Population Posterior and Bayesian Inference on Streams. In Neural Information Processing Systems. [\nlink\n]\n\n\nRanganath, R., Gerrish, S., \n Blei, D. M. (2014). Black Box Variational Inference. In Artificial Intelligence and Statistics. [\nlink\n]\n\n\nRanganath, R., Tran, D., \n Blei, D. M. (2015). Hierarchical Variational Models. arXiv preprint arXiv:1511.02386. [\nlink\n]\n\n\nRezende, D. J., \n Mohamed, S. (2015). Variational Inference with Normalizing Flows. In International Conference on Machine Learning. [\nlink\n]\n\n\nRezende, D. J., Mohamed, S., \n Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In International Conference on Machine Learning. [\nlink\n]\n\n\nSalimans, T., Kingma, D. P., \n Welling, M. (2015). Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. In International Conference on Machine Learning. [\nlink\n]\n\n\nTran, D., Ranganath, R., \n Blei, D. M. (2016). Variational Gaussian Process. In International Conference on Learning Representations. [\nlink\n]\n\n\n\n\nReferences for criticism\n\n\n\n\nBox, G. E. P. (1980). Sampling and Bayes' inference in scientific modelling and robustness. Journal of the Royal Statistical Society. Series a. General, 143(4), 383\u2013430.\n\n\nGelman, A., Meng, X.-L., \n Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. Statistica Sinica.", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/misc/#advanced-features", 
            "text": "Here is a list of advanced features currently supported.   Score function gradient  Reparameterization gradient  Variance reduction techniques  Analytic KL's and entropy decompositions    Variational models  Mean-field  Global parameterizations (inference networks, recognition networks, inverse mappings)", 
            "title": "Advanced features"
        }, 
        {
            "location": "/misc/#references", 
            "text": "References for the process   Box, G. E. P. (1976). Science and Statistics. Journal of the American Statistical Association, 71(356), 791\u2013799. [ link ]  Blei, D. M. (2014). Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application. [ link ]  Gelman, A.,   Shalizi, C. R. (2012). Philosophy and the practice of Bayesian statistics. British Journal of Mathematical and Statistical Psychology, 66(1), 8\u201338. [ link ]   References for modeling languages   Carpenter, B., Gelman A., Hoffman, M., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M. A., Guo, J., Li, P.,   Riddell, A. (2016). Stan: A probabilistic programming language. Journal of Statistical Software. [ link ]  Abadi, M., Agarwal, A., Barham, P., Brevdo, E.,   Chen, Z. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Technical Report. [ link ]  van der Walt, S., Colbert, S. C.,   Varoquaux, G. (2011). The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science   Engineering, 13, 22-30. [ link ]   References for inference   Hoffman, M. D., Blei, D. M., Wang, C.,   Paisley, J. (2013). Stochastic Variational Inference. Journal of Machine Learning Research, 14, 1303\u20131347. [ link ]  Kingma, D. P.,   Welling, M. (2014). Auto-Encoding Variational Bayes. In International Conference on Learning Representations. [ link ]  Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A.,   Blei, D. M. (2016). Automatic Differentiation Variational Inference. arXiv preprint arXiv:1603.00788. [ link ]  Kurihara, K., Welling, M.,   Vlassis, N. (2006). Accelerated Variational Dirichlet Process Mixtures. In Neural Information Processing Systems. [ link ]  Mandt, S., Mcinerney, J., Abrol, F., Ranganath, R.,   Blei, D. M. (2014). Multicanonical Stochastic Variational Inference. In Artificial Intelligence and Statistics. [ link ]  Mcinerney, J., Ranganath, R.,   Blei, D. M. (2015). The Population Posterior and Bayesian Inference on Streams. In Neural Information Processing Systems. [ link ]  Ranganath, R., Gerrish, S.,   Blei, D. M. (2014). Black Box Variational Inference. In Artificial Intelligence and Statistics. [ link ]  Ranganath, R., Tran, D.,   Blei, D. M. (2015). Hierarchical Variational Models. arXiv preprint arXiv:1511.02386. [ link ]  Rezende, D. J.,   Mohamed, S. (2015). Variational Inference with Normalizing Flows. In International Conference on Machine Learning. [ link ]  Rezende, D. J., Mohamed, S.,   Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In International Conference on Machine Learning. [ link ]  Salimans, T., Kingma, D. P.,   Welling, M. (2015). Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. In International Conference on Machine Learning. [ link ]  Tran, D., Ranganath, R.,   Blei, D. M. (2016). Variational Gaussian Process. In International Conference on Learning Representations. [ link ]   References for criticism   Box, G. E. P. (1980). Sampling and Bayes' inference in scientific modelling and robustness. Journal of the Royal Statistical Society. Series a. General, 143(4), 383\u2013430.  Gelman, A., Meng, X.-L.,   Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. Statistica Sinica.", 
            "title": "References"
        }
    ]
}